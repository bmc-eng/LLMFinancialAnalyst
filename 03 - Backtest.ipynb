{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3912eb-612a-499e-9c4e-edee06488a45",
   "metadata": {},
   "source": [
    "# Backtest the strategies\n",
    "\n",
    "Use an LLM to go through and predict the buy/ sell/ hold recommendation for the company for the given date. Steps needed:\n",
    "\n",
    "1. Load the LLM - use DeepSeek R1 Qwen model at 7B parameters first and try the quantised models next\n",
    "2. Step through each data and each financial statement to get a result\n",
    "3. Log the results in a file and save to S3 (will need a logging file to save to S3 and resume in case of kernel crash)\n",
    "4. Need a backtesting framework to apply the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d93b98-69cc-4462-a0f4-a17f2f95dd3f",
   "metadata": {},
   "source": [
    "## Load libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3af882af-086b-45c8-a773-215a83e2a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from s3fs import S3FileSystem\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "from helper import get_s3_folder\n",
    "import s3Helpers\n",
    "import company_data\n",
    "from s3Helpers import S3ModelHelper, Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4fb4ed3-be43-40fa-bd65-13d7588c9c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 's3Helpers' from '/project/s3Helpers.py'>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(company_data)\n",
    "importlib.reload(s3Helpers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb94ecb-4b18-41c5-8849-696b395f4de1",
   "metadata": {},
   "source": [
    "## Load the LLM\n",
    "\n",
    "Models to test:\n",
    "- Qwen (Qwen/Qwen2.5-7B-Instruct)\n",
    "- Llama (meta-llama/Llama-3.2-7B-Instruct)\n",
    "- DeepSeek (deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd948d2f-1791-4313-bedc-f9ed31aa63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into Huggingface\n",
    "\n",
    "with open('pass.txt') as p:\n",
    "    hf_login = p.read()\n",
    "    \n",
    "hf_login = hf_login[hf_login.find('=')+1:hf_login.find('\\n')]\n",
    "login(hf_login, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aaea478-b1ed-420b-bd8f-0851b1c6e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Quantization \n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab36cbb0-6764-4b22-b83d-7ff24b4aa054",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2bb562-aec7-44a3-87a7-c509839c6dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3d79eb1d1d46d682e830b1d23b1e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac9c6277b064fb387d54e38fbb43f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63495a6e959a477790a7ed26ccd5b944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f537695f1d4a7ca4df0c83146c86ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Flag to download from Huggingface again or use stored model\n",
    "USE_HF = False\n",
    "USE_QUANTIZATION = False\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model_id_s3 = 'llama'\n",
    "\n",
    "\n",
    "if USE_HF:\n",
    "   \n",
    "    pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_id,\n",
    "        model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    if USE_QUANTIZATION:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', quantization_config=quant_config)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', torch_dtype=torch.bfloat16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "else:\n",
    "    model_helper = s3Helpers.S3ModelHelper(s3_sub_folder='tmp/fs')\n",
    "    if USE_QUANTIZATION:\n",
    "        model = model_helper.load_model(model_id_s3, quant_config)\n",
    "    else:\n",
    "        model = model_helper.load_model(model_id_s3)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model_helper.clear_folder(model_id_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029e0fb-a198-4dea-b42b-d506c783a7f4",
   "metadata": {},
   "source": [
    "## Load Financial PIT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eaa01423-1a10-4b83-a25c-ebf18b932470",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load from S3 using the helper file\n",
    "filename = 'data_quarterly_pit_indu.json'\n",
    "sec_helper = company_data.SecurityData('tmp/fs',filename)\n",
    "all_data = sec_helper.get_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "407bc769-45aa-44b4-96c8-24d14ca19040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE WHILE DEVELOPING to\n",
    "importlib.reload(company_data)\n",
    "sec_helper = company_data.SecurityData('tmp/fs',filename, all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e386e4c6-a489-46f2-b023-f58fb4313728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec_helper.total_securities_in_backtest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "08ad27ae-6370-4efd-a9a2-185c13992ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-04-24</th>\n",
       "      <td>114.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-24</th>\n",
       "      <td>119.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-24</th>\n",
       "      <td>124.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-24</th>\n",
       "      <td>127.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-24</th>\n",
       "      <td>117.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-24</th>\n",
       "      <td>118.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-24</th>\n",
       "      <td>116.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-24</th>\n",
       "      <td>119.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-24</th>\n",
       "      <td>124.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-24</th>\n",
       "      <td>135.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-24</th>\n",
       "      <td>128.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-24</th>\n",
       "      <td>84.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-24</th>\n",
       "      <td>83.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Price\n",
       "Date              \n",
       "2019-04-24  114.02\n",
       "2019-05-24  119.51\n",
       "2019-06-24  124.14\n",
       "2019-07-24  127.95\n",
       "2019-08-24  117.76\n",
       "2019-09-24  118.17\n",
       "2019-10-24  116.41\n",
       "2019-11-24  119.06\n",
       "2019-12-24  124.74\n",
       "2020-01-24  135.11\n",
       "2020-02-24  128.19\n",
       "2020-03-24   84.05\n",
       "2020-04-24   83.17"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec_helper.get_security_statement('2020-04-24','AXP UN Equity','px') #AXP UN Equity2020-04-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b8fb357b-065c-47f9-a0de-077ab895129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a financial analyst and must make a buy, sell or hold decision on a company based only on the provided datasets. \\\n",
    "        Compute common financial ratios and then determine the buy or sell decision. Explain your reasons in less than 250 words. Provide a \\\n",
    "        confidence score for how confident you are of the decision. If you are not confident then lower the confidence score. \\\n",
    "        Your answer must be in a JSON object. Provide your answer in JSON format like the two examples below: \\\n",
    "        {'decision': BUY, 'confidence score': 80, 'reason': 'Gross profit and EPS have both increased over time'} \\\n",
    "        {'decision': SELL, 'confidence score': 90, 'reason': 'Price has declined and EPS is falling'}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eaed3958-56ad-43ca-92e1-17b6c3698e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = sec_helper.get_prompt('2020-04-24','AXP UN Equity', system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc4ce7f3-45c8-4cd3-89fc-92044d7a9e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2895"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.apply_chat_template(prompt, tokenize=True, add_generation_prompt=True)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30342446-1709-4731-bb97-37a1a94dd62a",
   "metadata": {},
   "source": [
    "## Run an example in LLM\n",
    "\n",
    "Run into out of memory problem - Potential fixes:\n",
    "1. reduce size of model (quantize)\n",
    "2. explore multi-gpu\n",
    "3. reduce tokens\n",
    "\n",
    "https://saturncloud.io/blog/how-to-solve-cuda-out-of-memory-error-in-pytorch/\n",
    "\n",
    "https://huggingface.co/docs/accelerate/usage_guides/distributed_inference\n",
    "\n",
    "https://medium.com/@geronimo7/llms-multi-gpu-inference-with-accelerate-5a8333e4c5db\n",
    "\n",
    "Problem with splitting a single prompt into multiple gpus to calculate the result. Tensor parallelism - https://huggingface.co/docs/transformers/main/en/perf_train_gpu_many#tensor-parallelism\n",
    "\n",
    "nvidia-smi will show available GPUs on the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d04287fc-33e1-4934-8647-80000236de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(prompt, tokenizer, model):\n",
    "    tokens = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([tokens], return_tensors='pt').to('cuda')\n",
    "    generated_ids = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=1000)\n",
    "    parsed_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    return tokenizer.batch_decode(parsed_ids, skip_special_tokens=True)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cad81f0d-e28a-4c9e-9380-6b4d09b9b24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to execute:  0:00:21.858961\n"
     ]
    }
   ],
   "source": [
    "# Time the execution\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Run the model\n",
    "response = run_model(prompt, tokenizer, model)\n",
    "\n",
    "#Print the length of time to run\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Time to execute: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "04b38225-6385-4fba-9afe-cbea2bfa2a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is the JSON object with the financial ratios and buy/sell/hold decision:\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"Decision\": \"HOLD\",\n",
       "  \"confidence score\": 60,\n",
       "  \"Reason\": \"Operating Income and Net Income have been declining over the past two years, but the company has shown a slight increase in revenue. The debt-to-equity ratio is also increasing, which may be a concern.\"\n",
       "}\n",
       "```\n",
       "\n",
       "To calculate the financial ratios, I used the following:\n",
       "\n",
       "1. Debt-to-equity ratio: Total Liabilities / Total Equity\n",
       "2. Return on Equity (ROE): Net Income / Total Equity\n",
       "3. Current Ratio: Current Assets / Current Liabilities\n",
       "4. Interest Coverage Ratio: Earnings Before Interest and Taxes (EBIT) / Interest Expense\n",
       "\n",
       "Here are the calculations:\n",
       "\n",
       "1. Debt-to-equity ratio:\n",
       "   - 2020: 1.663120e+11 / 2.100600e+10 = 0.794\n",
       "   - 2019: 1.752500e+11 / 2.307100e+10 = 0.762\n",
       "   - 2018: 1.711590e+11 / 2.302500e+10 = 0.743\n",
       "\n",
       "The debt-to-equity ratio has been increasing over the past three years, which may indicate that the company is taking on more debt to fund its operations.\n",
       "\n",
       "2. Return on Equity (ROE):\n",
       "   - 2020: 2.229000e+10 / 2.100600e+10 = 1.061\n",
       "   - 2019: 2.307100e+10 / 2.307100e+10 = 1.000\n",
       "   - 2018: 2.302500e+10 / 2.302500e+10 = 1.000\n",
       "\n",
       "The ROE has been declining over the past three years, which may indicate that the company is not generating enough profits from its operations.\n",
       "\n",
       "3. Current Ratio:\n",
       "   - 2020: 1.886020e+11 / 1.650540e+11 = 1.140\n",
       "   - 2019: 1.983210e+11 / 1.752500e+11 = 1.120\n",
       "   - 2018: 1.941840e+11 / 1.711590e+11 = 1.130\n",
       "\n",
       "The current ratio has been declining over the past three years, which may indicate that the company is not generating enough cash from its operations to cover its liabilities.\n",
       "\n",
       "4. Interest Coverage Ratio:\n",
       "   - 2020: 2.229000e+10 / 3.046000e+09 = 7.373\n",
       "   - 2019: 2.307100e+10 / 3.085000e+09 = 7.479\n",
       "   - 2018: 2.302500e+10 / 3.080000e+09 = 7.473\n",
       "\n",
       "The interest coverage ratio has been declining over the past three years, which may indicate that the company is not generating enough cash from its operations to cover its interest expenses.\n",
       "\n",
       "Based on these ratios, I am recommending a HOLD decision due to the declining operating income, increasing debt-to-equity ratio, declining current ratio, and declining interest coverage ratio. However, the company has shown a slight increase in revenue over the past year, which may indicate some potential for growth."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4396aaf6-8bc5-40e3-8b66-d740b9499d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_json(llm_output):\n",
    "    form = llm_output.replace('\\n','')\n",
    "    soj = form.find('```json')\n",
    "    eoj = form.find('}```')\n",
    "    additional = form[eoj + 4:]\n",
    "    json_obj = json.loads(form[soj + 7:eoj + 1])\n",
    "    json_obj['AdditionalContext'] = additional\n",
    "    return json_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "13805876-d894-4fa7-9256-245ba9a6fdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the JSON object with the financial ratios and buy/sell/hold decision:\\n\\n```json\\n{\\n  \"Decision\": \"HOLD\",\\n  \"confidence score\": 60,\\n  \"Reason\": \"Operating Income and Net Income have been declining over the past two years, but the company has shown a slight increase in revenue. The debt-to-equity ratio is also increasing, which may be a concern.\"\\n}\\n```\\n\\nTo calculate the financial ratios, I used the following:\\n\\n1. Debt-to-equity ratio: Total Liabilities / Total Equity\\n2. Return on Equity (ROE): Net Income / Total Equity\\n3. Current Ratio: Current Assets / Current Liabilities\\n4. Interest Coverage Ratio: Earnings Before Interest and Taxes (EBIT) / Interest Expense\\n\\nHere are the calculations:\\n\\n1. Debt-to-equity ratio:\\n   - 2020: 1.663120e+11 / 2.100600e+10 = 0.794\\n   - 2019: 1.752500e+11 / 2.307100e+10 = 0.762\\n   - 2018: 1.711590e+11 / 2.302500e+10 = 0.743\\n\\nThe debt-to-equity ratio has been increasing over the past three years, which may indicate that the company is taking on more debt to fund its operations.\\n\\n2. Return on Equity (ROE):\\n   - 2020: 2.229000e+10 / 2.100600e+10 = 1.061\\n   - 2019: 2.307100e+10 / 2.307100e+10 = 1.000\\n   - 2018: 2.302500e+10 / 2.302500e+10 = 1.000\\n\\nThe ROE has been declining over the past three years, which may indicate that the company is not generating enough profits from its operations.\\n\\n3. Current Ratio:\\n   - 2020: 1.886020e+11 / 1.650540e+11 = 1.140\\n   - 2019: 1.983210e+11 / 1.752500e+11 = 1.120\\n   - 2018: 1.941840e+11 / 1.711590e+11 = 1.130\\n\\nThe current ratio has been declining over the past three years, which may indicate that the company is not generating enough cash from its operations to cover its liabilities.\\n\\n4. Interest Coverage Ratio:\\n   - 2020: 2.229000e+10 / 3.046000e+09 = 7.373\\n   - 2019: 2.307100e+10 / 3.085000e+09 = 7.479\\n   - 2018: 2.302500e+10 / 3.080000e+09 = 7.473\\n\\nThe interest coverage ratio has been declining over the past three years, which may indicate that the company is not generating enough cash from its operations to cover its interest expenses.\\n\\nBased on these ratios, I am recommending a HOLD decision due to the declining operating income, increasing debt-to-equity ratio, declining current ratio, and declining interest coverage ratio. However, the company has shown a slight increase in revenue over the past year, which may indicate some potential for growth.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "45302cbe-908d-4753-aed6-390d1e2d513e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Decision': 'HOLD',\n",
       " 'confidence score': 60,\n",
       " 'Reason': 'Operating Income and Net Income have been declining over the past two years, but the company has shown a slight increase in revenue. The debt-to-equity ratio is also increasing, which may be a concern.',\n",
       " 'AdditionalContext': 'To calculate the financial ratios, I used the following:1. Debt-to-equity ratio: Total Liabilities / Total Equity2. Return on Equity (ROE): Net Income / Total Equity3. Current Ratio: Current Assets / Current Liabilities4. Interest Coverage Ratio: Earnings Before Interest and Taxes (EBIT) / Interest ExpenseHere are the calculations:1. Debt-to-equity ratio:   - 2020: 1.663120e+11 / 2.100600e+10 = 0.794   - 2019: 1.752500e+11 / 2.307100e+10 = 0.762   - 2018: 1.711590e+11 / 2.302500e+10 = 0.743The debt-to-equity ratio has been increasing over the past three years, which may indicate that the company is taking on more debt to fund its operations.2. Return on Equity (ROE):   - 2020: 2.229000e+10 / 2.100600e+10 = 1.061   - 2019: 2.307100e+10 / 2.307100e+10 = 1.000   - 2018: 2.302500e+10 / 2.302500e+10 = 1.000The ROE has been declining over the past three years, which may indicate that the company is not generating enough profits from its operations.3. Current Ratio:   - 2020: 1.886020e+11 / 1.650540e+11 = 1.140   - 2019: 1.983210e+11 / 1.752500e+11 = 1.120   - 2018: 1.941840e+11 / 1.711590e+11 = 1.130The current ratio has been declining over the past three years, which may indicate that the company is not generating enough cash from its operations to cover its liabilities.4. Interest Coverage Ratio:   - 2020: 2.229000e+10 / 3.046000e+09 = 7.373   - 2019: 2.307100e+10 / 3.085000e+09 = 7.479   - 2018: 2.302500e+10 / 3.080000e+09 = 7.473The interest coverage ratio has been declining over the past three years, which may indicate that the company is not generating enough cash from its operations to cover its interest expenses.Based on these ratios, I am recommending a HOLD decision due to the declining operating income, increasing debt-to-equity ratio, declining current ratio, and declining interest coverage ratio. However, the company has shown a slight increase in revenue over the past year, which may indicate some potential for growth.'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_json(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457537ec-fb2b-4bba-8181-0eae4bdb92b4",
   "metadata": {},
   "source": [
    "## Run the backtest and generate all responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a3f658c-98c0-4a73-81c6-5fb79959f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sec_helper.get_dates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eea06af1-1e6f-43e6-9874-6e27ce8e1d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'company_data' from '/project/company_data.py'>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(s3Helpers)\n",
    "importlib.reload(company_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "beeb6302-6ecf-483a-a191-67a49296cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = s3Helpers.Logger('tmp/fs')\n",
    "def run_backtest(company_info, tokenizer, model, logger, log_at=50):\n",
    "    # get the dates\n",
    "    dates = company_info.get_dates()\n",
    "    # set the current date year\n",
    "    current_year = dates[0][:4]\n",
    "\n",
    "    # set the array\n",
    "    year_log = []\n",
    "    \n",
    "    # set up the display\n",
    "    max_count = company_info.total_securities_in_backtest()\n",
    "    f = IntProgress(min=0, max=max_count) # instantiate the bar\n",
    "    l = Label(value=f.Val)\n",
    "    display(f)\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    # run the backtest \n",
    "    for date in dates:\n",
    "        \n",
    "        securities = company_info.get_securities_reporting_on_date(date)\n",
    "\n",
    "        for security in securities:\n",
    "            if date[:4] != current_year or count % log_at == 0:\n",
    "                # save the file to S3 and reset when it is a new year\n",
    "                logger.log(year_log, current_year + str(count) + '.json')\n",
    "                # reset the stats\n",
    "                year_log = []\n",
    "                current_year = date[:4]\n",
    "\n",
    "        \n",
    "            prompt = company_info.get_prompt(date, security, system_prompt)\n",
    "            response = run_model(prompt, tokenizer, model)\n",
    "            try:\n",
    "                formatted_response = format_json(response)\n",
    "                formatted_response['security'] = security\n",
    "                formatted_response['date'] = date\n",
    "                year_log.append(formatted_response)\n",
    "            except:\n",
    "                print(\"error with \" + security + date)\n",
    "                error_json = {'security': security, 'date': date, 'response': response}\n",
    "                year_log.append(error_json)\n",
    "            f.value += 1\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8c692788-ce72-43ca-8009-ac7249d0e917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a3e12841744aae8ed4db2ab46ffe25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=896)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 20200.json\n",
      "Saved 20202.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_backtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43msec_helper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[105], line 33\u001b[0m, in \u001b[0;36mrun_backtest\u001b[0;34m(company_info, tokenizer, model, logger, log_at)\u001b[0m\n\u001b[1;32m     29\u001b[0m     current_year \u001b[38;5;241m=\u001b[39m date[:\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m     32\u001b[0m prompt \u001b[38;5;241m=\u001b[39m company_info\u001b[38;5;241m.\u001b[39mget_prompt(date, security, system_prompt)\n\u001b[0;32m---> 33\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     formatted_response \u001b[38;5;241m=\u001b[39m format_json(response)\n",
      "Cell \u001b[0;32mIn[74], line 4\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(prompt, tokenizer, model)\u001b[0m\n\u001b[1;32m      2\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(prompt, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m tokenizer([tokens], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m parsed_ids \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     output_ids[\u001b[38;5;28mlen\u001b[39m(input_ids):] \u001b[38;5;28;01mfor\u001b[39;00m input_ids, output_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model_inputs\u001b[38;5;241m.\u001b[39minput_ids, generated_ids)\n\u001b[1;32m      7\u001b[0m ]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(parsed_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/transformers/generation/utils.py:2252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2245\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2246\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2247\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2249\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2265\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2266\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2272\u001b[0m     )\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/transformers/generation/utils.py:3297\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3295\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3296\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[0;32m-> 3297\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3299\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_backtest(sec_helper, tokenizer, model, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da415c52-15fc-4bd8-8917-805093786f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_json = [{'test':'test'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "968fe09b-51a7-4140-be96-b5e122aa594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.log(test_json, 'test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "632f5d0d-e6c6-4260-84ba-2e0c965aa4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bclarke16/tmp/fs/logs/20200.json',\n",
       " 'bclarke16/tmp/fs/logs/202010.json',\n",
       " 'bclarke16/tmp/fs/logs/test.json']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.get_list_of_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "94789e89-fe94-4060-aca1-5a0b23550c00",
   "metadata": {},
   "outputs": [],
   "source": [
    " d = logger.get_log('202010.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3bf67b18-e45d-4b2d-80b7-c74901a168d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6508b723-ead0-49f3-b82b-e08f0303188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sec_helper.get_security_statement('2020-01-31','AON UN Equity','is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee8fba-2085-45d2-8fd9-9dc4d39299c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = datetime.datetime.now()\n",
    "# #formatted_chat = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "# outputs = pipeline(\n",
    "#     prompt,\n",
    "#     max_new_tokens=1000,\n",
    "# )\n",
    "# end_time = datetime.datetime.now()\n",
    "# print(\"Time to execute: \", end_time - start_time)\n",
    "\n",
    "# test_output = outputs[0]['generated_text'][-1]\n",
    "# display(Markdown(test_output['content']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BQuant Python 3",
   "language": "python",
   "name": "remote-jupyterpython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
