{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3912eb-612a-499e-9c4e-edee06488a45",
   "metadata": {},
   "source": [
    "# Multi-GPU Backtest the strategies\n",
    "\n",
    "Use an LLM to go through and predict the buy/ sell/ hold recommendation for the company for the given date. Steps needed:\n",
    "\n",
    "1. Load the LLM - use DeepSeek R1 Qwen model at 7B parameters first and try the quantised models next\n",
    "2. Step through each data and each financial statement to get a result\n",
    "3. Log the results in a file and save to S3 (will need a logging file to save to S3 and resume in case of kernel crash)\n",
    "4. Need a backtesting framework to apply the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d93b98-69cc-4462-a0f4-a17f2f95dd3f",
   "metadata": {},
   "source": [
    "## Load libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1d33ab5-af9c-4fdc-9f1c-6a210084e202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: micromamba install pytorch-gpu torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia --yes --quiet --log-level=error\n",
      "\n",
      "Note: Packages not from Bloomberg channels are not vetted by Bloomberg.\n",
      "\u001b[93mPlease restart the Jupyter kernel if you run into any issues after installing or updating packages via %package.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%package install pytorch-gpu torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3af882af-086b-45c8-a773-215a83e2a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from s3fs import S3FileSystem\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from accelerate.utils import gather_object\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "from ipywidgets import IntProgress, Label, HBox\n",
    "\n",
    "from helper import get_s3_folder\n",
    "import s3Helpers\n",
    "import company_data\n",
    "import prompts\n",
    "from s3Helpers import S3ModelHelper, Logger\n",
    "from prompts import SYSTEM_PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4fb4ed3-be43-40fa-bd65-13d7588c9c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'prompts' from '/project/prompts.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(company_data)\n",
    "importlib.reload(s3Helpers)\n",
    "importlib.reload(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09b47836-92ef-4a05-99b8-7bbcad4c9acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb94ecb-4b18-41c5-8849-696b395f4de1",
   "metadata": {},
   "source": [
    "## Load the LLM\n",
    "\n",
    "Models to test:\n",
    "- Qwen (Qwen/Qwen2.5-7B-Instruct)\n",
    "- Llama (meta-llama/Llama-3.2-7B-Instruct)\n",
    "- DeepSeek (deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)\n",
    "- DeepSeek Quantized (deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) quantized to 4 bits\n",
    "- DeepSeek Quantized ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd948d2f-1791-4313-bedc-f9ed31aa63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into Huggingface\n",
    "\n",
    "with open('pass.txt') as p:\n",
    "    hf_login = p.read()\n",
    "    \n",
    "hf_login = hf_login[hf_login.find('=')+1:hf_login.find('\\n')]\n",
    "login(hf_login, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aaea478-b1ed-420b-bd8f-0851b1c6e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Constants and Quantization \n",
    "USE_HF = False\n",
    "USE_QUANTIZATION = True\n",
    "\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "model_id_s3 = 'deepseek14Q'\n",
    "\n",
    "# dataset to use\n",
    "dataset = 'data_quarterly_pit_indu.json'#'data_annual_pit_indu.json' #'data_quarterly_pit_indu.json'\n",
    "data_local = 'Data/SPX/prompts.json'\n",
    "\n",
    "# Quant configuration\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5ff0af-3883-42b4-b699-910bf2c0522d",
   "metadata": {},
   "source": [
    "## Get all the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a53cd72c-df8b-429b-b429-05f87209e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load from S3 using the helper file\n",
    "def get_all_data():\n",
    "    sec_helper = company_data.SecurityData('tmp/fs',dataset)\n",
    "    return sec_helper\n",
    "\n",
    "def create_all_prompts(company_info, system_prompt):\n",
    "    all_prompts = []\n",
    "    # Get all the dates\n",
    "    dates = company_info.get_dates()\n",
    "    # Loop through each date\n",
    "    for date in dates:\n",
    "        # Pull out the securities reporting on that date\n",
    "        securities = company_info.get_securities_reporting_on_date(date)\n",
    "        # Loop through the securities\n",
    "        for security in securities:\n",
    "            # Calculate the prompt\n",
    "            prompt = company_info.get_prompt(date, security, system_prompt)\n",
    "            record = {'security': security, 'date': date, 'prompt': prompt}\n",
    "            all_prompts.append(record)\n",
    "    return all_prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3cdc9e6-4ddc-4731-8624-d95742ff8b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('Data/base1.1.json', 'rb') as f:\n",
    "    a = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca012ba0-3415-499e-bc2e-0e5e878f55a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'security': 'MMM UN Equity',\n",
       " 'date': '2020-02-06',\n",
       " 'response': 'Based on the provided income statement and balance sheet, I will compute some common financial ratios to make a buy, sell, or hold decision.\\n\\nHere are the computed ratios:\\n\\n1. Debt-to-Equity Ratio:\\n   t: 0.354\\n   t-1: 0.342\\n   t-2: 0.342\\n   t-3: 0.373\\n   t-4: 0.346\\n   t-5: 0.352\\n\\n2. Return on Equity (ROE):\\n   t: 0.103\\n   t-1: 0.124\\n   t-2: 0.114\\n   t-3: 0.114\\n   t-4: 0.114\\n   t-5: 0.114\\n\\n3. Current Ratio:\\n   t: 0.135\\n   t-1: 0.142\\n   t-2: 0.135\\n   t-3: 0.137\\n   t-4: 0.141\\n   t-5: 0.140\\n\\n4. Quick Ratio:\\n   t: 0.145\\n   t-1: 0.153\\n   t-2: 0.145\\n   t-3: 0.147\\n   t-4: 0.150\\n   t-5: 0.151\\n\\n5. Interest Coverage Ratio:\\n   t: 0.151\\n   t-1: 0.154\\n   t-2: 0.148\\n   t-3: 0.149\\n   t-4: 0.152\\n   t-5: 0.153\\n\\n6. Debt-to-Asset Ratio:\\n   t: 0.039\\n   t-1: 0.039\\n   t-2: 0.039\\n   t-3: 0.040\\n   t-4: 0.038\\n   t-5: 0.039\\n\\n7. Return on Assets (ROA):\\n   t: 0.103\\n   t-1: 0.124\\n   t-2: 0.114\\n   t-3: 0.114\\n   t-4: 0.114\\n   t-5: 0.114\\n\\nBased on these ratios, the company appears to be in a stable financial position with a relatively low debt-to-equity ratio and a decent return on equity. The current ratio and quick ratio are slightly below the ideal range, indicating some liquidity concerns. However, the interest coverage ratio is sufficient to cover the interest expenses.\\n\\nGiven the stable financial position, I would recommend a \"HOLD\" decision. The company\\'s financial ratios are not indicating any major red flags, but the slightly low liquidity ratios suggest that the company may be taking on too much debt. Therefore, a cautious approach is recommended.\\n\\nConfidence score: 70\\n\\nReason: The company\\'s financial ratios are relatively stable, but the liquidity ratios are slightly below the ideal range, indicating some concerns about the company\\'s ability to meet its short-term obligations.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67772a75-172a-4792-aed1-9c52b66b4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_data = get_all_data()\n",
    "system_prompt = prompts.SYSTEM_PROMPTS['CoT']['prompt']\n",
    "all_prompts = create_all_prompts(sec_data, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54851917-9666-4e01-ba62-83b844a6e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/all_prompts.json', 'w') as f:\n",
    "    json.dump(all_prompts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dafd6d-8257-4ae5-b431-90c477f1e791",
   "metadata": {},
   "source": [
    "## Define the functions needed for Multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c2bb562-aec7-44a3-87a7-c509839c6dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "\n",
    "def load_model(model_id, model_id_s3, accelerator=None, quant_config=None):\n",
    "    \n",
    "    if USE_HF:\n",
    "\n",
    "        if USE_QUANTIZATION:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id, device_map={\"\":accelerator.process_index}, quantization_config=quant_config)\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id, device_map={\"\":accelerator.process_index}, torch_dtype=torch.bfloat16)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    else:\n",
    "        # load the pre-saved model from S3\n",
    "        model_helper = s3Helpers.S3ModelHelper(s3_sub_folder='tmp/fs')\n",
    "        model = model_helper.load_model(model_id_s3, accelerator)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        #model_helper.clear_folder(model_id_s3)\n",
    "\n",
    "    print(f\"Memory footprint: {model.get_memory_footprint() / 1e9:,.1f} GB\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "542d27f5-5bc8-4651-9c33-7eb0a94a7019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a12d5eb-1c7a-4469-a14b-c3de1d898a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-GPU implementation of run model function\n",
    "def run_model(prompt, tokenizer, model):\n",
    "    tokens = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([tokens], return_tensors='pt').to(\"cuda\")\n",
    "    generated_ids = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=5000)\n",
    "    parsed_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    return tokenizer.batch_decode(parsed_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45845ea3-3bf7-4bed-99e8-fd348f0b716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_json(llm_output):\n",
    "    form = llm_output.replace('\\n','')\n",
    "    # Find the start and end of the JSON input\n",
    "    soj = form.find('```json')\n",
    "    eoj = form.find('}```')\n",
    "    # Pull out the additional context\n",
    "    additional = form[:soj]\n",
    "    additional += form[eoj + 4:]\n",
    "    json_obj = json.loads(form[soj + 7:eoj + 1])\n",
    "    json_obj['AdditionalContext'] = additional\n",
    "    return json_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaa01423-1a10-4b83-a25c-ebf18b932470",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load from S3 using the helper file\n",
    "def get_all_data():\n",
    "    sec_helper = company_data.SecurityData('tmp/fs',dataset)\n",
    "    return sec_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beeb6302-6ecf-483a-a191-67a49296cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the backtest\n",
    "def run_backtest(all_prompts, tokenizer, model, logger, accelerator, log_at=50, start_count=0):\n",
    "    # start the timer\n",
    "    # sync GPUs and start the timer\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    results = []\n",
    "    count = 0\n",
    "    \n",
    "    # set up the display\n",
    "    # max_count = len(all_prompts)\n",
    "    # f = IntProgress(min=0, max=max_count) # instantiate the bar\n",
    "    # l = Label(value=str(f.value))\n",
    "    # display(HBox([f,l]))\n",
    "    \n",
    "    print(\"Starting batch\")\n",
    "    \n",
    "    with accelerator.split_between_processes(all_prompts) as prompts:\n",
    "        results=[]\n",
    "        \n",
    "        for prompt in all_prompts:\n",
    "            start_i = datetime.datetime.now()\n",
    "            response = run_model(prompt['prompt'], tokenizer, model)\n",
    "            formatted_response = {}\n",
    "            formatted_response['response'] = response\n",
    "            formatted_response['security'] = prompt['security']\n",
    "            formatted_response['date'] = prompt['date']\n",
    "            results.append(formatted_response)\n",
    "            \n",
    "            \n",
    "            end_i = datetime.datetime.now()\n",
    "            print(f\"Returned in: {end_i - start_i} {count}\")\n",
    "            count = count + 1\n",
    "            if accelerator.is_main_process:\n",
    "                if count > 0 and count % log_at == 0:\n",
    "                    results_gathered = gather_object(results)\n",
    "                    logger.log(results_gathered, f\"results - {datetime.datetime.now()}.json\")\n",
    "            \n",
    "            \n",
    "    # gather all of the results into a single object\n",
    "    results_gathered = gather_object(results)\n",
    "    # Log the last values\n",
    "    logger.log(results_gathered, f\"results - {datetime.datetime.now()}.json\")\n",
    "    # end the timer\n",
    "    end_time = datetime.datetime.now()\n",
    "    print(\"Completed! Time to execute: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e7caefd-eb26-426f-9953-aa3ebd190e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the entry point for backtest\n",
    "def run_inference():\n",
    "    accelerator = Accelerator()\n",
    "    model, tokenizer = load_model(model_id, model_id_s3, accelerator, quant_config)\n",
    "    \n",
    "    accelerator.wait_for_everyone()\n",
    "    \n",
    "    \n",
    "    # Clear the folder cache\n",
    "    s3 = s3Helpers.S3ModelHelper('tmp/fs')\n",
    "    s3.clear_folder(model_id_s3)\n",
    "\n",
    "    company_data = get_all_data()\n",
    "\n",
    "    # set up system prompts\n",
    "    \n",
    "    all_prompts = create_all_prompts(company_data, system_prompt)\n",
    "\n",
    "    # batch into groups of 8\n",
    "    #batches = [all_prompts[i:i + 8] for i in range(0, len(all_prompts), 8)]  \n",
    "        \n",
    "    accelerator.wait_for_everyone()\n",
    "    # Limit for testing\n",
    "    #prompt_limit = all_prompts[:5]\n",
    "    \n",
    "    # set up the logger\n",
    "    logger = s3Helpers.Logger('tmp/fs')\n",
    "    #for batch in batches:\n",
    "        #run the backtest\n",
    "    run_backtest(all_prompts, tokenizer, model, logger, accelerator)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c692788-ce72-43ca-8009-ac7249d0e917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02eb35a9c05543788bc9c18400ee7f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ed5d5189c842e58295920bd3c254b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 9.7 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45eaa3b4082a466c87e31db48734d472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38bde1a623314244b358ab3157f1176f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 9.7 GB\n",
      "Memory footprint: 9.7 GB\n",
      "Memory footprint: 9.7 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-19 23:25:29,493] torch.distributed.elastic.agent.server.api: [WARNING] Received 2 death signal, shutting down workers\n",
      "[2025-02-19 23:25:29,495] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 1287 via signal SIGINT\n",
      "[2025-02-19 23:25:29,496] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 1288 via signal SIGINT\n",
      "[2025-02-19 23:25:29,496] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 1289 via signal SIGINT\n",
      "[2025-02-19 23:25:29,499] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 1290 via signal SIGINT\n",
      "[2025-02-19 23:25:59,529] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 1287 via 2, forcefully exiting via 9\n",
      "[2025-02-19 23:25:59,720] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 1288 via 2, forcefully exiting via 9\n",
      "[2025-02-19 23:25:59,921] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 1289 via 2, forcefully exiting via 9\n"
     ]
    },
    {
     "ename": "SignalException",
     "evalue": "Process 1205 got signal: 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSignalException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_inference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/accelerate/launchers.py:244\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, ELASTIC_LOG_LINE_PREFIX_TEMPLATE_PYTORCH_VERSION):\n\u001b[1;32m    243\u001b[0m         launch_config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_line_prefix_template\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log_line_prefix_template\n\u001b[0;32m--> 244\u001b[0m     \u001b[43melastic_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLaunchConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlaunch_config_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/torch/distributed/launcher/api.py:134\u001b[0m, in \u001b[0;36melastic_launch.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlaunch_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entrypoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/torch/distributed/launcher/api.py:255\u001b[0m, in \u001b[0;36mlaunch_agent\u001b[0;34m(config, entrypoint, args)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     metrics\u001b[38;5;241m.\u001b[39minitialize_metrics(metrics\u001b[38;5;241m.\u001b[39mMetricsConfig(config\u001b[38;5;241m.\u001b[39mmetrics_cfg))\n\u001b[0;32m--> 255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# records that agent.run() has succeeded NOT that workers have succeeded\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     events\u001b[38;5;241m.\u001b[39mrecord(agent\u001b[38;5;241m.\u001b[39mget_event_succeeded())\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/torch/distributed/elastic/metrics/api.py:124\u001b[0m, in \u001b[0;36mprof.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 124\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     put_metric(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.success\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, group)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/torch/distributed/elastic/agent/server/api.py:736\u001b[0m, in \u001b[0;36mSimpleElasticAgent.run\u001b[0;34m(self, role)\u001b[0m\n\u001b[1;32m    734\u001b[0m shutdown_called: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 736\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_execution_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_metrics(result)\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/torch/distributed/elastic/agent/server/api.py:877\u001b[0m, in \u001b[0;36mSimpleElasticAgent._invoke_run\u001b[0;34m(self, role)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_group\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m!=\u001b[39m WorkerState\u001b[38;5;241m.\u001b[39mINIT\n\u001b[0;32m--> 877\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(monitor_interval)\n\u001b[1;32m    878\u001b[0m     run_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_monitor_workers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_group)\n\u001b[1;32m    879\u001b[0m     state \u001b[38;5;241m=\u001b[39m run_result\u001b[38;5;241m.\u001b[39mstate\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/api.py:62\u001b[0m, in \u001b[0;36m_terminate_process_handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Termination handler that raises exceptions on the main process.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mWhen the process receives death signal(SIGTERM, SIGINT), this termination handler will\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mbe terminated.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m sigval \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mSignals(signum)\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m SignalException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mgetpid()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got signal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msigval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, sigval\u001b[38;5;241m=\u001b[39msigval)\n",
      "\u001b[0;31mSignalException\u001b[0m: Process 1205 got signal: 2"
     ]
    }
   ],
   "source": [
    "notebook_launcher(run_inference, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d54639-2bf0-4ee5-b28c-51067a50360e",
   "metadata": {},
   "source": [
    "### Concatenate all of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d21e2d-37c2-4c63-92f6-c97e39e56992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "319908bb-6076-43b1-bd7f-4af4fc409baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_helper = s3Helpers.S3ModelHelper('tmp/fs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76e4e6f0-58e4-483c-b35a-150f4b6bc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_helper.clear_folder('deepseek32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64b057d5-eb29-447e-ae65-cb23046ae4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bclarke16/tmp/fs/logs/results.json', 'bclarke16/tmp/fs/logs/results2.json']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "logger = s3Helpers.Logger('tmp/fs')\n",
    "log_list = logger.get_list_of_logs()\n",
    "log_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0045bda3-133e-4654-bfda-ebae2f39e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logger.create_master_log(save_to_s3=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bbc0f159-f6d1-4063-bc62-f4ce98759148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_all_logs():\n",
    "    log_list = logger.get_list_of_logs()\n",
    "    logs = []\n",
    "    for logfile in log_list:\n",
    "        logs += logger.get_log(logfile[logfile.find('/logs/') + 6:])\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3ed3b7a7-8293-4b44-b58f-f7fd8ca30a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = concat_all_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "37b4fdad-d6a0-45c5-98fa-d5cd85bf745d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "909"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a0bdb21-17b9-4b00-b6fd-2db26070b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logger.get_log('results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d39d6268-9a33-46f7-a4cc-791f7f711927",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('Data/test_run.json', 'w') as f:\n",
    "    json.dump(log, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b112a6ae-142f-4d07-b900-ca2d367cf5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JNJ UN Equity\n",
      "WMT UN Equity\n",
      "NVDA UQ Equity\n",
      "VZ UN Equity\n",
      "GS UN Equity\n"
     ]
    }
   ],
   "source": [
    "for l in log:\n",
    "    print(l['security'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d300bff-276f-445b-b98e-a3810e8d591f",
   "metadata": {},
   "source": [
    "## Multi GPU run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43a03512-c413-4531-83c2-e254da5c54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_helper = get_all_data()\n",
    "system_prompt = prompts.SYSTEM_PROMPTS['CoT']['prompt']\n",
    "all_prompts = create_all_prompts(sec_helper, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e31bc5a8-5917-487d-a7d2-74253b928027",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [all_prompts[i:i + 8] for i in range(0, len(all_prompts), 8)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "883b21aa-0ecc-4c83-a7f7-7efa3f051ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104a472-6c11-4ddd-970d-d64fda33ca79",
   "metadata": {},
   "source": [
    "## Save any model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fbe7cc4-4a4d-449d-bd5e-8950c139485f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'process_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_id_s3\u001b[49m\u001b[43m,\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_id, model_id_s3, accelerator, quant_config)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m USE_HF:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_QUANTIZATION:\n\u001b[0;32m----> 8\u001b[0m         model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, device_map\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_index\u001b[49m}, quantization_config\u001b[38;5;241m=\u001b[39mquant_config)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m         model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, device_map\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:accelerator\u001b[38;5;241m.\u001b[39mprocess_index}, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'process_index'"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(model_id,model_id_s3,quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6def2848-5dd7-410e-9977-1cf3dfb422dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_helper = S3ModelHelper('tmp/fs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "634ec4d3-ca3f-40e7-8390-420b59c1bdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bclarke16/tmp/fs/deepseek32/config.json\n",
      "bclarke16/tmp/fs/deepseek32/generation_config.json\n",
      "bclarke16/tmp/fs/deepseek32/model-00001-of-00004.safetensors\n",
      "bclarke16/tmp/fs/deepseek32/model-00002-of-00004.safetensors\n",
      "bclarke16/tmp/fs/deepseek32/model-00003-of-00004.safetensors\n",
      "bclarke16/tmp/fs/deepseek32/model-00004-of-00004.safetensors\n",
      "bclarke16/tmp/fs/deepseek32/model.safetensors.index.json\n",
      "Files deleted in S3\n"
     ]
    }
   ],
   "source": [
    "model_helper.delete_model_in_s3('deepseek32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd98c82f-eec9-4b94-b5b0-660117a746b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('Data/DeepSeek32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8642f594-7220-42b0-9aae-4cc5a0cc4796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model_helper.save_model_to_s3('Data/DeepSeek32','deepseek32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64771e33-e965-4b7f-a378-608a2dcf5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_helper.clear_folder('Data/DeepSeek32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee8fba-2085-45d2-8fd9-9dc4d39299c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = datetime.datetime.now()\n",
    "# #formatted_chat = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "# outputs = pipeline(\n",
    "#     prompt,\n",
    "#     max_new_tokens=1000,\n",
    "# )\n",
    "# end_time = datetime.datetime.now()\n",
    "# print(\"Time to execute: \", end_time - start_time)\n",
    "\n",
    "# test_output = outputs[0]['generated_text'][-1]\n",
    "# display(Markdown(test_output['content']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bloomberg Lab Python 3",
   "language": "python",
   "name": "remote-jupyterpython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
