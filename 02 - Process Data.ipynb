{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c51b3fbc-ac21-4d4e-84a6-ab65f22015cd",
   "metadata": {},
   "source": [
    "# Process Data\n",
    "\n",
    "#### To Do:\n",
    "- Process to save the model - cannot download from Huggingface each time\n",
    "- How to store the results - what format should this be in for Equity Signal Lab\n",
    "- Base case, look at following the trend.\n",
    "- Backtesting process to ensure point in time. Will need to request on daily basis for the proper test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "614d2518-e92a-4a7b-9df8-d5e3789c06c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from s3fs import S3FileSystem\n",
    "import os\n",
    "import json\n",
    "\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from helper import get_s3_folder\n",
    "import s3_model\n",
    "from s3_model import S3ModelHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b43a2fe1-0f15-4888-b37c-b0da25fd8e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 's3_model' from '/project/s3_model.py'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(s3_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d6ad91-fed5-4c0c-be20-a90168958528",
   "metadata": {},
   "source": [
    "## Bring in Financial Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a2d3e9-18fe-4f8d-a77d-c1949a94eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load from S3\n",
    "user_bucket_name = os.environ['BQUANT_SANDBOX_USER_BUCKET']\n",
    "bqnt_username = os.environ['BQUANT_USERNAME']\n",
    "\n",
    "path_to_s3 = f's3://{user_bucket_name}/{bqnt_username}/tmp/fs/data.json'\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "all_data = {}\n",
    "with s3.open(path_to_s3, 'rb') as f:\n",
    "    all_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a9474e-b98b-4c5b-8b6e-930aa7c678a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d10ba-d591-47c0-923c-d4d67f5b1528",
   "metadata": {},
   "source": [
    "## Check the data and reformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707fb9a9-f1f9-4a80-b207-74aad3a8d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_data = all_data['2007-12-31']\n",
    "#fin_data\n",
    "date_is_all = pd.DataFrame(fin_data['is']).set_index(['ID', 'level_1'])\n",
    "date_bs_all = pd.DataFrame(fin_data['bs']).set_index(['ID', 'level_1'])\n",
    "date_is_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301c6d8-c863-44b6-bfa6-26e590a84453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a single security\n",
    "def get_securities(df):\n",
    "    return df.reset_index(inplace=False)['ID'].drop_duplicates(inplace=False)\n",
    "\n",
    "securities = get_securities(date_is_all)\n",
    "sec_test = date_is_all.loc[securities[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e132295-510b-4295-8506-070a077b0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all non-zero values\n",
    "test_sec = sec_test.loc[(sec_test!=0).any(axis=1)]\n",
    "test_sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb301dc-fdef-463c-bc51-d8f280c15841",
   "metadata": {},
   "source": [
    "## Set up the LLM\n",
    "\n",
    "To download the llama model, run the following code to login with an access token and then run the code to download and access the model. This requires a login from Huggingface. We use the Llama model (1B parameters but can try this with the 70B parameters too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee92109a-783f-4e7c-aa53-80265651a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "with open('pass.txt') as p:\n",
    "    hf_login = p.read()\n",
    "    \n",
    "hf_login = hf_login[hf_login.find('=')+1:]\n",
    "login(hf_login, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "530eb17b-19e4-4199-8709-907fb30956ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e59792e05dc4ae5bd92a2781369021e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2569b8b3b924f8f9ae1f14c170a6fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2010d05a62345cfb9ecf4a5625456f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a59a24590d44f69acf6168f9803091b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ec49300d924c96a7a815405a6d37a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c272d5f86ad410bac2d0d57e52d5a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8debc91d6c654591acebe955ca979efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accd636e24a54cc5a5b8a6cdd9048d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7eb0f6de3a4e23bf6741dc1064473b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6088b9e7ce347e688f1ad69cf0362b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8aa13a51146415cb6c940e881fca4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b103266f879e4167a6423eadf06f2ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bdd4a0e0704ff1809fcb0a8f9dafad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Execute with a small model first`\n",
    "\n",
    "# Llama Small model\n",
    "#model_id = \"meta-llama/Llama-3.2-3B-Instruct\"#\"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "# Qwen2 72B Insturct\n",
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d6de63-8330-46af-b367-02c7aa4555d2",
   "metadata": {},
   "source": [
    "Check out: https://huggingface.co/docs/transformers/conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f85f5c-bdb7-4b06-8b7d-b67513b98d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model is in S3 or download from scratch\n",
    "USE_HF = True\n",
    "\n",
    "if USE_HF:\n",
    "    model_id = \"meta-llama/Llama-3.2-7B-Instruct\"\n",
    "    \n",
    "\n",
    "    pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_id,\n",
    "        model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', torch_dtype=torch.bfloat16 )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id )\n",
    "else:\n",
    "    model_id = 'llama'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "262dfd1c-dffe-422c-a24d-48f47d798132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fc3dd97228448482976f6448554455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', torch_dtype=torch.bfloat16 )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a342c6f-f2ef-433d-a2de-0d3737863257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#messages = [\n",
    "#    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "#    {\"role\": \"user\", \"content\": \"where do you live?\"}\n",
    "#]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a financial analyst and must make a buy, sell or hold decision on a company based only on the provided datasets. \\\n",
    "        Compute common financial ratios and then determine the buy sell decision. Explain your reasons and answer in a format that compiles to a JSON object.\\\n",
    "        Answer as a JSON string with the following example format: \\\n",
    "        {'Investment Decision': BUY, 'Reason': 'Gross profit and EPS have both increased over time'}\"},\n",
    "    {\"role\": \"user\", \"content\": test_sec.to_string()}\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799dd641-62f0-4224-9b3e-485b519f0f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(\"Chat: \", formatted_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ae1c2-14d6-495b-8c3f-a236fe23885c",
   "metadata": {},
   "source": [
    "## Get the output from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c05c2-5b4f-45b5-8e86-2a176aba85f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=500,\n",
    ")\n",
    "\n",
    "test_output = outputs[0]['generated_text'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43910a0d-84fe-4192-a66d-06bb4987c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(test_output['content'])) #[8:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc2fc9-cd51-4f44-9eee-4a95c6e6a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_json(llm_output):\n",
    "    form = llm_output['content'].replace('\\n','')\n",
    "    eoj = form.find('}```')\n",
    "    additional = form[eoj + 4:]\n",
    "    json_obj = json.loads(form[7:eoj + 1])\n",
    "    json_obj['AdditionalContext'] = additional\n",
    "    return json_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124f8a1-7b96-4b2c-a46c-301e9dc02a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = format_json(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86cdfb10-b27b-44cb-9959-f107c4bebaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('qwen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2257c79e-298b-433c-baf8-9061eccc1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_folder('llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54f5af86-1437-4f37-ac49-68c167f441ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.environ['BQUANT_USERNAME']\n",
    "username_folder = 'tmp/fs'\n",
    "\n",
    "def save_folder_to_s3(name):\n",
    "    client = boto3.client(\"s3\")\n",
    "    bucket = os.environ['BQUANT_SANDBOX_USER_BUCKET']\n",
    "    \n",
    "    files = os.listdir(name)\n",
    "    for file in files:\n",
    "        local_path = f'{name}/{file}'\n",
    "        obj_name = f'{username}/{username_folder}/{name}/{file}'\n",
    "        res = client.upload_file(local_path, bucket, obj_name)\n",
    "    print(res)\n",
    "    \n",
    "# Need to clear the files from local drive after downloading the model\n",
    "def clear_folder(name):\n",
    "    for root, dirs, files in os.walk(name, topdown=False):\n",
    "        for name in files:\n",
    "            os.remove(os.path.join(root, name))\n",
    "        for name in dirs:\n",
    "            os.rmdir(os.path.join(root, name))\n",
    "            \n",
    "def list_model_files(model_name):\n",
    "    client = boto3.client(\"s3\")\n",
    "    bucket = os.environ['BQUANT_SANDBOX_USER_BUCKET']\n",
    "    folder = f'{username}/{username_folder}/{model_name}'\n",
    "    \n",
    "    files = []\n",
    "    for file in client.list_objects(Bucket=bucket, Prefix=folder)['Contents']:\n",
    "        key = file['Key']\n",
    "        files.append(key)\n",
    "    return files\n",
    "\n",
    "                      \n",
    "# re-load the model from s3\n",
    "def load_model(model_name):\n",
    "    client = boto3.client(\"s3\")\n",
    "    bucket = os.environ['BQUANT_SANDBOX_USER_BUCKET']\n",
    "    folder = f'{username}/{username_folder}/{model_name}'\n",
    "    \n",
    "    if not os.path.exists(model_name):\n",
    "        os.makedirs(model_name)\n",
    "        \n",
    "    for file in client.list_objects(Bucket=bucket, Prefix=folder)['Contents']:\n",
    "        key = file['Key']\n",
    "        file_name = model_name + '/' + key[key.find(model_name + '/') + len(model_name) + 1:]\n",
    "        client.download_file(bucket, key, file_name)\n",
    "    return AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.bfloat16 )\n",
    "\n",
    "def delete_model_in_s3(model_name):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1e9ba40-9f88-45d1-a07c-1bd148ac86f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "save_folder_to_s3('qwen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22a24959-4c19-4ec3-94ac-85d1b6b75889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0023df403c094bfd98b9353480d170d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen = s3_model.S3ModelHelper(s3_sub_folder='tmp/fs')\n",
    "qwen.load_model('qwen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "323de05f-e25e-4cdb-a66f-277f70316b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen.clear_folder('qwen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c243569-8790-4724-a048-57442af067a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37504e384aa4ffb8b9358a3bbfe3391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = load_model('llama')\n",
    "clear_folder('llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "825751a8-9a9d-44d3-bbfd-657cfb20ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_folder('qwen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddad3183-c882-480b-af73-5331d108e556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bclarke16/tmp/fs/llama/config.json',\n",
       " 'bclarke16/tmp/fs/llama/generation_config.json',\n",
       " 'bclarke16/tmp/fs/llama/model-00001-of-00002.safetensors',\n",
       " 'bclarke16/tmp/fs/llama/model-00002-of-00002.safetensors',\n",
       " 'bclarke16/tmp/fs/llama/model.safetensors.index.json']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_model_files('llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b064520c-9b56-4c47-8046-e878326af078",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\"s3\")\n",
    "s3 = boto3.resource(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2f42c-067c-4dab-8af9-16ecdb034b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = os.environ['BQUANT_SANDBOX_USER_BUCKET']\n",
    "folder = f'{username}/{username_folder}/llama'\n",
    "\n",
    "my_bucket = s3.Bucket(bucket)\n",
    "\n",
    "for file in client.list_objects(Bucket=bucket, Prefix=folder)['Contents']:\n",
    "    key = file['Key']\n",
    "    file_name = key[key.find('llama' + '/') + len('llama') + 1:]\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddbc1f1-848b-404f-b078-b296d92e116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = client.list_objects(Bucket=bucket, Prefix=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e07070-ef7e-4ad2-959d-5b334634acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.download_file(bucket, 'bclarke16/tmp/fs/llama/config.json', 'config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4248891a-9f71-476f-a2b6-daceb1af3b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference set with Llama/ Qwen\n",
    "\n",
    "#Loop through each security, \n",
    "\n",
    "#extract the IS and the BS, combine into a single prompt\n",
    "\n",
    "# convert to json\n",
    "\n",
    "# store in file with the date and security name\n",
    "\n",
    "# upload to cloud\n",
    "\n",
    "#change"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BQuant Python 3",
   "language": "python",
   "name": "remote-jupyterpython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
