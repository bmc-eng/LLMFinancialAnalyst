{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea0b4d2-9a58-426f-a551-07d8d5eceae1",
   "metadata": {},
   "source": [
    "# Run Open Source Inference \n",
    "\n",
    "This notebook runs inference on the open source models using the multi-gpu process implemented in the Huggingface_constructor file. This file requires the llm-base environment to run correctly. The results of the backtest are stored in the Data folder under each model folder. \n",
    "\n",
    "We test with the following open-source models:\n",
    "\n",
    "- Llama 3.2 3B Instruct\n",
    "- Qwen 2.5 7B Instruct\n",
    "- Deepseek R1 Qwen 14B \n",
    "- Qwen 2.5 3B Instruct\n",
    "\n",
    "We also test with a fine-tuned model, the Qwen 2.5 3B model. We test this with the earnings prompt and also with a new EPS estimate prompt before fine-tuning to provide a comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d33e342b-edf8-4ce9-8a44-759d241f1fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prompts\n",
    "import importlib\n",
    "import datetime\n",
    "import torch\n",
    "import json\n",
    "from huggingface_hub import login\n",
    "import constructors.huggingface_strategy as hs\n",
    "\n",
    "import models.model_helper as mh\n",
    "\n",
    "from constructors.huggingface_strategy import HuggingfaceRun\n",
    "from transformers import BitsAndBytesConfig\n",
    "from accelerate import Accelerator, notebook_launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dcf2505-c7f9-440e-91f1-323f2922817a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'constructors.huggingface_strategy' from '/project/constructors/huggingface_strategy.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(prompts)\n",
    "importlib.reload(mh)\n",
    "importlib.reload(hs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3e71f-48ee-483a-b834-17fea3d28a04",
   "metadata": {},
   "source": [
    "### Set up the environment\n",
    "Log into Huggingface and check the number of GPUs available, pytorch version currently loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a89aad1-f53b-4d47-840c-5d6c0556f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into Huggingface with login credentials\n",
    "with open('pass.txt') as p:\n",
    "    hf_login = p.read()\n",
    "    \n",
    "hf_login = hf_login[hf_login.find('=')+1:hf_login.find('\\n')]\n",
    "login(hf_login, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dff8b64b-1dfa-4073-96ba-41f89ceab9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.2.post300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Torch version: {torch.__version__}')\n",
    "#print(f'Device Count: {torch.cuda.device_count()}')\n",
    "import accelerate\n",
    "accelerate.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca868c0b-6b7d-4c03-b4a3-6a01b55c6c5c",
   "metadata": {},
   "source": [
    "### Run 1: Llama 3.2 Earning analysis - Base\n",
    "The first model we test is the smallest one - Llama 3.2 3B model. As a Huggingface model we use the HuggingfaceRun object to create the strategy and produce the list of trades from the run. We must launch in notebook_launcher to use multi-GPUs. \n",
    "\n",
    "Results File: Results/Earnings/results - llama - base - 2025-03-30 896587.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f2ac3bb-0cb4-49a0-9342-af022c765db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the run config\n",
    "run_config = {\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu_blended',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_blended.json'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a56b0b4b-b11d-4b0a-9fb5-6c2da4124f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "ir = hs.HuggingfaceRun(run_name='Llama 3B Earnings',\n",
    "                       model_id='meta-llama/Llama-3.2-3B-Instruct',\n",
    "                       dataset_id='data_quarterly_pit_indu_refresh_blended.json',system_prompt=prompts.SYSTEM_PROMPTS['BASE_EARN'],run_config=run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cff85cc3-7825-4367-b9c5-5858170939f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 6.4 GB\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [26:08,  1.34s/it]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished backtest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [26:45,  1.81s/it]\n",
      "[2025-04-29 21:30:27,302] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 8450 via signal SIGTERM\n",
      "[2025-04-29 21:30:27,303] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 8452 via signal SIGTERM\n",
      "[2025-04-29 21:30:27,304] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 8453 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "# Run the multi-gpu model with notebook_launcher\n",
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0779e2-8a65-4167-99d7-19af14ae0051",
   "metadata": {},
   "source": [
    "### Run 2: Llama 3.2 Earning analysis - Chain of Thought\n",
    "\n",
    "Run the Llama 3.2 model on Chain of Thought prompts. Results in the following folder:\n",
    "\n",
    "Results/Earnings/results - llama - cot - 2025-03-30 332729.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e6b92c1-b6a6-4b8b-a6f1-873c8a146dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'multi-gpu':True,\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_blended.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0970f294-cdf5-4885-b71c-9cf888d13cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "ir = HuggingfaceRun(run_name='Llama 3B Earnings COT',\n",
    "                       model_id='meta-llama/Llama-3.2-3B-Instruct',\n",
    "                       dataset_id='data_quarterly_pit_indu_refresh_blended.json',\n",
    "                       system_prompt=prompts.SYSTEM_PROMPTS['COT_EARN'],run_config=run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bf2d3c2-6e5f-46ca-94b9-bbd515903613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 6.4 GB\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [1:03:35,  4.31s/it]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run in 1:03:43.495022\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-04-21 16:40:45.191930.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [1:03:44,  4.31s/it]\n",
      "[2025-04-21 17:45:30,848] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 2716 via signal SIGTERM\n",
      "[2025-04-21 17:45:30,850] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 2717 via signal SIGTERM\n",
      "[2025-04-21 17:45:30,850] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 2718 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "# Run the multi-gpu model with notebook_launcher\n",
    "notebook_launcher(ir.run, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e5438-aa23-430f-93d8-8f632e1a950d",
   "metadata": {},
   "source": [
    "### Run 3: DeepSeek R1 Qwen 7B Base\n",
    "\n",
    "Results in the following file:\n",
    "\n",
    "Results/Earnings/results - Deepseek - BASE C -2025-04-01 20:43:33.936328.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02500a92-6c4d-47ec-a87f-d36c9e1633f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_s3_loc': 'deepseek7B',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'multi-gpu':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18257919-498e-4d66-80a2-50cf3f6a51d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "ir = HuggingfaceRun(run_name='DeepSeek 7B Earnings Base',\n",
    "                       model_id='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',\n",
    "                       dataset_id='data_quarterly_pit_indu_refresh_blended.json',\n",
    "                       system_prompt=prompts.SYSTEM_PROMPTS['BASE_EARN'],run_config=run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "975b7ad7-6985-46e0-9bc9-656fff07665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 8 GPUs.\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7Bdeepseek7B\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b022893e2fc94874a032fb4fad9b1cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9d96d0a756423aa034b4e0ea289a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70d4f29c86f426b98cf6ae6dc88eba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddc747a4bb142a5aa9e84e20cc68aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21627aaa81a34e4fa858deea9084ee30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e7e38fe7e04341a8bf637fabafab68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4aa290fb5544721a95b943f679664ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533094192bfc41aaae4b24bcc89b5d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7f1eeb40474823835669b8af7ee8b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6de0bd4687443298abd480fe06777f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5.4 GB\n",
      "starting backtest...starting backtest...starting backtest...\n",
      "\n",
      "\n",
      "starting backtest...starting backtest...\n",
      "starting backtest...\n",
      "\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [2:17:03,  9.49s/it]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run in 2:17:37.334527\n",
      "Called Save run\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-03-30 17:44:18.481428.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [2:17:37,  9.30s/it]\n",
      "[2025-03-30 20:02:58,292] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 187 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,294] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 188 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,294] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 189 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,295] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 190 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,296] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 192 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,297] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 193 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,297] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 194 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "# Run the multi-gpu model with notebook_launcher\n",
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b984dbc-7874-46ae-9b84-3e9100f54735",
   "metadata": {},
   "source": [
    "### Run 4: DeepSeek 7B COT\n",
    "\n",
    "Results in the following file:\n",
    "\n",
    "Results/Earnings/results - Deepseek - COT C -2025-04-01 21:13:35.944412.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02aea17c-33e8-4ed7-9fd7-ebcf73b6ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_s3_loc': 'deepseek7B',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'multi-gpu':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd0d50b-6c95-4ce5-b503-12215cfe902d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "ir = HuggingfaceRun(run_name='DeepSeek 7B Earnings COT',\n",
    "                       model_id='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',\n",
    "                       dataset_id='data_quarterly_pit_indu_refresh_blended.json',\n",
    "                       system_prompt=prompts.SYSTEM_PROMPTS['COT_EARN'],run_config=run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae221de2-c71d-49f9-ab74-23876c132ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 8 GPUs.\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11faff2b6a724e0fa15daf54d72a89e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c953bf644c4a9ea4b69bdfcbc5c560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529da691ffcf4c7195160cf33f24b42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e727ab9919754053a8151c3d45a743c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3908dcf8b744a2bac73be9afbb4c079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ab89be3fac4a4e81e1ef1df0271f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9536a6a9c79b4633abc4a15455bdfe55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae19905fff644e583df3ddbd20757d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5.4 GB\n",
      "starting backtest...starting backtest...starting backtest...\n",
      "starting backtest...\n",
      "\n",
      "starting backtest...\n",
      "\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [2:26:10,  9.47s/it]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run in 2:30:12.540523\n",
      "Called Save run\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-03-30 20:11:39.828629.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [2:30:13, 10.15s/it]\n",
      "[2025-03-30 22:42:55,767] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3671 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,768] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3672 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,769] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3673 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,770] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3674 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,771] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3676 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,771] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3677 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3678 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f8ebc-2756-4eb0-a2d9-777fc5fd0bb9",
   "metadata": {},
   "source": [
    "### Qwen 2.5 - 3B Instruct\n",
    "For this model, we apply quantization to reduce the size of the model. This will be used for fine-tuning and so memory footprint needs to be smaller to permit training on a single GPU. We use the Bits and Bytes library from Huggingface to convert to 4bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0be801bc-c018-42b4-98a0-ebacbc024890",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_s3_loc': 'qwen3b',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'multi-gpu':True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb76425a-1690-4998-8448-a4d439685fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "ir = HuggingfaceRun(run_name='Qwen 3B Earnings BASE',\n",
    "                       model_id='Qwen/Qwen2.5-3B-Instruct',\n",
    "                       dataset_id='data_quarterly_pit_indu_refresh_blended.json',\n",
    "                       system_prompt=prompts.SYSTEM_PROMPTS['BASE_EARN'],run_config=run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c54db2c-0ede-4571-a364-90f688cf462f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a49c5eb99a4aa0a64c412aac5b9c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7356268d285344eaa868aa4d99f47aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd551e637b8e4f4bbce3fecb98499033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d7b510b3d04566a56e531e0043158e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 2.0 GB\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [16:02,  1.09s/it]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run in 0:16:02.309246\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-04-29 16:17:41.890187.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [16:02,  1.08s/it]\n",
      "[2025-04-29 16:34:21,064] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 318 via signal SIGTERM\n",
      "[2025-04-29 16:34:21,065] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 319 via signal SIGTERM\n",
      "[2025-04-29 16:34:21,066] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 320 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4dfbc32-0c0b-4b6b-874a-c897f70652fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_s3_loc': 'qwen3b',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'multi-gpu':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c6c33e5-606d-4d16-8e20-ce449c2e5c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "ir = HuggingfaceRun(run_name='Qwen 3B Earnings COT',\n",
    "                       model_id='Qwen/Qwen2.5-3B-Instruct',\n",
    "                       dataset_id='data_quarterly_pit_indu_refresh_blended.json',\n",
    "                       system_prompt=prompts.SYSTEM_PROMPTS['COT_EARN'],run_config=run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea7dfd49-e0cd-4fd4-8543-31edb9843aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "qwen3b\n",
      "qwen3b\n",
      "qwen3b\n",
      "qwen3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 2.0 GB\n",
      "starting backtest...\n",
      "starting backtest...starting backtest...\n",
      "\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [13:42,  1.09it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run in 0:13:42.324489\n",
      "Called Save run\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-04-03 20:06:52.680533.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [13:42,  1.08it/s]\n",
      "[2025-04-03 20:21:16,801] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 1305 via signal SIGTERM\n",
      "[2025-04-03 20:21:16,802] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 1306 via signal SIGTERM\n",
      "[2025-04-03 20:21:16,803] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 1307 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a613d5-ea61-40bc-bc26-4b4ef60f95f4",
   "metadata": {},
   "source": [
    "## Run 5 - Fine tuned model - EPS only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a234749-8d26-453d-bb1d-af522d694cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_hf_id': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "    'model_s3_loc': 'qwen3b',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE_FINE_TUNED'],\n",
    "    'multi-gpu':False,\n",
    "    'dataset': 'data_quarterly_pit_indu_blended_base',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_blended.json',\n",
    "    'fine_tuned_dir': 'fine_tuned2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ade0f00-1ec6-4f37-9728-00d1cfc9e966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "ftm = mft.FineTunedInference(\"EPS only FT Run\", run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ab2e96-94cf-4399-b0fd-99ee62889995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "prompt_set = ftm.create_all_prompts(force_refresh=True, is_save_prompts=True)\n",
    "prompt_set_appended = ftm.reformat_prompts(prompt_set, \"The next period EPS is \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "716cb386-58e6-4993-926c-9df66b320455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 887/887 [1:13:27<00:00,  4.97s/it]\n"
     ]
    }
   ],
   "source": [
    "output_eps = ftm.run_finetuned_backtest(prompt_set_appended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "124420f5-f726-4560-89d3-7a51ac328414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the output\n",
    "with open(f\"Results/Earnings/results - Qwen3B Finetuned - EPS only.json\", 'w') as f:\n",
    "    json.dump(output_eps, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680e151-9474-4e6f-af18-fa4bda30783d",
   "metadata": {},
   "source": [
    "### Qwen Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83378e09-b3ea-462a-ab46-fc23a02ea6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = HuggingfaceRun(run_name='Qwen Earnings',\n",
    "                       model_id='Qwen/Qwen2.5-3B-Instruct',\n",
    "                       dataset_id='data_quarterly_pit_indu_refresh_blended.json',\n",
    "                       system_prompt=prompts.SYSTEM_PROMPTS['BASE_FINE_TUNED'],run_config=run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b48ff569-21b3-4a55-aa5e-b4892f60114b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe160210-08ed-4815-896d-0faa1e3c8378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14c3aeb0db944e188cdea31dad3af9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39f6890c2c94932ad7c3bb9b7b1b674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d370c71f5c8549f4b7d21492a4e26287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f49cc31dfd4a41af5ebb08ce5f88f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 2.0 GB\n",
      "starting backtest...\n",
      "starting backtest...starting backtest...\n",
      "\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [1:15:25,  4.15s/it]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished backtest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [1:16:07,  5.14s/it]\n",
      "[2025-05-01 08:44:30,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 190 via signal SIGTERM\n",
      "[2025-05-01 08:44:30,046] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 191 via signal SIGTERM\n",
      "[2025-05-01 08:44:30,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 193 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9f29a94-3def-4285-871e-c23dc76f252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ir.cached_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d84c69-7826-4335-befe-48cadaef00fd",
   "metadata": {},
   "source": [
    "## Stock Recommendation Runs\n",
    "\n",
    "This section now runs for each of the Stock Price Recommendation runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c59e02-d15c-4ff0-a4ea-bf7312deb6b6",
   "metadata": {},
   "source": [
    "### Run 1 - Llama 3B\n",
    "\n",
    "Results from this run: Results/ Stock Price/results - 2025-02-23 13:59:33.728956.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09b25bb3-cc8d-4cb1-b939-7ab19e66df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'multi-gpu':True,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "936cb54c-f0d2-4b4d-ad21-abe4d7c8bf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "ir = hs.HuggingfaceRun(run_name='Llama 3B Recommendation - BASE',\n",
    "                       model_id='meta-llama/Llama-3.2-3B-Instruct',\n",
    "                       dataset_id='data_quarterly_pit_indu.json',system_prompt=prompts.SYSTEM_PROMPTS['BASE'],run_config=run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe305a91-6533-4607-a825-6ee42c33217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/896 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 6.4 GB\n",
      "starting backtest...starting backtest...\n",
      "\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 896/896 [43:12<00:00,  2.56s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished backtest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 896/896 [44:27<00:00,  2.98s/it]\n",
      "[2025-05-01 12:12:55,066] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 1223 via signal SIGTERM\n",
      "[2025-05-01 12:12:55,068] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 1224 via signal SIGTERM\n",
      "[2025-05-01 12:12:55,068] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 1225 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d834b7ab-2c78-4ea5-bcde-01095f79c617",
   "metadata": {},
   "source": [
    "### Run 2 - Llama 3B CoT\n",
    "\n",
    "Results from this run: Results/Stock Price/results - 2025-02-23 14:47:14.366854.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e973a9c-463b-40d9-9069-d0518ddc3281",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = hs.HuggingfaceRun(run_name='Llama 3B Recommendation - COT',\n",
    "                       model_id='meta-llama/Llama-3.2-3B-Instruct',\n",
    "                       dataset_id='data_quarterly_pit_indu.json',system_prompt=prompts.SYSTEM_PROMPTS['COT'],run_config=run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79757707-3d17-4116-a734-cecc1cbe2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1abc71-464e-4293-add6-f475fcb1d228",
   "metadata": {},
   "source": [
    "### Run 3 - DeepSeek 14B\n",
    "\n",
    "Results from this run: Results/Stock Price/results - 2025-02-23 15:39:43.667370.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c83b27-81ae-4896-9fee-018274508c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_s3_loc': 'deepseek14Q',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'multi-gpu':True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c647b3-55e5-4fcb-9105-a3a3bf29ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = hs.HuggingfaceRun(run_name='DeepSeek 14B Recommendation - BASE',\n",
    "                       model_id='deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "                       dataset_id='data_quarterly_pit_indu.json',system_prompt=prompts.SYSTEM_PROMPTS['BASE'],run_config=run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad68702b-39bd-4ee0-8296-8514e07bbbd5",
   "metadata": {},
   "source": [
    "### Run 4 - DeepSeek 14B - COT\n",
    "\n",
    "Results from this run: Results/Stock Price/results - 2025-02-23 20:25:09.495607.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6a2cc-645e-4256-a92e-49a4a284d255",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = hs.HuggingfaceRun(run_name='DeepSeek 14B Recommendation - COT',\n",
    "                       model_id='deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "                       dataset_id='data_quarterly_pit_indu.json',system_prompt=prompts.SYSTEM_PROMPTS['COT'],run_config=run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f2d06f-6f7e-4e51-a70b-5331d57ec257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4da0d04d-a9d3-42d7-87b9-1ba3e1d5027b",
   "metadata": {},
   "source": [
    "### Unused Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eade0d8d-ba3b-4706-a46a-9e76a77ec445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old Code\n",
    "# run_name = f\"{run_config['model_s3_loc']}_{run_config['dataset']}_finetuned\"\n",
    "# ir = model_inference.InferenceRun(run_name, run_config)\n",
    "\n",
    "# # Create the prompts and save to the Data folder\n",
    "# prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)\n",
    "# for prompt in prompt_set:\n",
    "#     prompt['prompt'] += 'The next period EPS is '\n",
    "#     #prompt['prompt'] += \"\\nAnswer in JSON format with the next period EPS, the direction, the magnitude and a confidence.\"\n",
    "\n",
    "# from peft import PeftModel\n",
    "# from transformers import AutoTokenizer\n",
    "# import json\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# outputs = []\n",
    "# def run_finetuned_backtest(prompts, fine_tuned_model, tokenizer):\n",
    "#     count = 0\n",
    "#     progress = tqdm(total=len(prompts), position=0, leave=True)\n",
    "#     for prompt in prompts:\n",
    "#         tokens = tokenizer.apply_chat_template(prompt['prompt'], tokenize=False, add_generation_prompt=True )\n",
    "#         print(tokens)\n",
    "#         model_inputs = tokenizer([tokens], return_tensors='pt').to(\"cuda\")\n",
    "#         generated_ids = fine_tuned_model.generate(**model_inputs, \n",
    "#                                        pad_token_id=tokenizer.eos_token_id, \n",
    "#                                        max_new_tokens=50,\n",
    "#                                       temperature=0.001)\n",
    "#         parsed_ids = [\n",
    "#             output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "#         ]\n",
    "#         resp = {\n",
    "#             'date': prompt['date'],\n",
    "#             'security': prompt['security'],\n",
    "#             'response': tokenizer.batch_decode(parsed_ids, skip_special_tokens=True)[0]\n",
    "#         }\n",
    "#         outputs.append(resp)\n",
    "#         progress.update()\n",
    "\n",
    "# # Load the base model \n",
    "# model_helper = mh.ModelHelper('tmp/fs')\n",
    "# base_model = ir.load_model_from_storage(run_config['model_s3_loc'])\n",
    "# # clear the local folder once completed loading into memory\n",
    "# model_helper.clear_folder(run_config['model_s3_loc'])\n",
    "\n",
    "# # Update the base model with the fine-tuned modules\n",
    "# fine_tuned_model = PeftModel.from_pretrained(base_model, 'fine_tuned2')\n",
    "# # Create the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(run_config['model_hf_id'])\n",
    "# run_finetuned_backtest(prompt_set[:2], fine_tuned_model, tokenizer)\n",
    "\n",
    "# # save the output\n",
    "# with open(f\"Results/Earnings/results - Qwen3B Finetuned - EPS only.json\", 'w') as f:\n",
    "#     json.dump(outputs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bloomberg Lab Python 3",
   "language": "python",
   "name": "remote-jupyterpython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
