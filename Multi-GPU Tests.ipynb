{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2fe0fc3-b648-4a0e-8a2a-0e767c6074c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "with open('pass.txt') as p:\n",
    "    hf_login = p.read()\n",
    "    \n",
    "hf_login = hf_login[hf_login.find('=')+1:hf_login.find('\\n')]\n",
    "login(hf_login, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f88ca549-96dc-47b0-8831-22500e500475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547ad1c1a4964451acea4adab9a65cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10f5a73a12a49558d24dddb637695de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58135fdc977748d6a6b87688112a7b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16878bdbead946599c3b703898a41a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837650d0f8ff4da3bbb55e6ce3aeb9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe0d0fbb4234151bc23dda8367d197d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from statistics import mean\n",
    "import torch, time, json\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# 10*10 Prompts. Source: https://www.penguin.co.uk/articles/2022/04/best-first-lines-in-books\n",
    "prompts_all=[\n",
    "    \"The King is dead. Long live the Queen.\",\n",
    "    \"Once there were four children whose names were Peter, Susan, Edmund, and Lucy.\",\n",
    "    \"The story so far: in the beginning, the universe was created.\",\n",
    "    \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
    "    \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\",\n",
    "    \"The sweat wis lashing oafay Sick Boy; he wis trembling.\",\n",
    "    \"124 was spiteful. Full of Baby's venom.\",\n",
    "    \"As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect.\",\n",
    "    \"I write this sitting in the kitchen sink.\",\n",
    "    \"We were somewhere around Barstow on the edge of the desert when the drugs began to take hold.\",\n",
    "] * 10\n",
    "\n",
    "# load a base model and tokenizer\n",
    "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,    \n",
    "    device_map={\"\": accelerator.process_index},\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c82280ed-194b-4880-ace5-bfa2956e0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_all= [\n",
    "    {\"role\": \"system\", \"content\": \"You are a financial analyst and must make a buy, sell or hold decision on a company based only on the provided datasets. \\\n",
    "        Compute common financial ratios and then determine the buy sell decision. Explain your reasons and answer in a format that compiles to a JSON object.\\\n",
    "        Answer as a JSON string with the following example format: \\\n",
    "        {'Investment Decision': BUY, 'Reason': 'Gross profit and EPS have both increased over time'}\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"                                                          t           t-1           t-2           t-3           t-4           t-5\n",
    "level_1                                                                                                                           \n",
    "01 Revenue (Adj)                                2.960000e+09  3.357800e+09  2.739700e+09  2.662700e+09  1.897400e+09  2.366300e+09\n",
    "05 Cost of Revenue (Adj)                        1.743700e+09  2.212400e+09  1.695000e+09  1.692700e+09  9.701000e+08  1.477500e+09\n",
    "08 Gross Profit (Adj)                           1.216300e+09  1.145400e+09  1.044700e+09  9.700000e+08  9.273000e+08  8.888000e+08\n",
    "10 Operating Expenses (Adj)                     1.010700e+09  9.740000e+08  8.744000e+08  7.806000e+08  7.339000e+08  6.857000e+08\n",
    "14 Operating Income or Losses (Adj)             2.056000e+08  1.714000e+08  1.703000e+08  1.894000e+08  1.934000e+08  2.031000e+08\n",
    "17 Interest Expense (Adj)                       4.910000e+07  4.680000e+07  4.120000e+07  3.730000e+07  3.850000e+07  4.600000e+07\n",
    "18 Interest Income (Adj)                        9.000000e+06  6.000000e+06  2.300000e+06  0.000000e+00  0.000000e+00  0.000000e+00\n",
    "21 Pretax Income (Loss), Adjusted (Adj)         1.741000e+08  1.710000e+08  1.053000e+08  1.694000e+08  1.856000e+08  1.812000e+08\n",
    "22 Abnormal Losses (Gains)                      3.100000e+06 -4.730000e+07  3.260000e+07 -2.370000e+07  0.000000e+00  0.000000e+00\n",
    "28 Pretax Income (Loss), GAAP                   1.741000e+08  1.710000e+08  1.053000e+08  1.694000e+08  1.856000e+08  1.812000e+08\n",
    "29 Income Tax Expense (Benefit)                 4.580000e+07  3.470000e+07  3.020000e+07  5.960000e+07  5.760000e+07  5.910000e+07\n",
    "32 Income (Loss) from Continuing Operations     1.283000e+08  1.363000e+08  7.510000e+07  1.098000e+08  1.280000e+08  1.221000e+08\n",
    "33 Net Extraordinary Losses (Gains)             0.000000e+00  0.000000e+00  0.000000e+00  4.500000e+06  0.000000e+00  0.000000e+00\n",
    "36 Net Income Including Minority Interest       1.283000e+08  1.363000e+08  7.510000e+07  1.053000e+08  1.280000e+08  1.221000e+08\n",
    "37 Net Income/Net Profit (Losses)               1.283000e+08  1.363000e+08  7.510000e+07  1.053000e+08  1.280000e+08  1.221000e+08\n",
    "38 Preferred Dividends                          0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+05  2.000000e+05  3.000000e+05\n",
    "40 Net Income Avail to Common, GAAP             1.283000e+08  1.363000e+08  7.510000e+07  1.052000e+08  1.278000e+08  1.218000e+08\n",
    "41 Net Income Avail to Common, Adj (Adj)        1.283000e+08  1.363000e+08  7.510000e+07  1.052000e+08  1.278000e+08  1.218000e+08\n",
    "43 Net Extraordinary Losses (Gains)             0.000000e+00  0.000000e+00  0.000000e+00  4.500000e+06  0.000000e+00  0.000000e+00\n",
    "44 Basic Weighted Average Number of Shares      4.460000e+07  4.420000e+07  4.410000e+07  4.400000e+07  4.410000e+07  4.510000e+07\n",
    "45 Basic Earnings per Share                     2.880000e+00  3.080000e+00  1.710000e+00  2.390000e+00  2.900000e+00  2.700000e+00\n",
    "46 Basic EPS from Continuing Operations         2.880000e+00  3.080000e+00  1.710000e+00  2.490000e+00  2.900000e+00  2.700000e+00\n",
    "47 Basic EPS from Continuing Operations         3.000000e+00  2.290000e+00  2.180000e+00  2.140000e+00  2.900000e+00  2.700000e+00\n",
    "48 Diluted Weighted Average Shares              4.470000e+07  4.440000e+07  4.430000e+07  4.420000e+07  4.430000e+07  4.520000e+07\n",
    "49 Diluted EPS                                  2.870000e+00  3.070000e+00  1.700000e+00  2.380000e+00  2.880000e+00  2.690000e+00\n",
    "50 Diluted EPS from Continuing Operations       2.870000e+00  3.070000e+00  1.700000e+00  2.480000e+00  2.880000e+00  2.690000e+00\n",
    "51 Diluted EPS from Continuing Operations, Adj  2.990000e+00  2.280000e+00  2.170000e+00  2.130000e+00  2.880000e+00  2.690000e+0\"\"\"}\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6005d661-c3e6-4bac-9cbe-68a95dc5a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"You are a financial analyst and must make a buy, sell or hold decision on a company based only on the provided datasets.         Compute common financial ratios and then determine the buy sell decision. Explain your reasons and answer in a format that compiles to a JSON object.        Answer as a JSON string with the following example format:         {'Investment Decision': BUY, 'Reason': 'Gross profit and EPS have both increased over time'}                                                          t           t-1           t-2           t-3           t-4           t-5\\nlevel_1                                                                                                                           \\n01 Revenue (Adj)                                2.960000e+09  3.357800e+09  2.739700e+09  2.662700e+09  1.897400e+09  2.366300e+09\\n05 Cost of Revenue (Adj)                        1.743700e+09  2.212400e+09  1.695000e+09  1.692700e+09  9.701000e+08  1.477500e+09\\n08 Gross Profit (Adj)                           1.216300e+09  1.145400e+09  1.044700e+09  9.700000e+08  9.273000e+08  8.888000e+08\\n10 Operating Expenses (Adj)                     1.010700e+09  9.740000e+08  8.744000e+08  7.806000e+08  7.339000e+08  6.857000e+08\\n14 Operating Income or Losses (Adj)             2.056000e+08  1.714000e+08  1.703000e+08  1.894000e+08  1.934000e+08  2.031000e+08\\n17 Interest Expense (Adj)                       4.910000e+07  4.680000e+07  4.120000e+07  3.730000e+07  3.850000e+07  4.600000e+07\\n18 Interest Income (Adj)                        9.000000e+06  6.000000e+06  2.300000e+06  0.000000e+00  0.000000e+00  0.000000e+00\\n21 Pretax Income (Loss), Adjusted (Adj)         1.741000e+08  1.710000e+08  1.053000e+08  1.694000e+08  1.856000e+08  1.812000e+08\\n22 Abnormal Losses (Gains)                      3.100000e+06 -4.730000e+07  3.260000e+07 -2.370000e+07  0.000000e+00  0.000000e+00\\n28 Pretax Income (Loss), GAAP                   1.741000e+08  1.710000e+08  1.053000e+08  1.694000e+08  1.856000e+08  1.812000e+08\\n29 Income Tax Expense (Benefit)                 4.580000e+07  3.470000e+07  3.020000e+07  5.960000e+07  5.760000e+07  5.910000e+07\\n32 Income (Loss) from Continuing Operations     1.283000e+08  1.363000e+08  7.510000e+07  1.098000e+08  1.280000e+08  1.221000e+08\\n33 Net Extraordinary Losses (Gains)             0.000000e+00  0.000000e+00  0.000000e+00  4.500000e+06  0.000000e+00  0.000000e+00\\n36 Net Income Including Minority Interest       1.283000e+08  1.363000e+08  7.510000e+07  1.053000e+08  1.280000e+08  1.221000e+08\\n37 Net Income/Net Profit (Losses)               1.283000e+08  1.363000e+08  7.510000e+07  1.053000e+08  1.280000e+08  1.221000e+08\\n38 Preferred Dividends                          0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+05  2.000000e+05  3.000000e+05\\n40 Net Income Avail to Common, GAAP             1.283000e+08  1.363000e+08  7.510000e+07  1.052000e+08  1.278000e+08  1.218000e+08\\n41 Net Income Avail to Common, Adj (Adj)        1.283000e+08  1.363000e+08  7.510000e+07  1.052000e+08  1.278000e+08  1.218000e+08\\n43 Net Extraordinary Losses (Gains)             0.000000e+00  0.000000e+00  0.000000e+00  4.500000e+06  0.000000e+00  0.000000e+00\\n44 Basic Weighted Average Number of Shares      4.460000e+07  4.420000e+07  4.410000e+07  4.400000e+07  4.410000e+07  4.510000e+07\\n45 Basic Earnings per Share                     2.880000e+00  3.080000e+00  1.710000e+00  2.390000e+00  2.900000e+00  2.700000e+00\\n46 Basic EPS from Continuing Operations         2.880000e+00  3.080000e+00  1.710000e+00  2.490000e+00  2.900000e+00  2.700000e+00\\n47 Basic EPS from Continuing Operations         3.000000e+00  2.290000e+00  2.180000e+00  2.140000e+00  2.900000e+00  2.700000e+00\\n48 Diluted Weighted Average Shares              4.470000e+07  4.440000e+07  4.430000e+07  4.420000e+07  4.430000e+07  4.520000e+07\\n49 Diluted EPS                                  2.870000e+00  3.070000e+00  1.700000e+00  2.380000e+00  2.880000e+00  2.690000e+00\\n50 Diluted EPS from Continuing Operations       2.870000e+00  3.070000e+00  1.700000e+00  2.480000e+00  2.880000e+00  2.690000e+00\\n51 Diluted EPS from Continuing Operations, Adj  2.990000e+00  2.280000e+00  2.170000e+00  2.130000e+00  2.880000e+00  2.690000e+0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b299fc9-1c73-4a54-88f9-6b452ee7e732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"You are a financial analyst and must make a buy, sell or hold decision on a company based only on the provided datasets.         Compute common financial ratios and then determine the buy sell decision. Explain your reasons and answer in a format that compiles to a JSON object.        Answer as a JSON string with the following example format:         {'Investment Decision': BUY, 'Reason': 'Gross profit and EPS have both increased over time'}\"},\n",
       " {'role': 'user',\n",
       "  'content': '                                                          t           t-1           t-2           t-3           t-4           t-5\\nlevel_1                                                                                                                           \\n01 Revenue (Adj)                                2.960000e+09  3.357800e+09  2.739700e+09  2.662700e+09  1.897400e+09  2.366300e+09\\n05 Cost of Revenue (Adj)                        1.743700e+09  2.212400e+09  1.695000e+09  1.692700e+09  9.701000e+08  1.477500e+09\\n08 Gross Profit (Adj)                           1.216300e+09  1.145400e+09  1.044700e+09  9.700000e+08  9.273000e+08  8.888000e+08\\n10 Operating Expenses (Adj)                     1.010700e+09  9.740000e+08  8.744000e+08  7.806000e+08  7.339000e+08  6.857000e+08\\n14 Operating Income or Losses (Adj)             2.056000e+08  1.714000e+08  1.703000e+08  1.894000e+08  1.934000e+08  2.031000e+08\\n17 Interest Expense (Adj)                       4.910000e+07  4.680000e+07  4.120000e+07  3.730000e+07  3.850000e+07  4.600000e+07\\n18 Interest Income (Adj)                        9.000000e+06  6.000000e+06  2.300000e+06  0.000000e+00  0.000000e+00  0.000000e+00\\n21 Pretax Income (Loss), Adjusted (Adj)         1.741000e+08  1.710000e+08  1.053000e+08  1.694000e+08  1.856000e+08  1.812000e+08\\n22 Abnormal Losses (Gains)                      3.100000e+06 -4.730000e+07  3.260000e+07 -2.370000e+07  0.000000e+00  0.000000e+00\\n28 Pretax Income (Loss), GAAP                   1.741000e+08  1.710000e+08  1.053000e+08  1.694000e+08  1.856000e+08  1.812000e+08\\n29 Income Tax Expense (Benefit)                 4.580000e+07  3.470000e+07  3.020000e+07  5.960000e+07  5.760000e+07  5.910000e+07\\n32 Income (Loss) from Continuing Operations     1.283000e+08  1.363000e+08  7.510000e+07  1.098000e+08  1.280000e+08  1.221000e+08\\n33 Net Extraordinary Losses (Gains)             0.000000e+00  0.000000e+00  0.000000e+00  4.500000e+06  0.000000e+00  0.000000e+00\\n36 Net Income Including Minority Interest       1.283000e+08  1.363000e+08  7.510000e+07  1.053000e+08  1.280000e+08  1.221000e+08\\n37 Net Income/Net Profit (Losses)               1.283000e+08  1.363000e+08  7.510000e+07  1.053000e+08  1.280000e+08  1.221000e+08\\n38 Preferred Dividends                          0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+05  2.000000e+05  3.000000e+05\\n40 Net Income Avail to Common, GAAP             1.283000e+08  1.363000e+08  7.510000e+07  1.052000e+08  1.278000e+08  1.218000e+08\\n41 Net Income Avail to Common, Adj (Adj)        1.283000e+08  1.363000e+08  7.510000e+07  1.052000e+08  1.278000e+08  1.218000e+08\\n43 Net Extraordinary Losses (Gains)             0.000000e+00  0.000000e+00  0.000000e+00  4.500000e+06  0.000000e+00  0.000000e+00\\n44 Basic Weighted Average Number of Shares      4.460000e+07  4.420000e+07  4.410000e+07  4.400000e+07  4.410000e+07  4.510000e+07\\n45 Basic Earnings per Share                     2.880000e+00  3.080000e+00  1.710000e+00  2.390000e+00  2.900000e+00  2.700000e+00\\n46 Basic EPS from Continuing Operations         2.880000e+00  3.080000e+00  1.710000e+00  2.490000e+00  2.900000e+00  2.700000e+00\\n47 Basic EPS from Continuing Operations         3.000000e+00  2.290000e+00  2.180000e+00  2.140000e+00  2.900000e+00  2.700000e+00\\n48 Diluted Weighted Average Shares              4.470000e+07  4.440000e+07  4.430000e+07  4.420000e+07  4.430000e+07  4.520000e+07\\n49 Diluted EPS                                  2.870000e+00  3.070000e+00  1.700000e+00  2.380000e+00  2.880000e+00  2.690000e+00\\n50 Diluted EPS from Continuing Operations       2.870000e+00  3.070000e+00  1.700000e+00  2.480000e+00  2.880000e+00  2.690000e+00\\n51 Diluted EPS from Continuing Operations, Adj  2.990000e+00  2.280000e+00  2.170000e+00  2.130000e+00  2.880000e+00  2.690000e+0'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "137933f0-6624-4ea7-8dae-02391959c80d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedError",
     "evalue": "dict object has no element 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUndefinedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# have each GPU do inference, prompt by prompt\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m---> 12\u001b[0m     prompt_tokenized\u001b[38;5;241m=\u001b[39m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;66;03m#(prompt, return_tensors=\"pt\").to(\"cuda\")\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     output_tokenized \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_tokenized, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# remove prompt from output \u001b[39;00m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1683\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     all_generation_indices\u001b[38;5;241m.\u001b[39mappend(generation_indices)\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m     rendered_chat \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_schemas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtemplate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m continue_final_message:\n\u001b[1;32m   1691\u001b[0m     final_message \u001b[38;5;241m=\u001b[39m chat[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/jinja2/environment.py:1304\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1304\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/jinja2/environment.py:939\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[0;32m--> 939\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[0;32m<template>:20\u001b[0m, in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/jinja2/sandbox.py:304\u001b[0m, in \u001b[0;36mSandboxedEnvironment.getitem\u001b[0;34m(self, obj, argument)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Subscribe an object from sandboxed code.\"\"\"\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[43margument\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mLookupError\u001b[39;00m):\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(argument, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;31mUndefinedError\u001b[0m: dict object has no element 0"
     ]
    }
   ],
   "source": [
    "# sync GPUs and start the timer\n",
    "accelerator.wait_for_everyone()\n",
    "start=time.time()\n",
    "\n",
    "# divide the prompt list onto the available GPUs \n",
    "with accelerator.split_between_processes(prompts_all) as prompts:\n",
    "    # store output of generations in dict\n",
    "    results=dict(outputs=[], num_tokens=0)\n",
    "\n",
    "    # have each GPU do inference, prompt by prompt\n",
    "    for prompt in prompts:\n",
    "        prompt_tokenized=tokenizer.apply_chat_template(prompt, tokenize=True, add_generation_prompt=True, return_tensors='pt').to(\"cuda\")#(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        output_tokenized = model.generate(**prompt_tokenized, max_new_tokens=200)[0]\n",
    "\n",
    "        # remove prompt from output \n",
    "        output_tokenized=output_tokenized[len(prompt_tokenized[\"input_ids\"][0]):]\n",
    "\n",
    "        # store outputs and number of tokens in result{}\n",
    "        results[\"outputs\"].append( tokenizer.decode(output_tokenized) )\n",
    "        results[\"num_tokens\"] += len(output_tokenized)\n",
    "\n",
    "    results=[ results ] # transform to list, otherwise gather_object() will not collect correctly\n",
    "\n",
    "# collect results from all the GPUs\n",
    "results_gathered=gather_object(results)\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    timediff=time.time()-start\n",
    "    num_tokens=sum([r[\"num_tokens\"] for r in results_gathered ])\n",
    "\n",
    "    print(f\"tokens/sec: {num_tokens//timediff}, time {timediff}, total tokens {num_tokens}, total prompts {len(prompts_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cc87073-c71a-4637-b0cf-07ef2dd1fca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" I am trying to remember the date of the last time I saw my friend and I wanted to know if you could help me.\\n\\nI am trying to remember the date of the last time I saw my friend. You know, the one we met in the mall last year. I'm pretty sure it was around the time the new iPhone was released. I remember we went to the Apple store together to see the new model and then we went to dinner at that new Italian place that just opened up downtown.\\n\\nI'm trying to figure out what date it was. Let me think for a moment... Let's see, I think it was around the 15th of June. Is that correct? Or was it the 20th? I'm really sorry, I'm getting a little mixed up. I know it was around that time, but I'm not sure which day.\\n\\nPlease let me know if you could help me figure out the date of the last time I saw my friend. I would\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_gathered[0]['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e935a8-a81d-408e-a139-3842813c86f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from statistics import mean\n",
    "import torch, time, json\n",
    "\n",
    "def write_pretty_json(file_path, data):\n",
    "    import json\n",
    "    with open(file_path, \"w\") as write_file:\n",
    "        json.dump(data, write_file, indent=4)\n",
    "\n",
    "# 10*10 Prompts. Source: https://www.penguin.co.uk/articles/2022/04/best-first-lines-in-books\n",
    "prompts_all=[\n",
    "    \"The King is dead. Long live the Queen.\",\n",
    "    \"Once there were four children whose names were Peter, Susan, Edmund, and Lucy.\",\n",
    "    \"The story so far: in the beginning, the universe was created.\",\n",
    "    \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
    "    \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\",\n",
    "    \"The sweat wis lashing oafay Sick Boy; he wis trembling.\",\n",
    "    \"124 was spiteful. Full of Baby's venom.\",\n",
    "    \"As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect.\",\n",
    "    \"I write this sitting in the kitchen sink.\",\n",
    "    \"We were somewhere around Barstow on the edge of the desert when the drugs began to take hold.\",\n",
    "] * 10\n",
    "\n",
    "# load a base model and tokenizer\n",
    "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)   \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# batch, left pad (for inference), and tokenize\n",
    "def prepare_prompts(prompts, tokenizer, batch_size=16):\n",
    "    batches=[prompts[i:i + batch_size] for i in range(0, len(prompts), batch_size)]  \n",
    "    batches_tok=[]\n",
    "    tokenizer.padding_side=\"left\"     \n",
    "    for prompt_batch in batches:\n",
    "        batches_tok.append(\n",
    "            tokenizer(\n",
    "                prompt_batch, \n",
    "                return_tensors=\"pt\", \n",
    "                padding='longest', \n",
    "                truncation=False, \n",
    "                pad_to_multiple_of=8,\n",
    "                add_special_tokens=False).to(\"cuda\") \n",
    "            )\n",
    "    tokenizer.padding_side=\"right\"\n",
    "    return batches_tok\n",
    "\n",
    "def run_inference():\n",
    "    # sync GPUs and start the timer\n",
    "    accelerator = Accelerator()\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,    \n",
    "        device_map={\"\": accelerator.process_index},\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    accelerator.wait_for_everyone()    \n",
    "    start=time.time()\n",
    "\n",
    "    # divide the prompt list onto the available GPUs \n",
    "    with accelerator.split_between_processes(prompts_all) as prompts:\n",
    "        results=dict(outputs=[], num_tokens=0)\n",
    "\n",
    "        # have each GPU do inference in batches\n",
    "        prompt_batches=prepare_prompts(prompts, tokenizer, batch_size=16)\n",
    "\n",
    "        for prompts_tokenized in prompt_batches:\n",
    "            outputs_tokenized=model.generate(**prompts_tokenized, max_new_tokens=100)\n",
    "\n",
    "            # remove prompt from gen. tokens\n",
    "            outputs_tokenized=[ tok_out[len(tok_in):] \n",
    "                for tok_in, tok_out in zip(prompts_tokenized[\"input_ids\"], outputs_tokenized) ] \n",
    "\n",
    "            # count and decode gen. tokens \n",
    "            num_tokens=sum([ len(t) for t in outputs_tokenized ])\n",
    "            outputs=tokenizer.batch_decode(outputs_tokenized)\n",
    "\n",
    "            # store in results{} to be gathered by accelerate\n",
    "            results[\"outputs\"].extend(outputs)\n",
    "            results[\"num_tokens\"] += num_tokens\n",
    "\n",
    "        results=[ results ] # transform to list, otherwise gather_object() will not collect correctly\n",
    "\n",
    "    # collect results from all the GPUs\n",
    "    results_gathered=gather_object(results)\n",
    "\n",
    "    timediff=time.time()-start\n",
    "    num_tokens=sum([r[\"num_tokens\"] for r in results_gathered ])\n",
    "\n",
    "    print(f\"tokens/sec: {num_tokens//timediff}, time elapsed: {timediff}, num_tokens {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8fef12c-0584-4337-abbf-c8be528472b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce6eb6ae-3a2a-41e1-907f-d1d7b9f5827d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/sec: 2593.0, time elapsed: 3.8558549880981445, num_tokens 10000tokens/sec: 2593.0, time elapsed: 3.8559179306030273, num_tokens 10000tokens/sec: 2593.0, time elapsed: 3.8559176921844482, num_tokens 10000tokens/sec: 2593.0, time elapsed: 3.8559558391571045, num_tokens 10000\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-17 22:13:32,510] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 873 via signal SIGTERM\n",
      "[2025-02-17 22:13:32,511] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 874 via signal SIGTERM\n",
      "[2025-02-17 22:13:32,512] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 875 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(run_inference, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19cc768e-42ba-4193-8714-8d105e941533",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0f32b11-8c9d-4617-bd71-6f42a35f9d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80de3cf7-0daf-4b81-925f-370fb9c9c9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "736.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_reserved()/1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4f5003f-c615-4a0a-992a-d3abdbc74d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/kernel/lib/python3.11/site-packages/torch/cuda/memory.py:444: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "736.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_cached()/1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0f677c-ce4b-4cd4-b29a-29a913edbbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/kernel/lib/python3.11/site-packages/torch/cuda/memory.py:444: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "20.0\n",
      "0.0\n",
      "20.0\n",
      "0.0\n",
      "0.0\n",
      "4.0\n",
      "20.0\n",
      "4.0\n",
      "20.0\n",
      "0.0\n",
      "20.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def memory_stats():\n",
    "    print(torch.cuda.memory_allocated()/1024**2)\n",
    "    print(torch.cuda.memory_cached()/1024**2)\n",
    "\n",
    "\n",
    "def allocate():\n",
    "    x = torch.randn(1024*1024, device='cuda')\n",
    "    memory_stats()\n",
    "    \n",
    "    \n",
    "memory_stats()\n",
    "# 0.0\n",
    "# 0.0\n",
    "\n",
    "allocate()\n",
    "# 4.0 # allocated inside the function\n",
    "# 20.0 # used cache\n",
    "\n",
    "memory_stats()\n",
    "# 0.0 # local tensor is free\n",
    "# 20.0 # cache is still alive\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "memory_stats()\n",
    "# 0.0\n",
    "# 0.0 # cache is free again\n",
    "\n",
    "x = torch.randn(1024, 1024, device='cuda')\n",
    "memory_stats()\n",
    "# 4.0\n",
    "# 20.0\n",
    "\n",
    "# store referece\n",
    "y = x\n",
    "\n",
    "del x # this does not free the memory of x since y still points to it\n",
    "memory_stats() \n",
    "# 4.0  \n",
    "# 20.0\n",
    "\n",
    "del y # this allows PyTorch to free the memory and reuse it in the cache\n",
    "memory_stats()\n",
    "# 0.0\n",
    "# 20.0\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "memory_stats() \n",
    "# 0.0\n",
    "# 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "811bde6e-b07b-40dc-8061-c512656909d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "386e63e2-f3ac-4247-b0fb-6b3abc04cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef2942e-58a4-4659-b34b-6ba8fc5d4ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from accelerate import PartialState, prepare_pippy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d737058-63ca-4bfd-9e95-29d56f3638b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0.post301'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eef9eb3-2f79-4058-be40-d40f08bc70a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\", low_cpu_mem_usage=True, attn_implementation=\"sdpa\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c046433-66d1-4d3b-8140-4c9c81af831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "prompts = (\"I would like to\", \"I really like to\")  # bs = 2, sending 2 per process\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb409519-f34c-4b60-bf0b-3985ca495bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: micromamba install pytorch==2.4.0 --yes --quiet --log-level=error\n",
      "\n",
      "Note: Packages not from Bloomberg channels are not vetted by Bloomberg.\n",
      "\u001b[93mPlease restart the Jupyter kernel if you run into any issues after installing or updating packages via %package.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%package install pytorch==2.4.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1101e2e-2cc5-47f6-a712-15ac6ac15fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = prepare_pippy(model, split_points=\"auto\", example_kwargs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6388993c-8814-4fde-8dd0-e4fc3f15e5df",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'RANK'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRANK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39minit_process_group(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnccl\u001b[39m\u001b[38;5;124m\"\u001b[39m, device_id\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m<frozen os>:679\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'RANK'"
     ]
    }
   ],
   "source": [
    "rank = int(os.environ[\"RANK\"])\n",
    "device = torch.device(f\"cuda:{rank}\")\n",
    "torch.distributed.init_process_group(\"nccl\", device_id=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77ab5e89-007a-4ee0-8f5d-57a41c6f27ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7451eca8-33d6-4a98-85b1-7ab6067462e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    device_map={\"\": accelerator.process_index},\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f4619b-fea2-4f30-a428-6d88f838dbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'outputs': [\" I'm a huge fan of your work. I was wondering if you would be willing to share your contact information so that I can get in touch with you. I would love to learn more about your projects and potentially collaborate on future endeavors.\\n\\nThank you so much for your time and consideration. I look forward to hearing from you soon!\\n\\nBest regards,\\n[Your Name]<|eot_id|>\", \" lovely, isn't it?\\nI was thinking of taking a walk outside and enjoying the fresh air. The sun is shining, and the birds are singing. It's a perfect day to get some exercise and clear your mind.\\nDo you have any plans for the day? I was thinking of trying out a new recipe in the kitchen. I've been wanting to try out some new flavors and ingredients.\\nI'm so glad you reminded me to get outside and enjoy the weather. You're always so full\"], 'num_tokens': 175}]\n"
     ]
    }
   ],
   "source": [
    "prompts_all = [\n",
    "    \"Hello what is your name?\",\n",
    "    \"The weather today is\"\n",
    "]\n",
    "\n",
    "# sync GPUs and start the timer\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "# divide the prompt list onto the available GPUs \n",
    "with accelerator.split_between_processes(prompts_all) as prompts:\n",
    "    # store output of generations in dict\n",
    "    results=dict(outputs=[], num_tokens=0)\n",
    "\n",
    "    # have each GPU do inference, prompt by prompt\n",
    "    for prompt in prompts:\n",
    "        prompt_tokenized=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        output_tokenized = model.generate(**prompt_tokenized, max_new_tokens=100)[0]\n",
    "\n",
    "        # remove prompt from output \n",
    "        output_tokenized=output_tokenized[len(prompt_tokenized[\"input_ids\"][0]):]\n",
    "\n",
    "        # store outputs and number of tokens in result{}\n",
    "        results[\"outputs\"].append( tokenizer.decode(output_tokenized) )\n",
    "        results[\"num_tokens\"] += len(output_tokenized)\n",
    "\n",
    "    results=[ results ] # transform to list, otherwise gather_object() will not collect correctly\n",
    "\n",
    "# collect results from all the GPUs\n",
    "results_gathered=gather_object(results)\n",
    "\n",
    "print(results_gathered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f47311b2-0e65-49d6-97d5-d6ebc6883fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallelformers import parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39e9a07b-9f0a-4b0c-ab43-58d21c986570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/kernel/lib/python3.11/site-packages/parallelformers/parallelize.py\", line 287, in parallelize\n",
      "    p_mutex.wait()\n",
      "  File \"/opt/kernel/lib/python3.11/multiprocessing/synchronize.py\", line 356, in wait\n",
      "    self._cond.wait(timeout)\n",
      "  File \"/opt/kernel/lib/python3.11/multiprocessing/synchronize.py\", line 268, in wait\n",
      "    return self._wait_semaphore.acquire(True, timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<parallelformers.parallelize.parallelize at 0x7f47191958d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallelize(model, num_gpus=2, fp16=True, verbose='detail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ceb86-9667-4a2f-89a1-50d95b693208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028207c9-b13a-418e-a361-9ec678d9aa7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f320c5-96cd-440e-b1b1-99178640939c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/bqnt-user/.cache/huggingface/accelerate/default_config.yaml')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config()  # Write a config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d9f5ba2-affd-4c62-acf1-e9cfec756ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffcfa68-5c9f-4dbb-ad5b-f65a74da7291",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/blacksamorez/tensor-parallel-int4-llm/\n",
    "\n",
    "Tensor_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c21c11a4-46a0-4d78-9af8-b439585a5aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /opt/kernel/lib/python3.11/site-packages/mamba_cell_magic-0.15.0-py3.7.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tensor_parallel\n",
      "  Downloading tensor_parallel-2.0.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch>=1.11 in /opt/kernel/lib/python3.11/site-packages (from tensor_parallel) (2.1.2.post300)\n",
      "Requirement already satisfied: transformers>=4.20.1 in /opt/kernel/lib/python3.11/site-packages (from tensor_parallel) (4.47.1)\n",
      "Requirement already satisfied: filelock in /opt/kernel/lib/python3.11/site-packages (from torch>=1.11->tensor_parallel) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/kernel/lib/python3.11/site-packages (from torch>=1.11->tensor_parallel) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/kernel/lib/python3.11/site-packages (from torch>=1.11->tensor_parallel) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/kernel/lib/python3.11/site-packages (from torch>=1.11->tensor_parallel) (3.4)\n",
      "Requirement already satisfied: jinja2 in /opt/kernel/lib/python3.11/site-packages (from torch>=1.11->tensor_parallel) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/kernel/lib/python3.11/site-packages (from torch>=1.11->tensor_parallel) (2023.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/kernel/lib/python3.11/site-packages (from transformers>=4.20.1->tensor_parallel) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/kernel/lib/python3.11/site-packages (from transformers>=4.20.1->tensor_parallel) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/kernel/lib/python3.11/site-packages (from transformers>=4.20.1->tensor_parallel) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/kernel/lib/python3.11/site-packages (from transformers>=4.20.1->tensor_parallel) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/kernel/lib/python3.11/site-packages (from transformers>=4.20.1->tensor_parallel) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/kernel/lib/python3.11/site-packages (from transformers>=4.20.1->tensor_parallel) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/kernel/lib/python3.11/site-packages (from transformers>=4.20.1->tensor_parallel) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/kernel/lib/python3.11/site-packages (from transformers>=4.20.1->tensor_parallel) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/kernel/lib/python3.11/site-packages (from transformers>=4.20.1->tensor_parallel) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/kernel/lib/python3.11/site-packages (from jinja2->torch>=1.11->tensor_parallel) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/kernel/lib/python3.11/site-packages (from requests->transformers>=4.20.1->tensor_parallel) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/kernel/lib/python3.11/site-packages (from requests->transformers>=4.20.1->tensor_parallel) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/kernel/lib/python3.11/site-packages (from requests->transformers>=4.20.1->tensor_parallel) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/kernel/lib/python3.11/site-packages (from requests->transformers>=4.20.1->tensor_parallel) (2024.12.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/kernel/lib/python3.11/site-packages (from sympy->torch>=1.11->tensor_parallel) (1.3.0)\n",
      "Downloading tensor_parallel-2.0.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: tensor_parallel\n",
      "Successfully installed tensor_parallel-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensor_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c34ea6c4-38b8-4b15-9879-a5f25c94321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensor_parallel as tp\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e482425-7d3e-4738-a46b-4365dccb73aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18181ed0-2b3d-46a7-99eb-fc91111589ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d6a7867-72e8-43ae-bfda-21a683f4f9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorParallelPreTrainedModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "model = tp.TensorParallelPreTrainedModel( # <- tensor parallelism starts here\n",
    "    model,\n",
    "    device_ids=[\"cuda:0\", \"cuda:1\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44a93f5d-661d-4b63-8f47-e173b84709b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(prompt, tokenizer, model):\n",
    "    #tokens = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([prompt], padding=True, truncation=True, return_tensors='pt')\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=1000)\n",
    "    parsed_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    return tokenizer.batch_decode(parsed_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bae19340-a814-4824-b6c7-964fce2c03e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 1163, in forward\n    outputs = self.model(\n              ^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 859, in forward\n    inputs_embeds = self.embed_tokens(input_ids)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/tensor_parallel/wrapper.py\", line 71, in forward\n    output = self.tp_wrapped_module(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/sparse.py\", line 162, in forward\n    return F.embedding(\n           ^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/functional.py\", line 2233, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: embedding(): argument 'indices' (position 2) must be Tensor, not list\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(prompt, tokenizer, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_model\u001b[39m(prompt, tokenizer, model):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#tokens = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      5\u001b[0m     parsed_ids \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m         output_ids[\u001b[38;5;28mlen\u001b[39m(input_ids):] \u001b[38;5;28;01mfor\u001b[39;00m input_ids, output_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model_inputs\u001b[38;5;241m.\u001b[39minput_ids, generated_ids)\n\u001b[1;32m      7\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/tensor_parallel/pretrained_model.py:76\u001b[0m, in \u001b[0;36mTensorParallelPreTrainedModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapped_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/tensor_parallel/tensor_parallel.py:159\u001b[0m, in \u001b[0;36mTensorParallel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m inputs, kwargs_tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_args_kwargs_for_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_cuda \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TENSOR_PARALLEL_USE_NATIVE:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_shards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_tup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device_index]\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parallel_apply_simple(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_shards, inputs, kwargs_tup, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device_index]\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:110\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    108\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 110\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 1163, in forward\n    outputs = self.model(\n              ^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 859, in forward\n    inputs_embeds = self.embed_tokens(input_ids)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/tensor_parallel/wrapper.py\", line 71, in forward\n    output = self.tp_wrapped_module(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/sparse.py\", line 162, in forward\n    return F.embedding(\n           ^^^^^^^^^^^^\n  File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/functional.py\", line 2233, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: embedding(): argument 'indices' (position 2) must be Tensor, not list\n"
     ]
    }
   ],
   "source": [
    "run_model(prompt, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "379401c6-75cd-41fd-a4b2-71cf1b8bf8e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43minputs\u001b[49m,\n\u001b[1;32m      3\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m      4\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m      5\u001b[0m )[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model.generate(\n",
    "    **inputs,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=20,\n",
    ")[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1273d482-7d53-4604-bd9a-8f9ba5dc7675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'TensorParallelPreTrainedModel' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ede9ce-f583-4812-9995-f02f446d3fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bloomberg Lab Python 3",
   "language": "python",
   "name": "remote-jupyterpython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
