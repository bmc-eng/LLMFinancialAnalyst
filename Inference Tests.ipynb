{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33e342b-edf8-4ce9-8a44-759d241f1fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modelinference\n",
    "import prompts\n",
    "import importlib\n",
    "import datetime\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "import utils.modelHelper as mh\n",
    "\n",
    "\n",
    "from accelerate import Accelerator, notebook_launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dcf2505-c7f9-440e-91f1-323f2922817a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'modelinference' from '/project/modelinference.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(modelinference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a89aad1-f53b-4d47-840c-5d6c0556f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into Huggingface\n",
    "\n",
    "with open('pass.txt') as p:\n",
    "    hf_login = p.read()\n",
    "    \n",
    "hf_login = hf_login[hf_login.find('=')+1:hf_login.find('\\n')]\n",
    "login(hf_login, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dba1319-9b03-4b71-8e6b-d568d11f233d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b1ac792-d286-4374-977e-a9ae6a0a352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "    'model_s3_loc': 'deepseek14Q',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoT'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_annual_pit_spx',\n",
    "    'data_location': 'data_annual_pit_spx.json'\n",
    "}\n",
    "\n",
    "run_config = {\n",
    "    'model_hf_id': 'Qwen/Qwen2.5-7B-Instruct',\n",
    "    'model_s3_loc': 'qwen',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 1\n",
    "run_config = {\n",
    "    'model_hf_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 2\n",
    "run_config = {\n",
    "    'model_hf_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoT'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 3\n",
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "    'model_s3_loc': 'deepseek14Q',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 4\n",
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "    'model_s3_loc': 'deepseek14Q',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoT'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 5 - FAILED - OUT OF MEMORY\n",
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B',\n",
    "    'model_s3_loc': 'deepseek32',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoT'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "run_name = f\"{run_config['model_s3_loc']}_{run_config['dataset']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe94d167-f17d-4a2e-be69-db2210d4bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = modelinference.InferenceRun(run_name, run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a0f3e83-e637-4d70-88a8-39b5a2d6e734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "deepseek32\n",
      "deepseek32\n",
      "deepseek32\n",
      "deepseek32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa06c7731c84226afeadb6830951f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f392d0c00a40c28b60a3934b2fb9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e57e4abfe19447e8086c0132d80eb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883817edae214f6a9a42978c5745fd99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting...\n",
      "Waiting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/896 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 18.7 GB\n",
      "Waiting...\n",
      "Waiting...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/896 [00:04<?, ?it/s]\n",
      "[2025-02-24 09:02:08,873] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 3 (pid: 1503) of fn: InferenceRun.run_multi_gpu (start_method: fork)\n"
     ]
    },
    {
     "ename": "ChildFailedError",
     "evalue": "\n============================================================\nrun_multi_gpu FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-02-24_09:02:07\n  host      : 8274481-prj47495909d3fe47e88cd6d023148eb07d-e50c1179a2-pod\n  rank      : 3 (local_rank: 3)\n  exitcode  : 1 (pid: 1503)\n  error_file: /tmp/torchelastic_r99njpsh/none_ip1wq3_4/attempt_0/3/error.json\n  traceback : Traceback (most recent call last):\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n      return f(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^\n    File \"/project/modelinference.py\", line 226, in run_multi_gpu\n      response = self.run_model(prompt['prompt'], tokenizer, model)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/project/modelinference.py\", line 160, in run_model\n      generated_ids = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=2000)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n      return func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2255, in generate\n      result = self._sample(\n               ^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/transformers/generation/utils.py\", line 3254, in _sample\n      outputs = self(**model_inputs, return_dict=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n      return forward_call(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 819, in forward\n      outputs = self.model(\n                ^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n      return forward_call(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 577, in forward\n      layer_outputs = decoder_layer(\n                      ^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n      return forward_call(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 275, in forward\n      hidden_states = self.mlp(hidden_states)\n                      ^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n      return forward_call(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 56, in forward\n      down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n                                                                  ^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n      return forward_call(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n      return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 533, in matmul_4bit\n      return MatMul4Bit.apply(A, B, out, bias, quant_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/autograd/function.py\", line 539, in apply\n      return super().apply(*args, **kwargs)  # type: ignore[misc]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 462, in forward\n      output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 422.00 MiB. GPU 3 has a total capacty of 21.99 GiB of which 363.00 MiB is free. Process 13226 has 21.62 GiB memory in use. Of the allocated memory 20.16 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n  \n============================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mChildFailedError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_multi_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/accelerate/launchers.py:244\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, ELASTIC_LOG_LINE_PREFIX_TEMPLATE_PYTORCH_VERSION):\n\u001b[1;32m    243\u001b[0m         launch_config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_line_prefix_template\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log_line_prefix_template\n\u001b[0;32m--> 244\u001b[0m     \u001b[43melastic_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLaunchConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlaunch_config_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/torch/distributed/launcher/api.py:134\u001b[0m, in \u001b[0;36melastic_launch.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlaunch_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entrypoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.11/site-packages/torch/distributed/launcher/api.py:264\u001b[0m, in \u001b[0;36mlaunch_agent\u001b[0;34m(config, entrypoint, args)\u001b[0m\n\u001b[1;32m    257\u001b[0m     events\u001b[38;5;241m.\u001b[39mrecord(agent\u001b[38;5;241m.\u001b[39mget_event_succeeded())\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mis_failed():\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# ChildFailedError is treated specially by @record\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# if the error files for the failed children exist\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;66;03m# @record will copy the first error (root cause)\u001b[39;00m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;66;03m# to the error file of the launcher process.\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChildFailedError(\n\u001b[1;32m    265\u001b[0m             name\u001b[38;5;241m=\u001b[39mentrypoint_name,\n\u001b[1;32m    266\u001b[0m             failures\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mfailures,\n\u001b[1;32m    267\u001b[0m         )\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreturn_values\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ChildFailedError:\n",
      "\u001b[0;31mChildFailedError\u001b[0m: \n============================================================\nrun_multi_gpu FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-02-24_09:02:07\n  host      : 8274481-prj47495909d3fe47e88cd6d023148eb07d-e50c1179a2-pod\n  rank      : 3 (local_rank: 3)\n  exitcode  : 1 (pid: 1503)\n  error_file: /tmp/torchelastic_r99njpsh/none_ip1wq3_4/attempt_0/3/error.json\n  traceback : Traceback (most recent call last):\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n      return f(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^\n    File \"/project/modelinference.py\", line 226, in run_multi_gpu\n      response = self.run_model(prompt['prompt'], tokenizer, model)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/project/modelinference.py\", line 160, in run_model\n      generated_ids = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=2000)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n      return func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2255, in generate\n      result = self._sample(\n               ^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/transformers/generation/utils.py\", line 3254, in _sample\n      outputs = self(**model_inputs, return_dict=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n      return forward_call(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 819, in forward\n      outputs = self.model(\n                ^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n      return forward_call(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 577, in forward\n      layer_outputs = decoder_layer(\n                      ^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n      return forward_call(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 275, in forward\n      hidden_states = self.mlp(hidden_states)\n                      ^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n      return forward_call(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 56, in forward\n      down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n                                                                  ^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n      return forward_call(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n      return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 533, in matmul_4bit\n      return MatMul4Bit.apply(A, B, out, bias, quant_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/torch/autograd/function.py\", line 539, in apply\n      return super().apply(*args, **kwargs)  # type: ignore[misc]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/opt/kernel/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py\", line 462, in forward\n      output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 422.00 MiB. GPU 3 has a total capacty of 21.99 GiB of which 363.00 MiB is free. Process 13226 has 21.62 GiB memory in use. Of the allocated memory 20.16 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n  \n============================================================"
     ]
    }
   ],
   "source": [
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62376bf8-8749-4fb6-a7d4-08371db294e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4576747c-8ad9-4e22-8bb9-bd453af4cac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e0166-68c7-475c-8866-a8df5b297292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03dc716-b772-49fc-8b42-4f0ecf2fbfe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9642caf9-5b73-4fb8-b32f-19717c4ca374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb455d3-7150-455d-99d8-8c72b787f33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef28887-fe90-4d2d-b069-8a425e5d9a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54ebba-0ee4-40a4-a92c-74a1f882d380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf4d6235-af59-4924-b54b-72ad749bd686",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = modelinference.InferenceRun(run_name, run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7925f016-9392-4142-a9ec-a6872af1bae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "p1 = ir.create_all_prompts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14fd8453-b388-488c-b553-aaff3333ea23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff153540-48a0-4059-acf8-bae25ca9dd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2a95716048465abd4618c818c8f93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ir.load_model_from_storage(ir.model_s3_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "370eddfa-1f87-4ac9-863f-57434aed7ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Config {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"deepseek32\",\n",
       "  \"architectures\": [\n",
       "    \"Qwen2ForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"eos_token_id\": 151643,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 5120,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 27648,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"max_window_layers\": 64,\n",
       "  \"model_type\": \"qwen2\",\n",
       "  \"num_attention_heads\": 40,\n",
       "  \"num_hidden_layers\": 64,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"quantization_config\": {\n",
       "    \"_load_in_4bit\": true,\n",
       "    \"_load_in_8bit\": false,\n",
       "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "    \"bnb_4bit_quant_type\": \"nf4\",\n",
       "    \"bnb_4bit_use_double_quant\": true,\n",
       "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "    \"llm_int8_has_fp16_weight\": false,\n",
       "    \"llm_int8_skip_modules\": null,\n",
       "    \"llm_int8_threshold\": 6.0,\n",
       "    \"load_in_4bit\": true,\n",
       "    \"load_in_8bit\": false,\n",
       "    \"quant_method\": \"bitsandbytes\"\n",
       "  },\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"sliding_window\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.48.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 152064\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfcdf6bf-07d3-4fb0-87de-034e94e92a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdf599ad-b6de-4a30-b48a-7c30a97ce88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(ir.model_hf_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a9cff15-9c86-48fd-be61-10bd0d160f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model...\n"
     ]
    }
   ],
   "source": [
    "output_result = ir.run_model(p1[0]['prompt'],tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86f4a03-2196-434b-b5a1-22f2435f9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_result = {\"date\": \"2020-02-06\", \"security\": \"MMM UN Equity\", \"response\": \"Here is the JSON response:\\n\\n```json\\n{\\n  \\\"decision\\\": \\\"BUY\\\",\\n  \\\"confidence score\\\": 80,\\n  \\\"reason\\\": \\\"Gross profit and EPS have increased over time, indicating a strong financial performance\\\"\\n}\\n```\\n\\nI have computed the following financial ratios:\\n\\n1. Gross Margin: \\n   - 2020: 3.786000e+09 / 8.111000e+09 = 0.466\\n   - 2019: 3.803000e+09 / 7.991000e+09 = 0.477\\n   - 2018: 3.858000e+09 / 8.171000e+09 = 0.471\\n   - 2017: 3.553000e+09 / 7.863000e+09 = 0.453\\n   - 2016: 3.885000e+09 / 7.945000e+09 = 0.492\\n\\nThe gross margin has been increasing over time, indicating a strong financial performance.\\n\\n2. EPS:\\n   - 2020: 9.690000e+08 / 1.012600e+10 = 0.953\\n   - 2019: 1.583000e+09 / 1.076400e+10 = 1.465\\n   - 2018: 1.127000e+09 / 1.014200e+10 = 1.117\\n   - 2017: 1.347000e+09 / 9.848000e+09 = 0.137\\n   - 2016: 1.543000e+09 / 9.848000e+09 = 0.156\\n\\nThe EPS has been increasing over time, indicating a strong financial performance.\\n\\n3. Current Ratio:\\n   - 2020: 2.441000e+09 / 9.222000e+09 = 0.264\\n   - 2019: 1.588000e+09 / 7.821000e+09 = 0.201\\n   - 2018: 1.131000e+09 / 7.265000e+09 = 0.157\\n   - 2017: 8.930000e+08 / 7.244000e+09 = 0.123\\n   - 2016: 8.910000e+08 / 5.020000e+09 = 0.177\\n\\nThe current ratio has been increasing over time, indicating a strong financial performance.\\n\\nBased on these financial ratios, the company has been performing well financially and has a strong track record of increasing gross profit and EPS over time. Therefore, I recommend a BUY decision with a confidence score of 80.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5457878-488c-48d5-935b-44cf0427b19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the JSON response:\\n\\n```json\\n{\\n  \"decision\": \"BUY\",\\n  \"confidence score\": 80,\\n  \"reason\": \"Gross profit and EPS have increased over time, indicating a strong financial performance\"\\n}\\n```\\n\\nI have computed the following financial ratios:\\n\\n1. Gross Margin: \\n   - 2020: 3.786000e+09 / 8.111000e+09 = 0.466\\n   - 2019: 3.803000e+09 / 7.991000e+09 = 0.477\\n   - 2018: 3.858000e+09 / 8.171000e+09 = 0.471\\n   - 2017: 3.553000e+09 / 7.863000e+09 = 0.453\\n   - 2016: 3.885000e+09 / 7.945000e+09 = 0.492\\n\\nThe gross margin has been increasing over time, indicating a strong financial performance.\\n\\n2. EPS:\\n   - 2020: 9.690000e+08 / 1.012600e+10 = 0.953\\n   - 2019: 1.583000e+09 / 1.076400e+10 = 1.465\\n   - 2018: 1.127000e+09 / 1.014200e+10 = 1.117\\n   - 2017: 1.347000e+09 / 9.848000e+09 = 0.137\\n   - 2016: 1.543000e+09 / 9.848000e+09 = 0.156\\n\\nThe EPS has been increasing over time, indicating a strong financial performance.\\n\\n3. Current Ratio:\\n   - 2020: 2.441000e+09 / 9.222000e+09 = 0.264\\n   - 2019: 1.588000e+09 / 7.821000e+09 = 0.201\\n   - 2018: 1.131000e+09 / 7.265000e+09 = 0.157\\n   - 2017: 8.930000e+08 / 7.244000e+09 = 0.123\\n   - 2016: 8.910000e+08 / 5.020000e+09 = 0.177\\n\\nThe current ratio has been increasing over time, indicating a strong financial performance.\\n\\nBased on these financial ratios, the company has been performing well financially and has a strong track record of increasing gross profit and EPS over time. Therefore, I recommend a BUY decision with a confidence score of 80.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_result['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5728ba70-2042-4250-af42-cac37f26eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30024d1f-b16b-4214-962c-190106dc2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_json(llm_output):\n",
    "    # remove all the broken lines\n",
    "    form = llm_output.replace('\\n','')\n",
    "    # Find the start and end of the JSON input\n",
    "    #try:\n",
    "    soj = form.find('```json')\n",
    "    eoj = form.find('}```')\n",
    "\n",
    "    if eoj == -1:\n",
    "        eoj = len(llm_output)\n",
    "        llm_output = llm_output + '}```'\n",
    "    # Pull out the additional context\n",
    "    additional = form[:soj]\n",
    "    additional += form[eoj + 4:]\n",
    "    json_obj = json.loads(form[soj + 7:eoj + 1])\n",
    "    json_obj['AdditionalContext'] = additional\n",
    "    return json_obj\n",
    "    #except:\n",
    "    #    return llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73b1782b-e52e-407d-b9d6-2d691cb8fc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  \"decision\": \"BUY\",  \"confidence score\": 80,  \"reason\": \"Gross profit and EPS have increased over time, indicating a strong financial performance\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'decision': 'BUY',\n",
       " 'confidence score': 80,\n",
       " 'reason': 'Gross profit and EPS have increased over time, indicating a strong financial performance',\n",
       " 'AdditionalContext': 'Here is the JSON response:I have computed the following financial ratios:1. Gross Margin:    - 2020: 3.786000e+09 / 8.111000e+09 = 0.466   - 2019: 3.803000e+09 / 7.991000e+09 = 0.477   - 2018: 3.858000e+09 / 8.171000e+09 = 0.471   - 2017: 3.553000e+09 / 7.863000e+09 = 0.453   - 2016: 3.885000e+09 / 7.945000e+09 = 0.492The gross margin has been increasing over time, indicating a strong financial performance.2. EPS:   - 2020: 9.690000e+08 / 1.012600e+10 = 0.953   - 2019: 1.583000e+09 / 1.076400e+10 = 1.465   - 2018: 1.127000e+09 / 1.014200e+10 = 1.117   - 2017: 1.347000e+09 / 9.848000e+09 = 0.137   - 2016: 1.543000e+09 / 9.848000e+09 = 0.156The EPS has been increasing over time, indicating a strong financial performance.3. Current Ratio:   - 2020: 2.441000e+09 / 9.222000e+09 = 0.264   - 2019: 1.588000e+09 / 7.821000e+09 = 0.201   - 2018: 1.131000e+09 / 7.265000e+09 = 0.157   - 2017: 8.930000e+08 / 7.244000e+09 = 0.123   - 2016: 8.910000e+08 / 5.020000e+09 = 0.177The current ratio has been increasing over time, indicating a strong financial performance.Based on these financial ratios, the company has been performing well financially and has a strong track record of increasing gross profit and EPS over time. Therefore, I recommend a BUY decision with a confidence score of 80.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_json(output_result['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50b16da3-394a-4aeb-8c5d-2a7456d3e0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved s3://awmgd-prod-finml-sandbox-user/bclarke16/tmp/fs/logs/results - llama - data_quarterly_pit_indu\n",
      "Run Completed!\n"
     ]
    }
   ],
   "source": [
    "ir.save_run({'test':'1234'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade0d8d-ba3b-4706-a46a-9e76a77ec445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4e95d-81d2-428d-9789-02d82b3f374f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd5cab-e6c8-457a-a6ab-0a66da205511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe3e06-50f0-4711-b8d6-a55ce1195aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96829e27-e611-4533-8ea3-2f8db98d3cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375dbe05-abfe-45be-a911-e7ff7c27250e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f5952b-df1a-47ac-b808-81148bdae20a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25404db2-acee-4b0a-83ee-0fef698539b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper = mh.ModelHelper('tmp/fs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77bde369-5016-4eb4-bca4-71b8df7d219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.clear_folder('deepseek32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a6b91-7e1c-44a1-90bd-7b254f5efee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bloomberg Lab Python 3",
   "language": "python",
   "name": "remote-jupyterpython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
