{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea0b4d2-9a58-426f-a551-07d8d5eceae1",
   "metadata": {},
   "source": [
    "# Run Open Source Inference \n",
    "\n",
    "This notebook runs inference on the open source models using the multi-gpu process implemented in the modelinference file. This file requires the llm-base environment to run correctly. The results of the backtest are stored in the Data folder under each model folder. \n",
    "\n",
    "We test with a handful of open source models:\n",
    "\n",
    "- Llama 3.2 3B Instruct\n",
    "- Qwen 2.5 7B Instruct\n",
    "- Deepseek\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33e342b-edf8-4ce9-8a44-759d241f1fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_inference\n",
    "import prompts\n",
    "import importlib\n",
    "import datetime\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "import utils.model_helper as mh\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from accelerate import Accelerator, notebook_launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dcf2505-c7f9-440e-91f1-323f2922817a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.model_helper' from '/project/utils/model_helper.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(model_inference)\n",
    "importlib.reload(prompts)\n",
    "importlib.reload(mh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3e71f-48ee-483a-b834-17fea3d28a04",
   "metadata": {},
   "source": [
    "### Set up the environment\n",
    "Log into Huggingface and check the number of GPUs available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a89aad1-f53b-4d47-840c-5d6c0556f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into Huggingface\n",
    "with open('pass.txt') as p:\n",
    "    hf_login = p.read()\n",
    "    \n",
    "hf_login = hf_login[hf_login.find('=')+1:hf_login.find('\\n')]\n",
    "login(hf_login, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dff8b64b-1dfa-4073-96ba-41f89ceab9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.2.post300\n",
      "Device Count: 4\n"
     ]
    }
   ],
   "source": [
    "print(f'Torch version: {torch.__version__}')\n",
    "print(f'Device Count: {torch.cuda.device_count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca868c0b-6b7d-4c03-b4a3-6a01b55c6c5c",
   "metadata": {},
   "source": [
    "### Run 1: Llama 3.2 Earning analysis - Base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2ac3bb-0cb4-49a0-9342-af022c765db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the run config\n",
    "run_config = {\n",
    "    'model_hf_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE_EARN'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu_blended',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_blended.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a56b0b4b-b11d-4b0a-9fb5-6c2da4124f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "run_name = f\"{run_config['model_s3_loc']}_{run_config['dataset']}\"\n",
    "ir = model_inference.InferenceRun(run_name, run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cff85cc3-7825-4367-b9c5-5858170939f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "llamallama\n",
      "\n",
      "llama\n",
      "llama\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0c0f6537e14fb18a46d133b63e9b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5929afc071654183b69333d27c30d4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e48f5735688436fb11462e7a6a231bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea30f12d92a94642a3e6889794f3bc74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting...\n",
      "Waiting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 6.4 GB\n",
      "Waiting...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [26:41,  2.21s/it]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n",
      "Finished run...\n",
      "Finished run...\n",
      "Finished run...\n",
      "Gathered results...Gathered results...Gathered results...Gathered results...\n",
      "\n",
      "\n",
      "\n",
      "Finished run in 0:28:23.145980\n",
      "Called Save run\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-03-30 10:05:34.896587.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [28:23,  1.92s/it]\n",
      "[2025-03-30 10:34:55,568] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 2401 via signal SIGTERM\n",
      "[2025-03-30 10:34:55,569] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 2402 via signal SIGTERM\n",
      "[2025-03-30 10:34:55,570] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 2403 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "# Run the multi-gpu model with notebook_launcher\n",
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0779e2-8a65-4167-99d7-19af14ae0051",
   "metadata": {},
   "source": [
    "### Run 2: Llama 3.2 Earning analysis - Chain of Thought\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e6b92c1-b6a6-4b8b-a6f1-873c8a146dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_hf_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['COT_EARN'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu_blended_cot',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_blended.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0970f294-cdf5-4885-b71c-9cf888d13cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "run_name = f\"{run_config['model_s3_loc']}_{run_config['dataset']}\"\n",
    "ir = model_inference.InferenceRun(run_name, run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bf2d3c2-6e5f-46ca-94b9-bbd515903613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "llama\n",
      "llama\n",
      "llama\n",
      "llama\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a1ce1191154bb1acfc79a38fadfdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00055e73a7b14b69b56b806e8d3e1fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2f668434e44e4eacc44138378617da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8941089c7e421cad94322f3198a594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting...\n",
      "Waiting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 6.4 GB\n",
      "Waiting...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 812/887 [53:59<04:41,  3.75s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 840/887 [56:09<03:06,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 868/887 [58:12<01:21,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [59:41,  4.45s/it]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n",
      "Gathered results...Gathered results...Gathered results...Gathered results...\n",
      "\n",
      "\n",
      "\n",
      "Finished run in 0:59:41.735791\n",
      "Called Save run\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-03-30 10:36:38.332729.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [59:42,  4.03s/it]\n",
      "[2025-03-30 11:37:17,363] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3624 via signal SIGTERM\n",
      "[2025-03-30 11:37:17,365] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3626 via signal SIGTERM\n",
      "[2025-03-30 11:37:17,365] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3627 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "# Run the multi-gpu model with notebook_launcher\n",
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e5438-aa23-430f-93d8-8f632e1a950d",
   "metadata": {},
   "source": [
    "### Run 3: DeepSeek R1 Qwen 7B Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3485696-9d28-4ce8-b41c-a2b6e09a0a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b426d33722824b6394cd1dddd37c8e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    "\n",
    ")\n",
    "\n",
    "model_helper = mh.ModelHelper('tmp/fs')\n",
    "model_helper.get_model_and_save('deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',\n",
    "                                'deepseek7B', \n",
    "                                'Data',\n",
    "                                True,\n",
    "                                quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02500a92-6c4d-47ec-a87f-d36c9e1633f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',\n",
    "    'model_s3_loc': 'deepseek7B',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE_EARN'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu_blended_base',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_blended.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18257919-498e-4d66-80a2-50cf3f6a51d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "run_name = f\"{run_config['model_s3_loc']}_{run_config['dataset']}\"\n",
    "ir = model_inference.InferenceRun(run_name, run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "975b7ad7-6985-46e0-9bc9-656fff07665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ede4b3954174609a1f553a73ef88041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e958ec993eac4dca8c64b67c67fc8834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e53951307343d6bf4c2df4d22ce930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f55f728aa88487fa72a40746858f241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46e5362e4aa4c6dbd0bf5dd2ce6b77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a279f33aa534e4394384ce97e49d5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5.4 GB\n",
      "Waiting...\n",
      "starting backtest...starting backtest...\n",
      "\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 12/887 [03:49<4:48:36, 19.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-4 crashed: Process 796 got signal: 2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-30 17:34:15,103] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGINT death signal, shutting down workers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-2 crashed: Process 794 got signal: 2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-30 17:34:15,106] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 793 via signal SIGINT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-1 crashed: Process 793 got signal: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-30 17:34:15,106] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 794 via signal SIGINT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-30 17:34:15,107] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 795 via signal SIGINT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-1 crashed: Process 793 got signal: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-30 17:34:15,108] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 796 via signal SIGINT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-3 crashed: Process 795 got signal: 2\n",
      "Process ForkProcess-4 crashed: Process 796 got signal: 2\n",
      "Process ForkProcess-2 crashed: Process 794 got signal: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-30 17:34:45,140] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 793 via Signals.SIGINT, forcefully exiting via Signals.SIGKILL\n",
      "[2025-03-30 17:34:45,368] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 794 via Signals.SIGINT, forcefully exiting via Signals.SIGKILL\n",
      "[2025-03-30 17:34:45,608] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 795 via Signals.SIGINT, forcefully exiting via Signals.SIGKILL\n",
      "[2025-03-30 17:34:45,848] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 796 via Signals.SIGINT, forcefully exiting via Signals.SIGKILL\n"
     ]
    },
    {
     "ename": "SignalException",
     "evalue": "Process 717 got signal: 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSignalException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the multi-gpu model with notebook_launcher\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_multi_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/accelerate/launchers.py:244\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, ELASTIC_LOG_LINE_PREFIX_TEMPLATE_PYTORCH_VERSION):\n\u001b[1;32m    243\u001b[0m         launch_config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_line_prefix_template\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log_line_prefix_template\n\u001b[0;32m--> 244\u001b[0m     \u001b[43melastic_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLaunchConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlaunch_config_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/torch/distributed/launcher/api.py:134\u001b[0m, in \u001b[0;36melastic_launch.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlaunch_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entrypoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/torch/distributed/launcher/api.py:255\u001b[0m, in \u001b[0;36mlaunch_agent\u001b[0;34m(config, entrypoint, args)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     metrics\u001b[38;5;241m.\u001b[39minitialize_metrics(metrics\u001b[38;5;241m.\u001b[39mMetricsConfig(config\u001b[38;5;241m.\u001b[39mmetrics_cfg))\n\u001b[0;32m--> 255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# records that agent.run() has succeeded NOT that workers have succeeded\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     events\u001b[38;5;241m.\u001b[39mrecord(agent\u001b[38;5;241m.\u001b[39mget_event_succeeded())\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py:124\u001b[0m, in \u001b[0;36mprof.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 124\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     put_metric(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.success\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, group)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py:736\u001b[0m, in \u001b[0;36mSimpleElasticAgent.run\u001b[0;34m(self, role)\u001b[0m\n\u001b[1;32m    734\u001b[0m shutdown_called: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 736\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_execution_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_metrics(result)\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py:877\u001b[0m, in \u001b[0;36mSimpleElasticAgent._invoke_run\u001b[0;34m(self, role)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_group\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m!=\u001b[39m WorkerState\u001b[38;5;241m.\u001b[39mINIT\n\u001b[0;32m--> 877\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    878\u001b[0m     run_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_monitor_workers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_group)\n\u001b[1;32m    879\u001b[0m     state \u001b[38;5;241m=\u001b[39m run_result\u001b[38;5;241m.\u001b[39mstate\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py:62\u001b[0m, in \u001b[0;36m_terminate_process_handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Termination handler that raises exceptions on the main process.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mWhen the process receives death signal(SIGTERM, SIGINT), this termination handler will\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mbe terminated.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m sigval \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mSignals(signum)\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m SignalException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mgetpid()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got signal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msigval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, sigval\u001b[38;5;241m=\u001b[39msigval)\n",
      "\u001b[0;31mSignalException\u001b[0m: Process 717 got signal: 2"
     ]
    }
   ],
   "source": [
    "# Run the multi-gpu model with notebook_launcher\n",
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b1ac792-d286-4374-977e-a9ae6a0a352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "    'model_s3_loc': 'deepseek14Q',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoT'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_annual_pit_spx',\n",
    "    'data_location': 'data_annual_pit_spx.json'\n",
    "}\n",
    "\n",
    "run_config = {\n",
    "    'model_hf_id': 'Qwen/Qwen2.5-7B-Instruct',\n",
    "    'model_s3_loc': 'qwen',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 1\n",
    "run_config = {\n",
    "    'model_hf_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 2\n",
    "run_config = {\n",
    "    'model_hf_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoT'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 3\n",
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "    'model_s3_loc': 'deepseek14Q',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 4\n",
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "    'model_s3_loc': 'deepseek14Q',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoT'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 5 - failed\n",
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B',\n",
    "    'model_s3_loc': 'deepseek32',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoT'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# Run 6 - failed\n",
    "run_config = {\n",
    "    'model_hf_id': 'Qwen/Qwen2.5-7B-Instruct',\n",
    "    'model_s3_loc': 'qwen',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoTDetailed'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "run_config = {\n",
    "    'model_hf_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE_EARN'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu_refresh_v2',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_v2.json'\n",
    "}\n",
    "\n",
    "# run_config = {\n",
    "#     'model_hf_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "#     'model_s3_loc': 'llama',\n",
    "#     'model_reload': False,\n",
    "#     'model_quant': None,\n",
    "#     'system_prompt': prompts.SYSTEM_PROMPTS['COT_EARN'],\n",
    "#     'multi-gpu':True,\n",
    "#     'dataset': 'data_quarterly_pit_indu',\n",
    "#     'data_location': 'data_quarterly_pit_indu.json'\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "run_name = f\"{run_config['model_s3_loc']}_{run_config['dataset']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe94d167-f17d-4a2e-be69-db2210d4bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = model_inference.InferenceRun(run_name, run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abf93574-9fd9-4739-89eb-77941804fe03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "# Create the prompts and save to the Data folder\n",
    "prompts = ir.create_all_prompts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a0f3e83-e637-4d70-88a8-39b5a2d6e734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "llama\n",
      "llama\n",
      "llama\n",
      "llama\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb37171aae694fd195996b4ccda5320f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86688187a52642aa9e11e53cff75c80c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1171c4eb5c2f43f8b324488a31c2ed1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b891ef7620a4f4d9769c56708e000f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04af17ad7cf4a93a3bdd906f3e1280b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b911d278f32f4d7cba8d9dee72d80bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd4f87d1ee54fbe8342687ae40042e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/891 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 6.4 GB\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 884/891 [27:05<00:12,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 888/891 [27:10<00:04,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "892it [27:19,  1.82s/it]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n",
      "Finished run...\n",
      "Gathered results...Gathered results...Gathered results...Gathered results...\n",
      "\n",
      "\n",
      "\n",
      "Finished run in 0:27:51.318931\n",
      "Called Save run\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-03-30 09:15:48.238865.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "892it [27:51,  1.87s/it]\n",
      "[2025-03-30 09:44:39,571] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 257 via signal SIGTERM\n",
      "[2025-03-30 09:44:39,572] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 258 via signal SIGTERM\n",
      "[2025-03-30 09:44:39,573] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 260 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "# Run the multi-gpu model with notebook_launcher\n",
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03dc716-b772-49fc-8b42-4f0ecf2fbfe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9642caf9-5b73-4fb8-b32f-19717c4ca374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb455d3-7150-455d-99d8-8c72b787f33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef28887-fe90-4d2d-b069-8a425e5d9a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54ebba-0ee4-40a4-a92c-74a1f882d380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf4d6235-af59-4924-b54b-72ad749bd686",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = model_inference.InferenceRun(run_name, run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7925f016-9392-4142-a9ec-a6872af1bae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "p1 = ir.create_all_prompts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14fd8453-b388-488c-b553-aaff3333ea23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff153540-48a0-4059-acf8-bae25ca9dd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2a95716048465abd4618c818c8f93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ir.load_model_from_storage(ir.model_s3_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "370eddfa-1f87-4ac9-863f-57434aed7ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Config {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"deepseek32\",\n",
       "  \"architectures\": [\n",
       "    \"Qwen2ForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"eos_token_id\": 151643,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 5120,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 27648,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"max_window_layers\": 64,\n",
       "  \"model_type\": \"qwen2\",\n",
       "  \"num_attention_heads\": 40,\n",
       "  \"num_hidden_layers\": 64,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"quantization_config\": {\n",
       "    \"_load_in_4bit\": true,\n",
       "    \"_load_in_8bit\": false,\n",
       "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "    \"bnb_4bit_quant_type\": \"nf4\",\n",
       "    \"bnb_4bit_use_double_quant\": true,\n",
       "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "    \"llm_int8_has_fp16_weight\": false,\n",
       "    \"llm_int8_skip_modules\": null,\n",
       "    \"llm_int8_threshold\": 6.0,\n",
       "    \"load_in_4bit\": true,\n",
       "    \"load_in_8bit\": false,\n",
       "    \"quant_method\": \"bitsandbytes\"\n",
       "  },\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"sliding_window\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.48.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 152064\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfcdf6bf-07d3-4fb0-87de-034e94e92a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdf599ad-b6de-4a30-b48a-7c30a97ce88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(ir.model_hf_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a9cff15-9c86-48fd-be61-10bd0d160f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model...\n"
     ]
    }
   ],
   "source": [
    "output_result = ir.run_model(p1[0]['prompt'],tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86f4a03-2196-434b-b5a1-22f2435f9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_result = {\"date\": \"2020-02-06\", \"security\": \"MMM UN Equity\", \"response\": \"Here is the JSON response:\\n\\n```json\\n{\\n  \\\"decision\\\": \\\"BUY\\\",\\n  \\\"confidence score\\\": 80,\\n  \\\"reason\\\": \\\"Gross profit and EPS have increased over time, indicating a strong financial performance\\\"\\n}\\n```\\n\\nI have computed the following financial ratios:\\n\\n1. Gross Margin: \\n   - 2020: 3.786000e+09 / 8.111000e+09 = 0.466\\n   - 2019: 3.803000e+09 / 7.991000e+09 = 0.477\\n   - 2018: 3.858000e+09 / 8.171000e+09 = 0.471\\n   - 2017: 3.553000e+09 / 7.863000e+09 = 0.453\\n   - 2016: 3.885000e+09 / 7.945000e+09 = 0.492\\n\\nThe gross margin has been increasing over time, indicating a strong financial performance.\\n\\n2. EPS:\\n   - 2020: 9.690000e+08 / 1.012600e+10 = 0.953\\n   - 2019: 1.583000e+09 / 1.076400e+10 = 1.465\\n   - 2018: 1.127000e+09 / 1.014200e+10 = 1.117\\n   - 2017: 1.347000e+09 / 9.848000e+09 = 0.137\\n   - 2016: 1.543000e+09 / 9.848000e+09 = 0.156\\n\\nThe EPS has been increasing over time, indicating a strong financial performance.\\n\\n3. Current Ratio:\\n   - 2020: 2.441000e+09 / 9.222000e+09 = 0.264\\n   - 2019: 1.588000e+09 / 7.821000e+09 = 0.201\\n   - 2018: 1.131000e+09 / 7.265000e+09 = 0.157\\n   - 2017: 8.930000e+08 / 7.244000e+09 = 0.123\\n   - 2016: 8.910000e+08 / 5.020000e+09 = 0.177\\n\\nThe current ratio has been increasing over time, indicating a strong financial performance.\\n\\nBased on these financial ratios, the company has been performing well financially and has a strong track record of increasing gross profit and EPS over time. Therefore, I recommend a BUY decision with a confidence score of 80.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5457878-488c-48d5-935b-44cf0427b19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the JSON response:\\n\\n```json\\n{\\n  \"decision\": \"BUY\",\\n  \"confidence score\": 80,\\n  \"reason\": \"Gross profit and EPS have increased over time, indicating a strong financial performance\"\\n}\\n```\\n\\nI have computed the following financial ratios:\\n\\n1. Gross Margin: \\n   - 2020: 3.786000e+09 / 8.111000e+09 = 0.466\\n   - 2019: 3.803000e+09 / 7.991000e+09 = 0.477\\n   - 2018: 3.858000e+09 / 8.171000e+09 = 0.471\\n   - 2017: 3.553000e+09 / 7.863000e+09 = 0.453\\n   - 2016: 3.885000e+09 / 7.945000e+09 = 0.492\\n\\nThe gross margin has been increasing over time, indicating a strong financial performance.\\n\\n2. EPS:\\n   - 2020: 9.690000e+08 / 1.012600e+10 = 0.953\\n   - 2019: 1.583000e+09 / 1.076400e+10 = 1.465\\n   - 2018: 1.127000e+09 / 1.014200e+10 = 1.117\\n   - 2017: 1.347000e+09 / 9.848000e+09 = 0.137\\n   - 2016: 1.543000e+09 / 9.848000e+09 = 0.156\\n\\nThe EPS has been increasing over time, indicating a strong financial performance.\\n\\n3. Current Ratio:\\n   - 2020: 2.441000e+09 / 9.222000e+09 = 0.264\\n   - 2019: 1.588000e+09 / 7.821000e+09 = 0.201\\n   - 2018: 1.131000e+09 / 7.265000e+09 = 0.157\\n   - 2017: 8.930000e+08 / 7.244000e+09 = 0.123\\n   - 2016: 8.910000e+08 / 5.020000e+09 = 0.177\\n\\nThe current ratio has been increasing over time, indicating a strong financial performance.\\n\\nBased on these financial ratios, the company has been performing well financially and has a strong track record of increasing gross profit and EPS over time. Therefore, I recommend a BUY decision with a confidence score of 80.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_result['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5728ba70-2042-4250-af42-cac37f26eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30024d1f-b16b-4214-962c-190106dc2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_json(llm_output):\n",
    "    # remove all the broken lines\n",
    "    form = llm_output.replace('\\n','')\n",
    "    # Find the start and end of the JSON input\n",
    "    #try:\n",
    "    soj = form.find('```json')\n",
    "    eoj = form.find('}```')\n",
    "\n",
    "    if eoj == -1:\n",
    "        eoj = len(llm_output)\n",
    "        llm_output = llm_output + '}```'\n",
    "    # Pull out the additional context\n",
    "    additional = form[:soj]\n",
    "    additional += form[eoj + 4:]\n",
    "    json_obj = json.loads(form[soj + 7:eoj + 1])\n",
    "    json_obj['AdditionalContext'] = additional\n",
    "    return json_obj\n",
    "    #except:\n",
    "    #    return llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73b1782b-e52e-407d-b9d6-2d691cb8fc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  \"decision\": \"BUY\",  \"confidence score\": 80,  \"reason\": \"Gross profit and EPS have increased over time, indicating a strong financial performance\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'decision': 'BUY',\n",
       " 'confidence score': 80,\n",
       " 'reason': 'Gross profit and EPS have increased over time, indicating a strong financial performance',\n",
       " 'AdditionalContext': 'Here is the JSON response:I have computed the following financial ratios:1. Gross Margin:    - 2020: 3.786000e+09 / 8.111000e+09 = 0.466   - 2019: 3.803000e+09 / 7.991000e+09 = 0.477   - 2018: 3.858000e+09 / 8.171000e+09 = 0.471   - 2017: 3.553000e+09 / 7.863000e+09 = 0.453   - 2016: 3.885000e+09 / 7.945000e+09 = 0.492The gross margin has been increasing over time, indicating a strong financial performance.2. EPS:   - 2020: 9.690000e+08 / 1.012600e+10 = 0.953   - 2019: 1.583000e+09 / 1.076400e+10 = 1.465   - 2018: 1.127000e+09 / 1.014200e+10 = 1.117   - 2017: 1.347000e+09 / 9.848000e+09 = 0.137   - 2016: 1.543000e+09 / 9.848000e+09 = 0.156The EPS has been increasing over time, indicating a strong financial performance.3. Current Ratio:   - 2020: 2.441000e+09 / 9.222000e+09 = 0.264   - 2019: 1.588000e+09 / 7.821000e+09 = 0.201   - 2018: 1.131000e+09 / 7.265000e+09 = 0.157   - 2017: 8.930000e+08 / 7.244000e+09 = 0.123   - 2016: 8.910000e+08 / 5.020000e+09 = 0.177The current ratio has been increasing over time, indicating a strong financial performance.Based on these financial ratios, the company has been performing well financially and has a strong track record of increasing gross profit and EPS over time. Therefore, I recommend a BUY decision with a confidence score of 80.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_json(output_result['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50b16da3-394a-4aeb-8c5d-2a7456d3e0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved s3://awmgd-prod-finml-sandbox-user/bclarke16/tmp/fs/logs/results - llama - data_quarterly_pit_indu\n",
      "Run Completed!\n"
     ]
    }
   ],
   "source": [
    "ir.save_run({'test':'1234'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade0d8d-ba3b-4706-a46a-9e76a77ec445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4e95d-81d2-428d-9789-02d82b3f374f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd5cab-e6c8-457a-a6ab-0a66da205511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe3e06-50f0-4711-b8d6-a55ce1195aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96829e27-e611-4533-8ea3-2f8db98d3cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375dbe05-abfe-45be-a911-e7ff7c27250e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f5952b-df1a-47ac-b808-81148bdae20a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25404db2-acee-4b0a-83ee-0fef698539b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper = mh.ModelHelper('tmp/fs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77bde369-5016-4eb4-bca4-71b8df7d219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.clear_folder('llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a6b91-7e1c-44a1-90bd-7b254f5efee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bloomberg Lab Python 3",
   "language": "python",
   "name": "remote-jupyterpython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
