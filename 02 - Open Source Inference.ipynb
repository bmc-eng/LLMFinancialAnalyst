{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea0b4d2-9a58-426f-a551-07d8d5eceae1",
   "metadata": {},
   "source": [
    "# Run Open Source Inference \n",
    "\n",
    "This notebook runs inference on the open source models using the multi-gpu process implemented in the modelinference file. This file requires the llm-base environment to run correctly. The results of the backtest are stored in the Data folder under each model folder. \n",
    "\n",
    "We test with a handful of open source models:\n",
    "\n",
    "- Llama 3.2 3B Instruct\n",
    "- Qwen 2.5 7B Instruct\n",
    "- Deepseek\n",
    "\n",
    "Also test with a fine-tuned model for estimating next period EPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33e342b-edf8-4ce9-8a44-759d241f1fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_inference\n",
    "import prompts\n",
    "import importlib\n",
    "import datetime\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "import utils.model_helper as mh\n",
    "import model_finetuned_inference as mft\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from accelerate import Accelerator, notebook_launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dcf2505-c7f9-440e-91f1-323f2922817a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model_finetuned_inference' from '/project/model_finetuned_inference.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(model_inference)\n",
    "importlib.reload(prompts)\n",
    "importlib.reload(mh)\n",
    "importlib.reload(mft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3e71f-48ee-483a-b834-17fea3d28a04",
   "metadata": {},
   "source": [
    "### Set up the environment\n",
    "Log into Huggingface and check the number of GPUs available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a89aad1-f53b-4d47-840c-5d6c0556f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into Huggingface\n",
    "with open('pass.txt') as p:\n",
    "    hf_login = p.read()\n",
    "    \n",
    "hf_login = hf_login[hf_login.find('=')+1:hf_login.find('\\n')]\n",
    "login(hf_login, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dff8b64b-1dfa-4073-96ba-41f89ceab9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.2.post300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Torch version: {torch.__version__}')\n",
    "#print(f'Device Count: {torch.cuda.device_count()}')\n",
    "import accelerate\n",
    "accelerate.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca868c0b-6b7d-4c03-b4a3-6a01b55c6c5c",
   "metadata": {},
   "source": [
    "### Run 1: Llama 3.2 Earning analysis - Base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2ac3bb-0cb4-49a0-9342-af022c765db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the run config\n",
    "run_config = {\n",
    "    'model_hf_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE_EARN'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu_blended',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_blended.json'\n",
    "    #'fine_tuned_dir':None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a56b0b4b-b11d-4b0a-9fb5-6c2da4124f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_name = f\"{run_config['model_s3_loc']}_{run_config['dataset']}\"\n",
    "ir = model_inference.InferenceRun(run_name, run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "#prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)\n",
    "torch.cuda.is_initialized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cff85cc3-7825-4367-b9c5-5858170939f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "llama\n",
      "llama\n",
      "llama\n",
      "llama\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f4f12a7c164d0488e301f1356655e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd33fb13f8548698d5b314679be01eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83509c02c94949b98a8f669338bbbdc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fa001eb6f74a4ba4c893a2d05a8a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865fb48f191a40e78bbb773b081a862e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6ac1fb77b4420bbf28a0be940c94e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0b8a20a85440dda3abb6c7135823c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 6.4 GB\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [22:53,  1.45s/it]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run in 0:24:51.327519\n",
      "Called Save run\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-04-11 10:41:09.337985.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [24:51,  1.68s/it]\n",
      "[2025-04-11 11:06:35,294] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 371 via signal SIGTERM\n",
      "[2025-04-11 11:06:35,295] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 372 via signal SIGTERM\n",
      "[2025-04-11 11:06:35,296] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 374 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "# Run the multi-gpu model with notebook_launcher\n",
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0779e2-8a65-4167-99d7-19af14ae0051",
   "metadata": {},
   "source": [
    "### Run 2: Llama 3.2 Earning analysis - Chain of Thought\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e6b92c1-b6a6-4b8b-a6f1-873c8a146dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_hf_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['COT_EARN'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu_blended_cot',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_blended.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0970f294-cdf5-4885-b71c-9cf888d13cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "run_name = f\"{run_config['model_s3_loc']}_{run_config['dataset']}\"\n",
    "ir = model_inference.InferenceRun(run_name, run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bf2d3c2-6e5f-46ca-94b9-bbd515903613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "llama\n",
      "llama\n",
      "llama\n",
      "llama\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a1ce1191154bb1acfc79a38fadfdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00055e73a7b14b69b56b806e8d3e1fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2f668434e44e4eacc44138378617da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8941089c7e421cad94322f3198a594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting...\n",
      "Waiting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 6.4 GB\n",
      "Waiting...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 812/887 [53:59<04:41,  3.75s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 840/887 [56:09<03:06,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 868/887 [58:12<01:21,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [59:41,  4.45s/it]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n",
      "Gathered results...Gathered results...Gathered results...Gathered results...\n",
      "\n",
      "\n",
      "\n",
      "Finished run in 0:59:41.735791\n",
      "Called Save run\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-03-30 10:36:38.332729.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [59:42,  4.03s/it]\n",
      "[2025-03-30 11:37:17,363] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3624 via signal SIGTERM\n",
      "[2025-03-30 11:37:17,365] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3626 via signal SIGTERM\n",
      "[2025-03-30 11:37:17,365] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3627 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "# Run the multi-gpu model with notebook_launcher\n",
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e5438-aa23-430f-93d8-8f632e1a950d",
   "metadata": {},
   "source": [
    "### Run 3: DeepSeek R1 Qwen 7B Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3485696-9d28-4ce8-b41c-a2b6e09a0a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b426d33722824b6394cd1dddd37c8e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    "\n",
    ")\n",
    "\n",
    "model_helper = mh.ModelHelper('tmp/fs')\n",
    "model_helper.get_model_and_save('deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',\n",
    "                                'deepseek7B', \n",
    "                                'Data',\n",
    "                                True,\n",
    "                                quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02500a92-6c4d-47ec-a87f-d36c9e1633f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',\n",
    "    'model_s3_loc': 'deepseek7B',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE_EARN'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu_blended_base',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_blended.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18257919-498e-4d66-80a2-50cf3f6a51d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "run_name = f\"{run_config['model_s3_loc']}_{run_config['dataset']}\"\n",
    "ir = model_inference.InferenceRun(run_name, run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "975b7ad7-6985-46e0-9bc9-656fff07665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 8 GPUs.\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7Bdeepseek7B\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b022893e2fc94874a032fb4fad9b1cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9d96d0a756423aa034b4e0ea289a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70d4f29c86f426b98cf6ae6dc88eba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddc747a4bb142a5aa9e84e20cc68aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21627aaa81a34e4fa858deea9084ee30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e7e38fe7e04341a8bf637fabafab68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4aa290fb5544721a95b943f679664ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533094192bfc41aaae4b24bcc89b5d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7f1eeb40474823835669b8af7ee8b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6de0bd4687443298abd480fe06777f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5.4 GB\n",
      "starting backtest...starting backtest...starting backtest...\n",
      "\n",
      "\n",
      "starting backtest...starting backtest...\n",
      "starting backtest...\n",
      "\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [2:17:03,  9.49s/it]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run in 2:17:37.334527\n",
      "Called Save run\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-03-30 17:44:18.481428.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [2:17:37,  9.30s/it]\n",
      "[2025-03-30 20:02:58,292] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 187 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,294] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 188 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,294] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 189 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,295] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 190 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,296] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 192 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,297] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 193 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,297] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 194 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "# Run the multi-gpu model with notebook_launcher\n",
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b984dbc-7874-46ae-9b84-3e9100f54735",
   "metadata": {},
   "source": [
    "### Run 4: DeepSeek 7B COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02aea17c-33e8-4ed7-9fd7-ebcf73b6ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',\n",
    "    'model_s3_loc': 'deepseek7B',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['COT_EARN'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu_blended_base',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_blended.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd0d50b-6c95-4ce5-b503-12215cfe902d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "run_name = f\"{run_config['model_s3_loc']}_{run_config['dataset']}\"\n",
    "ir = model_inference.InferenceRun(run_name, run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae221de2-c71d-49f9-ab74-23876c132ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 8 GPUs.\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11faff2b6a724e0fa15daf54d72a89e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c953bf644c4a9ea4b69bdfcbc5c560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529da691ffcf4c7195160cf33f24b42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e727ab9919754053a8151c3d45a743c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3908dcf8b744a2bac73be9afbb4c079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ab89be3fac4a4e81e1ef1df0271f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9536a6a9c79b4633abc4a15455bdfe55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae19905fff644e583df3ddbd20757d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5.4 GB\n",
      "starting backtest...starting backtest...starting backtest...\n",
      "starting backtest...\n",
      "\n",
      "starting backtest...\n",
      "\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [2:26:10,  9.47s/it]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run in 2:30:12.540523\n",
      "Called Save run\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-03-30 20:11:39.828629.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [2:30:13, 10.15s/it]\n",
      "[2025-03-30 22:42:55,767] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3671 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,768] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3672 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,769] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3673 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,770] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3674 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,771] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3676 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,771] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3677 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3678 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f8ebc-2756-4eb0-a2d9-777fc5fd0bb9",
   "metadata": {},
   "source": [
    "### Qwen 2.5 - 3B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c8e7142-b8f5-4bc6-8a93-fb9c20247fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4484d647b84a67bf1e5b5b64aac13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35b7dae13724b98a5ee535ee28c6be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/35.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0507125d34b4cd6a289d8bea03da158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94d0ca6ae014523b3ae08440e6e1081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca50a003196b45beb23eac1e8cf70741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a49654268841bd9e6854a674361ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531e8f2f215543cc8097ebbe180d661c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    "\n",
    ")\n",
    "\n",
    "model_helper = mh.ModelHelper('tmp/fs')\n",
    "model_helper.get_model_and_save('Qwen/Qwen2.5-3B-Instruct',\n",
    "                                'qwen3b', \n",
    "                                'Data',\n",
    "                                True,\n",
    "                                quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be801bc-c018-42b4-98a0-ebacbc024890",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_hf_id': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "    'model_s3_loc': 'qwen3b',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE_EARN'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu_blended_base',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_blended.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb76425a-1690-4998-8448-a4d439685fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "run_name = f\"{run_config['model_s3_loc']}_{run_config['dataset']}\"\n",
    "ir = model_inference.InferenceRun(run_name, run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c54db2c-0ede-4571-a364-90f688cf462f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "qwen3b\n",
      "qwen3b\n",
      "qwen3b\n",
      "qwen3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ce5ea0f3e841baa5e6f54700c0f414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1a91fbbc90444197bed468ed3dda73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81b9cc44b6646289467c2bcb20df147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81ce3e3a53b42eab8cfd812810fce0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 2.0 GB\n",
      "starting backtest...starting backtest...starting backtest...\n",
      "\n",
      "\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [15:51,  1.07s/it]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run in 0:15:56.753014\n",
      "Called Save run\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-04-03 19:46:42.048460.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [15:57,  1.08s/it]\n",
      "[2025-04-03 20:03:53,398] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 186 via signal SIGTERM\n",
      "[2025-04-03 20:03:53,400] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 187 via signal SIGTERM\n",
      "[2025-04-03 20:03:53,401] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 188 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4dfbc32-0c0b-4b6b-874a-c897f70652fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_hf_id': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "    'model_s3_loc': 'qwen3b',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['COT_EARN'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu_blended_base',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_blended.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c6c33e5-606d-4d16-8e20-ce449c2e5c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "run_name = f\"{run_config['model_s3_loc']}_{run_config['dataset']}\"\n",
    "ir = model_inference.InferenceRun(run_name, run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea7dfd49-e0cd-4fd4-8543-31edb9843aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "qwen3b\n",
      "qwen3b\n",
      "qwen3b\n",
      "qwen3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 2.0 GB\n",
      "starting backtest...\n",
      "starting backtest...starting backtest...\n",
      "\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [13:42,  1.09it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run in 0:13:42.324489\n",
      "Called Save run\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-04-03 20:06:52.680533.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [13:42,  1.08it/s]\n",
      "[2025-04-03 20:21:16,801] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 1305 via signal SIGTERM\n",
      "[2025-04-03 20:21:16,802] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 1306 via signal SIGTERM\n",
      "[2025-04-03 20:21:16,803] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 1307 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a613d5-ea61-40bc-bc26-4b4ef60f95f4",
   "metadata": {},
   "source": [
    "## Run 5 - Fine tuned model - EPS only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a234749-8d26-453d-bb1d-af522d694cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_hf_id': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "    'model_s3_loc': 'qwen3b',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE_FINE_TUNED'],\n",
    "    'multi-gpu':False,\n",
    "    'dataset': 'data_quarterly_pit_indu_blended_base',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_blended.json',\n",
    "    'fine_tuned_dir': 'fine_tuned2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ade0f00-1ec6-4f37-9728-00d1cfc9e966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "run_name = f\"{run_config['model_s3_loc']}_{run_config['dataset']}_finetuned\"\n",
    "ir = model_inference.InferenceRun(run_name, run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)\n",
    "for prompt in prompt_set:\n",
    "    prompt['prompt'] += 'The next period EPS is '\n",
    "    #prompt['prompt'] += \"\\nAnswer in JSON format with the next period EPS, the direction, the magnitude and a confidence.\"\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "outputs = []\n",
    "def run_finetuned_backtest(prompts, fine_tuned_model, tokenizer):\n",
    "    count = 0\n",
    "    progress = tqdm(total=len(prompts), position=0, leave=True)\n",
    "    for prompt in prompts:\n",
    "        tokens = tokenizer.apply_chat_template(prompt['prompt'], tokenize=False, add_generation_prompt=True )\n",
    "        print(tokens)\n",
    "        model_inputs = tokenizer([tokens], return_tensors='pt').to(\"cuda\")\n",
    "        generated_ids = fine_tuned_model.generate(**model_inputs, \n",
    "                                       pad_token_id=tokenizer.eos_token_id, \n",
    "                                       max_new_tokens=50,\n",
    "                                      temperature=0.001)\n",
    "        parsed_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        resp = {\n",
    "            'date': prompt['date'],\n",
    "            'security': prompt['security'],\n",
    "            'response': tokenizer.batch_decode(parsed_ids, skip_special_tokens=True)[0]\n",
    "        }\n",
    "        outputs.append(resp)\n",
    "        progress.update()\n",
    "\n",
    "# Load the base model \n",
    "model_helper = mh.ModelHelper('tmp/fs')\n",
    "base_model = ir.load_model_from_storage(run_config['model_s3_loc'])\n",
    "# clear the local folder once completed loading into memory\n",
    "model_helper.clear_folder(run_config['model_s3_loc'])\n",
    "\n",
    "# Update the base model with the fine-tuned modules\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model, 'fine_tuned2')\n",
    "# Create the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(run_config['model_hf_id'])\n",
    "run_finetuned_backtest(prompt_set[:2], fine_tuned_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61ab2e96-94cf-4399-b0fd-99ee62889995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "716cb386-58e6-4993-926c-9df66b320455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "124420f5-f726-4560-89d3-7a51ac328414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea760211-361b-410d-b0aa-46a059d294da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f57a5ab-e5d9-4406-a53c-b7853b777a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a7db607-ee59-4bb5-b1ec-a85e73e80b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_finetuned_backtest(prompt_set[:2], fine_tuned_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9ba4b3f-ae73-493d-86cd-2a378a36d400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the output\n",
    "with open(f\"Results/Earnings/results - Qwen3B Finetuned - EPS only.json\", 'w') as f:\n",
    "    json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc47f4b5-d84b-4742-a31d-744b044155a2",
   "metadata": {},
   "source": [
    "### Run 6 - Fine tuned with JSON output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ba92cc9-7a37-4408-88e6-c1c914713e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_hf_id': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "    'model_s3_loc': 'qwen3b',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE_FINE_TUNED'],\n",
    "    'multi-gpu':False,\n",
    "    'dataset': 'data_quarterly_pit_indu_blended_base',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_blended.json',\n",
    "    'fine_tuned_dir': 'fine_tuned_json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9781f79-f3ee-482d-b613-3fb4f0e14319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model_finetuned_inference' from '/project/model_finetuned_inference.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(mft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f602715e-3193-4fc6-a36f-6524251d9394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "ftm = mft.FineTunedInference(\"JSON FT Run\", run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d14fbb4b-8739-4641-951b-733b29ea21c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "prompt_set = ftm.create_all_prompts(force_refresh=True, is_save_prompts=True)\n",
    "prompt_set_appended = ftm.reformat_prompts(prompt_set, \"\\nAnswer in JSON format with the next period EPS, the direction, the magnitude and a confidence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74903e24-26c5-4da6-bb0c-80bbac1a639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 887/887 [1:13:19<00:00,  4.96s/it]\n"
     ]
    }
   ],
   "source": [
    "output_json = ftm.run_finetuned_backtest(prompt_set_appended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c8559f9-09d5-4a8f-827a-4ff77a8ef87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56b6ffd3-9d71-4d0e-a203-edaf3738f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the output\n",
    "with open(f\"Results/Earnings/results - Qwen3B Finetuned - JSON Format.json\", 'w') as f:\n",
    "    json.dump(output_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550ca67-ce40-46ff-978a-e6d969c336e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8b5be-6a73-49e6-b080-2f528d1baff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe160210-08ed-4815-896d-0faa1e3c8378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f29a94-3def-4285-871e-c23dc76f252c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd26f1fd-5a30-4fe8-ae90-f4eb01f80cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4eee830-dd64-4d41-a033-8d96a41c12b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"decision\": HOLD, \"confidence score\": 75, \"reason\": \"The company\\'s earnings have been stable but slightly declining, while revenue growth has slowed. Gross profit margin has remained relatively constant, but operating income has decreased. The stock price has shown some volatility, but the overall trend suggests a neutral position.\"}'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompts[0]['prompt']\n",
    "\n",
    "tokens = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True )\n",
    "model_inputs = tokenizer([tokens], return_tensors='pt').to(\"cuda\")\n",
    "generated_ids = fine_tuned_model.generate(**model_inputs, \n",
    "                               pad_token_id=tokenizer.eos_token_id, \n",
    "                               max_new_tokens=2000,\n",
    "                              temperature=0.4)\n",
    "parsed_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "tokenizer.batch_decode(parsed_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936cb54c-f0d2-4b4d-ad21-abe4d7c8bf23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b1ac792-d286-4374-977e-a9ae6a0a352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "    'model_s3_loc': 'deepseek14Q',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoT'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_annual_pit_spx',\n",
    "    'data_location': 'data_annual_pit_spx.json'\n",
    "}\n",
    "\n",
    "run_config = {\n",
    "    'model_hf_id': 'Qwen/Qwen2.5-7B-Instruct',\n",
    "    'model_s3_loc': 'qwen',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 1\n",
    "run_config = {\n",
    "    'model_hf_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 2\n",
    "run_config = {\n",
    "    'model_hf_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoT'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 3\n",
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "    'model_s3_loc': 'deepseek14Q',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 4\n",
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "    'model_s3_loc': 'deepseek14Q',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoT'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 5 - failed\n",
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B',\n",
    "    'model_s3_loc': 'deepseek32',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoT'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# Run 6 - failed\n",
    "run_config = {\n",
    "    'model_hf_id': 'Qwen/Qwen2.5-7B-Instruct',\n",
    "    'model_s3_loc': 'qwen',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoTDetailed'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "run_config = {\n",
    "    'model_hf_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE_EARN'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu_refresh_v2',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_v2.json'\n",
    "}\n",
    "\n",
    "# run_config = {\n",
    "#     'model_hf_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "#     'model_s3_loc': 'llama',\n",
    "#     'model_reload': False,\n",
    "#     'model_quant': None,\n",
    "#     'system_prompt': prompts.SYSTEM_PROMPTS['COT_EARN'],\n",
    "#     'multi-gpu':True,\n",
    "#     'dataset': 'data_quarterly_pit_indu',\n",
    "#     'data_location': 'data_quarterly_pit_indu.json'\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "run_name = f\"{run_config['model_s3_loc']}_{run_config['dataset']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe94d167-f17d-4a2e-be69-db2210d4bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = model_inference.InferenceRun(run_name, run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abf93574-9fd9-4739-89eb-77941804fe03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "# Create the prompts and save to the Data folder\n",
    "prompts = ir.create_all_prompts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a0f3e83-e637-4d70-88a8-39b5a2d6e734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "llama\n",
      "llama\n",
      "llama\n",
      "llama\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb37171aae694fd195996b4ccda5320f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86688187a52642aa9e11e53cff75c80c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1171c4eb5c2f43f8b324488a31c2ed1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b891ef7620a4f4d9769c56708e000f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04af17ad7cf4a93a3bdd906f3e1280b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b911d278f32f4d7cba8d9dee72d80bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd4f87d1ee54fbe8342687ae40042e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/891 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 6.4 GB\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 884/891 [27:05<00:12,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 888/891 [27:10<00:04,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "892it [27:19,  1.82s/it]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n",
      "Finished run...\n",
      "Gathered results...Gathered results...Gathered results...Gathered results...\n",
      "\n",
      "\n",
      "\n",
      "Finished run in 0:27:51.318931\n",
      "Called Save run\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-03-30 09:15:48.238865.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "892it [27:51,  1.87s/it]\n",
      "[2025-03-30 09:44:39,571] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 257 via signal SIGTERM\n",
      "[2025-03-30 09:44:39,572] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 258 via signal SIGTERM\n",
      "[2025-03-30 09:44:39,573] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 260 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "# Run the multi-gpu model with notebook_launcher\n",
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03dc716-b772-49fc-8b42-4f0ecf2fbfe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9642caf9-5b73-4fb8-b32f-19717c4ca374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb455d3-7150-455d-99d8-8c72b787f33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef28887-fe90-4d2d-b069-8a425e5d9a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54ebba-0ee4-40a4-a92c-74a1f882d380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf4d6235-af59-4924-b54b-72ad749bd686",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = model_inference.InferenceRun(run_name, run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7925f016-9392-4142-a9ec-a6872af1bae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "p1 = ir.create_all_prompts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14fd8453-b388-488c-b553-aaff3333ea23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff153540-48a0-4059-acf8-bae25ca9dd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2a95716048465abd4618c818c8f93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ir.load_model_from_storage(ir.model_s3_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370eddfa-1f87-4ac9-863f-57434aed7ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfcdf6bf-07d3-4fb0-87de-034e94e92a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdf599ad-b6de-4a30-b48a-7c30a97ce88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(ir.model_hf_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a9cff15-9c86-48fd-be61-10bd0d160f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model...\n"
     ]
    }
   ],
   "source": [
    "output_result = ir.run_model(p1[0]['prompt'],tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86f4a03-2196-434b-b5a1-22f2435f9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_result = {\"date\": \"2020-02-06\", \"security\": \"MMM UN Equity\", \"response\": \"Here is the JSON response:\\n\\n```json\\n{\\n  \\\"decision\\\": \\\"BUY\\\",\\n  \\\"confidence score\\\": 80,\\n  \\\"reason\\\": \\\"Gross profit and EPS have increased over time, indicating a strong financial performance\\\"\\n}\\n```\\n\\nI have computed the following financial ratios:\\n\\n1. Gross Margin: \\n   - 2020: 3.786000e+09 / 8.111000e+09 = 0.466\\n   - 2019: 3.803000e+09 / 7.991000e+09 = 0.477\\n   - 2018: 3.858000e+09 / 8.171000e+09 = 0.471\\n   - 2017: 3.553000e+09 / 7.863000e+09 = 0.453\\n   - 2016: 3.885000e+09 / 7.945000e+09 = 0.492\\n\\nThe gross margin has been increasing over time, indicating a strong financial performance.\\n\\n2. EPS:\\n   - 2020: 9.690000e+08 / 1.012600e+10 = 0.953\\n   - 2019: 1.583000e+09 / 1.076400e+10 = 1.465\\n   - 2018: 1.127000e+09 / 1.014200e+10 = 1.117\\n   - 2017: 1.347000e+09 / 9.848000e+09 = 0.137\\n   - 2016: 1.543000e+09 / 9.848000e+09 = 0.156\\n\\nThe EPS has been increasing over time, indicating a strong financial performance.\\n\\n3. Current Ratio:\\n   - 2020: 2.441000e+09 / 9.222000e+09 = 0.264\\n   - 2019: 1.588000e+09 / 7.821000e+09 = 0.201\\n   - 2018: 1.131000e+09 / 7.265000e+09 = 0.157\\n   - 2017: 8.930000e+08 / 7.244000e+09 = 0.123\\n   - 2016: 8.910000e+08 / 5.020000e+09 = 0.177\\n\\nThe current ratio has been increasing over time, indicating a strong financial performance.\\n\\nBased on these financial ratios, the company has been performing well financially and has a strong track record of increasing gross profit and EPS over time. Therefore, I recommend a BUY decision with a confidence score of 80.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5457878-488c-48d5-935b-44cf0427b19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the JSON response:\\n\\n```json\\n{\\n  \"decision\": \"BUY\",\\n  \"confidence score\": 80,\\n  \"reason\": \"Gross profit and EPS have increased over time, indicating a strong financial performance\"\\n}\\n```\\n\\nI have computed the following financial ratios:\\n\\n1. Gross Margin: \\n   - 2020: 3.786000e+09 / 8.111000e+09 = 0.466\\n   - 2019: 3.803000e+09 / 7.991000e+09 = 0.477\\n   - 2018: 3.858000e+09 / 8.171000e+09 = 0.471\\n   - 2017: 3.553000e+09 / 7.863000e+09 = 0.453\\n   - 2016: 3.885000e+09 / 7.945000e+09 = 0.492\\n\\nThe gross margin has been increasing over time, indicating a strong financial performance.\\n\\n2. EPS:\\n   - 2020: 9.690000e+08 / 1.012600e+10 = 0.953\\n   - 2019: 1.583000e+09 / 1.076400e+10 = 1.465\\n   - 2018: 1.127000e+09 / 1.014200e+10 = 1.117\\n   - 2017: 1.347000e+09 / 9.848000e+09 = 0.137\\n   - 2016: 1.543000e+09 / 9.848000e+09 = 0.156\\n\\nThe EPS has been increasing over time, indicating a strong financial performance.\\n\\n3. Current Ratio:\\n   - 2020: 2.441000e+09 / 9.222000e+09 = 0.264\\n   - 2019: 1.588000e+09 / 7.821000e+09 = 0.201\\n   - 2018: 1.131000e+09 / 7.265000e+09 = 0.157\\n   - 2017: 8.930000e+08 / 7.244000e+09 = 0.123\\n   - 2016: 8.910000e+08 / 5.020000e+09 = 0.177\\n\\nThe current ratio has been increasing over time, indicating a strong financial performance.\\n\\nBased on these financial ratios, the company has been performing well financially and has a strong track record of increasing gross profit and EPS over time. Therefore, I recommend a BUY decision with a confidence score of 80.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_result['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5728ba70-2042-4250-af42-cac37f26eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30024d1f-b16b-4214-962c-190106dc2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_json(llm_output):\n",
    "    # remove all the broken lines\n",
    "    form = llm_output.replace('\\n','')\n",
    "    # Find the start and end of the JSON input\n",
    "    #try:\n",
    "    soj = form.find('```json')\n",
    "    eoj = form.find('}```')\n",
    "\n",
    "    if eoj == -1:\n",
    "        eoj = len(llm_output)\n",
    "        llm_output = llm_output + '}```'\n",
    "    # Pull out the additional context\n",
    "    additional = form[:soj]\n",
    "    additional += form[eoj + 4:]\n",
    "    json_obj = json.loads(form[soj + 7:eoj + 1])\n",
    "    json_obj['AdditionalContext'] = additional\n",
    "    return json_obj\n",
    "    #except:\n",
    "    #    return llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73b1782b-e52e-407d-b9d6-2d691cb8fc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  \"decision\": \"BUY\",  \"confidence score\": 80,  \"reason\": \"Gross profit and EPS have increased over time, indicating a strong financial performance\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'decision': 'BUY',\n",
       " 'confidence score': 80,\n",
       " 'reason': 'Gross profit and EPS have increased over time, indicating a strong financial performance',\n",
       " 'AdditionalContext': 'Here is the JSON response:I have computed the following financial ratios:1. Gross Margin:    - 2020: 3.786000e+09 / 8.111000e+09 = 0.466   - 2019: 3.803000e+09 / 7.991000e+09 = 0.477   - 2018: 3.858000e+09 / 8.171000e+09 = 0.471   - 2017: 3.553000e+09 / 7.863000e+09 = 0.453   - 2016: 3.885000e+09 / 7.945000e+09 = 0.492The gross margin has been increasing over time, indicating a strong financial performance.2. EPS:   - 2020: 9.690000e+08 / 1.012600e+10 = 0.953   - 2019: 1.583000e+09 / 1.076400e+10 = 1.465   - 2018: 1.127000e+09 / 1.014200e+10 = 1.117   - 2017: 1.347000e+09 / 9.848000e+09 = 0.137   - 2016: 1.543000e+09 / 9.848000e+09 = 0.156The EPS has been increasing over time, indicating a strong financial performance.3. Current Ratio:   - 2020: 2.441000e+09 / 9.222000e+09 = 0.264   - 2019: 1.588000e+09 / 7.821000e+09 = 0.201   - 2018: 1.131000e+09 / 7.265000e+09 = 0.157   - 2017: 8.930000e+08 / 7.244000e+09 = 0.123   - 2016: 8.910000e+08 / 5.020000e+09 = 0.177The current ratio has been increasing over time, indicating a strong financial performance.Based on these financial ratios, the company has been performing well financially and has a strong track record of increasing gross profit and EPS over time. Therefore, I recommend a BUY decision with a confidence score of 80.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_json(output_result['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50b16da3-394a-4aeb-8c5d-2a7456d3e0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved s3://awmgd-prod-finml-sandbox-user/bclarke16/tmp/fs/logs/results - llama - data_quarterly_pit_indu\n",
      "Run Completed!\n"
     ]
    }
   ],
   "source": [
    "ir.save_run({'test':'1234'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade0d8d-ba3b-4706-a46a-9e76a77ec445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4e95d-81d2-428d-9789-02d82b3f374f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd5cab-e6c8-457a-a6ab-0a66da205511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe3e06-50f0-4711-b8d6-a55ce1195aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96829e27-e611-4533-8ea3-2f8db98d3cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375dbe05-abfe-45be-a911-e7ff7c27250e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94f5952b-df1a-47ac-b808-81148bdae20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.model_helper as mh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25404db2-acee-4b0a-83ee-0fef698539b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper = mh.ModelHelper('tmp/fs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77bde369-5016-4eb4-bca4-71b8df7d219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.clear_folder('qwen3b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a6b91-7e1c-44a1-90bd-7b254f5efee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bloomberg Lab Python 3",
   "language": "python",
   "name": "remote-jupyterpython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
