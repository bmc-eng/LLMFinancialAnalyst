{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea0b4d2-9a58-426f-a551-07d8d5eceae1",
   "metadata": {},
   "source": [
    "# Run Open Source Inference \n",
    "\n",
    "This notebook runs inference on the open source models using the multi-gpu process implemented in the modelinference file. This file requires the llm-base environment to run correctly. The results of the backtest are stored in the Data folder under each model folder. \n",
    "\n",
    "We test with a handful of open source models:\n",
    "\n",
    "- Llama 3.2 3B Instruct\n",
    "- Qwen 2.5 7B Instruct\n",
    "- Deepseek R1 Qwen 14B \n",
    "- Qwen 2.5 3B Instruct\n",
    "\n",
    "We also test with a fine-tuned model, the Qwen 2.5 3B model. We test this with the earnings prompt and also with a new EPS estimate prompt before fine-tuning to provide a comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea766b51-fff0-449a-81bb-15e9d2f61100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33e342b-edf8-4ce9-8a44-759d241f1fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model_inference\n",
    "import prompts\n",
    "import importlib\n",
    "import datetime\n",
    "import torch\n",
    "import json\n",
    "from huggingface_hub import login\n",
    "import constructors.huggingface_strategy as hs\n",
    "\n",
    "import models.model_helper as mh\n",
    "\n",
    "from constructors.huggingface_strategy import HuggingfaceRun\n",
    "from transformers import BitsAndBytesConfig\n",
    "from accelerate import Accelerator, notebook_launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dcf2505-c7f9-440e-91f1-323f2922817a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'constructors.huggingface_strategy' from '/project/constructors/huggingface_strategy.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(prompts)\n",
    "importlib.reload(mh)\n",
    "importlib.reload(hs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3e71f-48ee-483a-b834-17fea3d28a04",
   "metadata": {},
   "source": [
    "### Set up the environment\n",
    "Log into Huggingface and check the number of GPUs available, pytorch version currently loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a89aad1-f53b-4d47-840c-5d6c0556f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into Huggingface with login credentials\n",
    "with open('pass.txt') as p:\n",
    "    hf_login = p.read()\n",
    "    \n",
    "hf_login = hf_login[hf_login.find('=')+1:hf_login.find('\\n')]\n",
    "login(hf_login, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dff8b64b-1dfa-4073-96ba-41f89ceab9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.2.post300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Torch version: {torch.__version__}')\n",
    "#print(f'Device Count: {torch.cuda.device_count()}')\n",
    "import accelerate\n",
    "accelerate.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca868c0b-6b7d-4c03-b4a3-6a01b55c6c5c",
   "metadata": {},
   "source": [
    "### Run 1: Llama 3.2 Earning analysis - Base\n",
    "The first model we test is the smallest one - Llama 3.2 3B model. As a Huggingface model we use the HuggingfaceRun object to create the strategy and produce the list of trades from the run. We must launch in notebook_launcher to use multi-GPUs. \n",
    "\n",
    "Results File: Results/Earnings/results - llama - base - 2025-03-30 896587.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f2ac3bb-0cb4-49a0-9342-af022c765db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the run config\n",
    "run_config = {\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu_blended',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_blended.json'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a56b0b4b-b11d-4b0a-9fb5-6c2da4124f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "ir = hs.HuggingfaceRun(run_name='Llama 3B Earnings',\n",
    "                       model_id='meta-llama/Llama-3.2-3B-Instruct',\n",
    "                       dataset_id='data_quarterly_pit_indu_refresh_blended.json',system_prompt=prompts.SYSTEM_PROMPTS['BASE_EARN'],run_config=run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cff85cc3-7825-4367-b9c5-5858170939f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 6.4 GB\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [26:08,  1.34s/it]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished backtest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [26:45,  1.81s/it]\n",
      "[2025-04-29 21:30:27,302] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 8450 via signal SIGTERM\n",
      "[2025-04-29 21:30:27,303] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 8452 via signal SIGTERM\n",
      "[2025-04-29 21:30:27,304] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 8453 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "# Run the multi-gpu model with notebook_launcher\n",
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0779e2-8a65-4167-99d7-19af14ae0051",
   "metadata": {},
   "source": [
    "### Run 2: Llama 3.2 Earning analysis - Chain of Thought\n",
    "\n",
    "Run the Llama 3.2 model on Chain of Thought prompts. Results in the following folder:\n",
    "\n",
    "Results/Earnings/results - llama - cot - 2025-03-30 332729.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e6b92c1-b6a6-4b8b-a6f1-873c8a146dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'multi-gpu':True,\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_blended.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0970f294-cdf5-4885-b71c-9cf888d13cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "ir = HuggingfaceRun(run_name='Llama 3B Earnings COT',\n",
    "                       model_id='meta-llama/Llama-3.2-3B-Instruct',\n",
    "                       dataset_id='data_quarterly_pit_indu_refresh_blended.json',\n",
    "                       system_prompt=prompts.SYSTEM_PROMPTS['COT_EARN'],run_config=run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bf2d3c2-6e5f-46ca-94b9-bbd515903613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 6.4 GB\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [1:03:35,  4.31s/it]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run in 1:03:43.495022\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-04-21 16:40:45.191930.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [1:03:44,  4.31s/it]\n",
      "[2025-04-21 17:45:30,848] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 2716 via signal SIGTERM\n",
      "[2025-04-21 17:45:30,850] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 2717 via signal SIGTERM\n",
      "[2025-04-21 17:45:30,850] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 2718 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "# Run the multi-gpu model with notebook_launcher\n",
    "notebook_launcher(ir.run, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e5438-aa23-430f-93d8-8f632e1a950d",
   "metadata": {},
   "source": [
    "### Run 3: DeepSeek R1 Qwen 7B Base\n",
    "\n",
    "Results in the following file:\n",
    "\n",
    "Results/Earnings/results - Deepseek - BASE C -2025-04-01 20:43:33.936328.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02500a92-6c4d-47ec-a87f-d36c9e1633f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_s3_loc': 'deepseek7B',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'multi-gpu':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18257919-498e-4d66-80a2-50cf3f6a51d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "ir = HuggingfaceRun(run_name='DeepSeek 7B Earnings Base',\n",
    "                       model_id='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',\n",
    "                       dataset_id='data_quarterly_pit_indu_refresh_blended.json',\n",
    "                       system_prompt=prompts.SYSTEM_PROMPTS['BASE_EARN'],run_config=run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "975b7ad7-6985-46e0-9bc9-656fff07665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 8 GPUs.\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7Bdeepseek7B\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b022893e2fc94874a032fb4fad9b1cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9d96d0a756423aa034b4e0ea289a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70d4f29c86f426b98cf6ae6dc88eba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddc747a4bb142a5aa9e84e20cc68aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21627aaa81a34e4fa858deea9084ee30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e7e38fe7e04341a8bf637fabafab68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4aa290fb5544721a95b943f679664ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533094192bfc41aaae4b24bcc89b5d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7f1eeb40474823835669b8af7ee8b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6de0bd4687443298abd480fe06777f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5.4 GB\n",
      "starting backtest...starting backtest...starting backtest...\n",
      "\n",
      "\n",
      "starting backtest...starting backtest...\n",
      "starting backtest...\n",
      "\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [2:17:03,  9.49s/it]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run in 2:17:37.334527\n",
      "Called Save run\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-03-30 17:44:18.481428.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [2:17:37,  9.30s/it]\n",
      "[2025-03-30 20:02:58,292] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 187 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,294] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 188 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,294] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 189 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,295] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 190 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,296] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 192 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,297] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 193 via signal SIGTERM\n",
      "[2025-03-30 20:02:58,297] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 194 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "# Run the multi-gpu model with notebook_launcher\n",
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b984dbc-7874-46ae-9b84-3e9100f54735",
   "metadata": {},
   "source": [
    "### Run 4: DeepSeek 7B COT\n",
    "\n",
    "Results in the following file:\n",
    "\n",
    "Results/Earnings/results - Deepseek - COT C -2025-04-01 21:13:35.944412.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02aea17c-33e8-4ed7-9fd7-ebcf73b6ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_s3_loc': 'deepseek7B',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'multi-gpu':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd0d50b-6c95-4ce5-b503-12215cfe902d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "ir = HuggingfaceRun(run_name='DeepSeek 7B Earnings COT',\n",
    "                       model_id='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',\n",
    "                       dataset_id='data_quarterly_pit_indu_refresh_blended.json',\n",
    "                       system_prompt=prompts.SYSTEM_PROMPTS['COT_EARN'],run_config=run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae221de2-c71d-49f9-ab74-23876c132ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 8 GPUs.\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n",
      "deepseek7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11faff2b6a724e0fa15daf54d72a89e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c953bf644c4a9ea4b69bdfcbc5c560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529da691ffcf4c7195160cf33f24b42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e727ab9919754053a8151c3d45a743c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3908dcf8b744a2bac73be9afbb4c079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ab89be3fac4a4e81e1ef1df0271f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9536a6a9c79b4633abc4a15455bdfe55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae19905fff644e583df3ddbd20757d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5.4 GB\n",
      "starting backtest...starting backtest...starting backtest...\n",
      "starting backtest...\n",
      "\n",
      "starting backtest...\n",
      "\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [2:26:10,  9.47s/it]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run in 2:30:12.540523\n",
      "Called Save run\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-03-30 20:11:39.828629.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [2:30:13, 10.15s/it]\n",
      "[2025-03-30 22:42:55,767] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3671 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,768] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3672 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,769] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3673 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,770] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3674 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,771] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3676 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,771] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3677 via signal SIGTERM\n",
      "[2025-03-30 22:42:55,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3678 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f8ebc-2756-4eb0-a2d9-777fc5fd0bb9",
   "metadata": {},
   "source": [
    "### Qwen 2.5 - 3B Instruct\n",
    "For this model, we apply quantization to reduce the size of the model. This will be used for fine-tuning and so memory footprint needs to be smaller to permit training on a single GPU. We use the Bits and Bytes library from Huggingface to convert to 4bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0be801bc-c018-42b4-98a0-ebacbc024890",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_s3_loc': 'qwen3b',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'multi-gpu':True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb76425a-1690-4998-8448-a4d439685fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "ir = HuggingfaceRun(run_name='Qwen 3B Earnings BASE',\n",
    "                       model_id='Qwen/Qwen2.5-3B-Instruct',\n",
    "                       dataset_id='data_quarterly_pit_indu_refresh_blended.json',\n",
    "                       system_prompt=prompts.SYSTEM_PROMPTS['BASE_EARN'],run_config=run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c54db2c-0ede-4571-a364-90f688cf462f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a49c5eb99a4aa0a64c412aac5b9c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7356268d285344eaa868aa4d99f47aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd551e637b8e4f4bbce3fecb98499033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d7b510b3d04566a56e531e0043158e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 2.0 GB\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [16:02,  1.09s/it]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run in 0:16:02.309246\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-04-29 16:17:41.890187.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [16:02,  1.08s/it]\n",
      "[2025-04-29 16:34:21,064] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 318 via signal SIGTERM\n",
      "[2025-04-29 16:34:21,065] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 319 via signal SIGTERM\n",
      "[2025-04-29 16:34:21,066] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 320 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4dfbc32-0c0b-4b6b-874a-c897f70652fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_s3_loc': 'qwen3b',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'multi-gpu':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c6c33e5-606d-4d16-8e20-ce449c2e5c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "ir = HuggingfaceRun(run_name='Qwen 3B Earnings COT',\n",
    "                       model_id='Qwen/Qwen2.5-3B-Instruct',\n",
    "                       dataset_id='data_quarterly_pit_indu_refresh_blended.json',\n",
    "                       system_prompt=prompts.SYSTEM_PROMPTS['COT_EARN'],run_config=run_config)\n",
    "\n",
    "# Create the prompts and save to the Data folder\n",
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea7dfd49-e0cd-4fd4-8543-31edb9843aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "qwen3b\n",
      "qwen3b\n",
      "qwen3b\n",
      "qwen3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 2.0 GB\n",
      "starting backtest...\n",
      "starting backtest...starting backtest...\n",
      "\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [13:42,  1.09it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run in 0:13:42.324489\n",
      "Called Save run\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-04-03 20:06:52.680533.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [13:42,  1.08it/s]\n",
      "[2025-04-03 20:21:16,801] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 1305 via signal SIGTERM\n",
      "[2025-04-03 20:21:16,802] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 1306 via signal SIGTERM\n",
      "[2025-04-03 20:21:16,803] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 1307 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a613d5-ea61-40bc-bc26-4b4ef60f95f4",
   "metadata": {},
   "source": [
    "## Run 5 - Fine tuned model - EPS only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a234749-8d26-453d-bb1d-af522d694cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    'model_hf_id': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "    'model_s3_loc': 'qwen3b',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE_FINE_TUNED'],\n",
    "    'multi-gpu':False,\n",
    "    'dataset': 'data_quarterly_pit_indu_blended_base',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_blended.json',\n",
    "    'fine_tuned_dir': 'fine_tuned2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ade0f00-1ec6-4f37-9728-00d1cfc9e966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "ftm = mft.FineTunedInference(\"EPS only FT Run\", run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ab2e96-94cf-4399-b0fd-99ee62889995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "prompt_set = ftm.create_all_prompts(force_refresh=True, is_save_prompts=True)\n",
    "prompt_set_appended = ftm.reformat_prompts(prompt_set, \"The next period EPS is \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "716cb386-58e6-4993-926c-9df66b320455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 887/887 [1:13:27<00:00,  4.97s/it]\n"
     ]
    }
   ],
   "source": [
    "output_eps = ftm.run_finetuned_backtest(prompt_set_appended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "124420f5-f726-4560-89d3-7a51ac328414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the output\n",
    "with open(f\"Results/Earnings/results - Qwen3B Finetuned - EPS only.json\", 'w') as f:\n",
    "    json.dump(output_eps, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dff2f44-0e17-4102-a489-dcef054afc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83378e09-b3ea-462a-ab46-fc23a02ea6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = HuggingfaceRun(run_name='Qwen Earnings',\n",
    "                       model_id='Qwen/Qwen2.5-3B-Instruct',\n",
    "                       dataset_id='data_quarterly_pit_indu_refresh_blended.json',\n",
    "                       system_prompt=prompts.SYSTEM_PROMPTS['BASE_FINE_TUNED'],run_config=run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b48ff569-21b3-4a55-aa5e-b4892f60114b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe160210-08ed-4815-896d-0faa1e3c8378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "  0%|          | 0/887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 2.0 GB\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [1:12:06,  4.87s/it]                         \n",
      "[2025-04-29 20:07:24,621] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGINT death signal, shutting down workers\n",
      "[2025-04-29 20:07:24,623] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 2901 via signal SIGINT\n",
      "[2025-04-29 20:07:24,624] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 2902 via signal SIGINT\n",
      "[2025-04-29 20:07:24,625] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 2903 via signal SIGINT\n",
      "[2025-04-29 20:07:54,656] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2901 via Signals.SIGINT, forcefully exiting via Signals.SIGKILL\n",
      "[2025-04-29 20:07:54,919] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2902 via Signals.SIGINT, forcefully exiting via Signals.SIGKILL\n",
      "[2025-04-29 20:07:55,144] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2903 via Signals.SIGINT, forcefully exiting via Signals.SIGKILL\n"
     ]
    },
    {
     "ename": "SignalException",
     "evalue": "Process 2820 got signal: 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSignalException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_multi_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/accelerate/launchers.py:244\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, ELASTIC_LOG_LINE_PREFIX_TEMPLATE_PYTORCH_VERSION):\n\u001b[1;32m    243\u001b[0m         launch_config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_line_prefix_template\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log_line_prefix_template\n\u001b[0;32m--> 244\u001b[0m     \u001b[43melastic_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLaunchConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlaunch_config_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/torch/distributed/launcher/api.py:134\u001b[0m, in \u001b[0;36melastic_launch.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlaunch_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entrypoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/torch/distributed/launcher/api.py:255\u001b[0m, in \u001b[0;36mlaunch_agent\u001b[0;34m(config, entrypoint, args)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     metrics\u001b[38;5;241m.\u001b[39minitialize_metrics(metrics\u001b[38;5;241m.\u001b[39mMetricsConfig(config\u001b[38;5;241m.\u001b[39mmetrics_cfg))\n\u001b[0;32m--> 255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# records that agent.run() has succeeded NOT that workers have succeeded\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     events\u001b[38;5;241m.\u001b[39mrecord(agent\u001b[38;5;241m.\u001b[39mget_event_succeeded())\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py:124\u001b[0m, in \u001b[0;36mprof.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 124\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     put_metric(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.success\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, group)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py:736\u001b[0m, in \u001b[0;36mSimpleElasticAgent.run\u001b[0;34m(self, role)\u001b[0m\n\u001b[1;32m    734\u001b[0m shutdown_called: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 736\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_execution_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_metrics(result)\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py:878\u001b[0m, in \u001b[0;36mSimpleElasticAgent._invoke_run\u001b[0;34m(self, role)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_group\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m!=\u001b[39m WorkerState\u001b[38;5;241m.\u001b[39mINIT\n\u001b[1;32m    877\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(monitor_interval)\n\u001b[0;32m--> 878\u001b[0m run_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_monitor_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m state \u001b[38;5;241m=\u001b[39m run_result\u001b[38;5;241m.\u001b[39mstate\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_group\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m state\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py:124\u001b[0m, in \u001b[0;36mprof.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 124\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     put_metric(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.success\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, group)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py:307\u001b[0m, in \u001b[0;36mLocalElasticAgent._monitor_workers\u001b[0;34m(self, worker_group)\u001b[0m\n\u001b[1;32m    300\u001b[0m     log\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m] worker pids do not match process_context pids.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Expected: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, actual: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    303\u001b[0m         role, worker_pids, pc_pids\n\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunResult(state\u001b[38;5;241m=\u001b[39mWorkerState\u001b[38;5;241m.\u001b[39mUNKNOWN)\n\u001b[0;32m--> 307\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mis_failed():\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;66;03m# map local rank failure to global rank\u001b[39;00m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py:288\u001b[0m, in \u001b[0;36mPContext.wait\u001b[0;34m(self, timeout, period)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03mWaits for the specified ``timeout`` seconds, polling every ``period`` seconds\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03mfor the processes to be done. Returns ``None`` if the processes are still running\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03mthe SIGKILL.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    291\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmaxsize\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py:454\u001b[0m, in \u001b[0;36mMultiprocessContext._poll\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# assertion for mypy type checker\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;66;03m# torch.mp.ProcessContext Throws an Exception if some/all of\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;66;03m# worker processes failed\u001b[39;00m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;66;03m# timeout < 0 checks worker status and return immediately\u001b[39;00m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;66;03m# Join will never return success since we use synchronize.Event to wait\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# for all processes to finish.\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;66;03m# IMPORTANT: we use multiprocessing.Queue to carry worker return values\u001b[39;00m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# back to the parent, the worker process will wait before terminating\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;66;03m# until all the buffered items are fed by the feeder thread to the underlying\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# pipe. Hence to prevent deadlocks on large return values,\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# we opportunistically try queue.get on each join call\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# See: https://docs.python.org/2/library/multiprocessing.html#all-platforms\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m local_rank \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnprocs):\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:137\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m process\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m    136\u001b[0m         process\u001b[38;5;241m.\u001b[39mterminate()\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# There won't be an error on the queue if the process crashed.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m failed_process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocesses[error_index]\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/multiprocessing/popen_fork.py:43\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWNOHANG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/multiprocessing/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         pid, sts \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/kernel/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py:62\u001b[0m, in \u001b[0;36m_terminate_process_handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Termination handler that raises exceptions on the main process.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mWhen the process receives death signal(SIGTERM, SIGINT), this termination handler will\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mbe terminated.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m sigval \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mSignals(signum)\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m SignalException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mgetpid()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got signal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msigval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, sigval\u001b[38;5;241m=\u001b[39msigval)\n",
      "\u001b[0;31mSignalException\u001b[0m: Process 2820 got signal: 2"
     ]
    }
   ],
   "source": [
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9f29a94-3def-4285-871e-c23dc76f252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ir.cached_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d84c69-7826-4335-befe-48cadaef00fd",
   "metadata": {},
   "source": [
    "## Stock Recommendation Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936cb54c-f0d2-4b4d-ad21-abe4d7c8bf23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b1ac792-d286-4374-977e-a9ae6a0a352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "    'model_s3_loc': 'deepseek14Q',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoT'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_annual_pit_spx',\n",
    "    'data_location': 'data_annual_pit_spx.json'\n",
    "}\n",
    "\n",
    "run_config = {\n",
    "    'model_hf_id': 'Qwen/Qwen2.5-7B-Instruct',\n",
    "    'model_s3_loc': 'qwen',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 1\n",
    "run_config = {\n",
    "    'model_hf_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 2\n",
    "run_config = {\n",
    "    'model_hf_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoT'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 3\n",
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "    'model_s3_loc': 'deepseek14Q',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "# RUN 4\n",
    "run_config = {\n",
    "    'model_hf_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "    'model_s3_loc': 'deepseek14Q',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoT'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "\n",
    "# Run 6 - failed\n",
    "run_config = {\n",
    "    'model_hf_id': 'Qwen/Qwen2.5-7B-Instruct',\n",
    "    'model_s3_loc': 'qwen',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['CoTDetailed'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu',\n",
    "    'data_location': 'data_quarterly_pit_indu.json'\n",
    "}\n",
    "\n",
    "run_config = {\n",
    "    'model_hf_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    'model_s3_loc': 'llama',\n",
    "    'model_reload': False,\n",
    "    'model_quant': None,\n",
    "    'system_prompt': prompts.SYSTEM_PROMPTS['BASE_EARN'],\n",
    "    'multi-gpu':True,\n",
    "    'dataset': 'data_quarterly_pit_indu_refresh_v2',\n",
    "    'data_location': 'data_quarterly_pit_indu_refresh_v2.json'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "run_name = f\"{run_config['model_s3_loc']}_{run_config['dataset']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe94d167-f17d-4a2e-be69-db2210d4bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = model_inference.InferenceRun(run_name, run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abf93574-9fd9-4739-89eb-77941804fe03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "# Create the prompts and save to the Data folder\n",
    "prompts = ir.create_all_prompts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a0f3e83-e637-4d70-88a8-39b5a2d6e734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "llama\n",
      "llama\n",
      "llama\n",
      "llama\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb37171aae694fd195996b4ccda5320f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86688187a52642aa9e11e53cff75c80c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1171c4eb5c2f43f8b324488a31c2ed1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b891ef7620a4f4d9769c56708e000f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04af17ad7cf4a93a3bdd906f3e1280b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b911d278f32f4d7cba8d9dee72d80bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd4f87d1ee54fbe8342687ae40042e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/891 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 6.4 GB\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n",
      "starting backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 884/891 [27:05<00:12,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 888/891 [27:10<00:04,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "892it [27:19,  1.82s/it]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run...\n",
      "Finished run...\n",
      "Gathered results...Gathered results...Gathered results...Gathered results...\n",
      "\n",
      "\n",
      "\n",
      "Finished run in 0:27:51.318931\n",
      "Called Save run\n",
      "called log\n",
      "Saved bclarke16/tmp/fs/logs/Results_2025-03-30 09:15:48.238865.json\n",
      "Run Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "892it [27:51,  1.87s/it]\n",
      "[2025-03-30 09:44:39,571] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 257 via signal SIGTERM\n",
      "[2025-03-30 09:44:39,572] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 258 via signal SIGTERM\n",
      "[2025-03-30 09:44:39,573] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 260 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "# Run the multi-gpu model with notebook_launcher\n",
    "notebook_launcher(ir.run_multi_gpu, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03dc716-b772-49fc-8b42-4f0ecf2fbfe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9642caf9-5b73-4fb8-b32f-19717c4ca374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb455d3-7150-455d-99d8-8c72b787f33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef28887-fe90-4d2d-b069-8a425e5d9a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54ebba-0ee4-40a4-a92c-74a1f882d380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf4d6235-af59-4924-b54b-72ad749bd686",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = model_inference.InferenceRun(run_name, run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7925f016-9392-4142-a9ec-a6872af1bae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting all datasets...\n",
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "p1 = ir.create_all_prompts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14fd8453-b388-488c-b553-aaff3333ea23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff153540-48a0-4059-acf8-bae25ca9dd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2a95716048465abd4618c818c8f93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ir.load_model_from_storage(ir.model_s3_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370eddfa-1f87-4ac9-863f-57434aed7ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfcdf6bf-07d3-4fb0-87de-034e94e92a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdf599ad-b6de-4a30-b48a-7c30a97ce88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(ir.model_hf_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a9cff15-9c86-48fd-be61-10bd0d160f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model...\n"
     ]
    }
   ],
   "source": [
    "output_result = ir.run_model(p1[0]['prompt'],tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86f4a03-2196-434b-b5a1-22f2435f9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_result = {\"date\": \"2020-02-06\", \"security\": \"MMM UN Equity\", \"response\": \"Here is the JSON response:\\n\\n```json\\n{\\n  \\\"decision\\\": \\\"BUY\\\",\\n  \\\"confidence score\\\": 80,\\n  \\\"reason\\\": \\\"Gross profit and EPS have increased over time, indicating a strong financial performance\\\"\\n}\\n```\\n\\nI have computed the following financial ratios:\\n\\n1. Gross Margin: \\n   - 2020: 3.786000e+09 / 8.111000e+09 = 0.466\\n   - 2019: 3.803000e+09 / 7.991000e+09 = 0.477\\n   - 2018: 3.858000e+09 / 8.171000e+09 = 0.471\\n   - 2017: 3.553000e+09 / 7.863000e+09 = 0.453\\n   - 2016: 3.885000e+09 / 7.945000e+09 = 0.492\\n\\nThe gross margin has been increasing over time, indicating a strong financial performance.\\n\\n2. EPS:\\n   - 2020: 9.690000e+08 / 1.012600e+10 = 0.953\\n   - 2019: 1.583000e+09 / 1.076400e+10 = 1.465\\n   - 2018: 1.127000e+09 / 1.014200e+10 = 1.117\\n   - 2017: 1.347000e+09 / 9.848000e+09 = 0.137\\n   - 2016: 1.543000e+09 / 9.848000e+09 = 0.156\\n\\nThe EPS has been increasing over time, indicating a strong financial performance.\\n\\n3. Current Ratio:\\n   - 2020: 2.441000e+09 / 9.222000e+09 = 0.264\\n   - 2019: 1.588000e+09 / 7.821000e+09 = 0.201\\n   - 2018: 1.131000e+09 / 7.265000e+09 = 0.157\\n   - 2017: 8.930000e+08 / 7.244000e+09 = 0.123\\n   - 2016: 8.910000e+08 / 5.020000e+09 = 0.177\\n\\nThe current ratio has been increasing over time, indicating a strong financial performance.\\n\\nBased on these financial ratios, the company has been performing well financially and has a strong track record of increasing gross profit and EPS over time. Therefore, I recommend a BUY decision with a confidence score of 80.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5457878-488c-48d5-935b-44cf0427b19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the JSON response:\\n\\n```json\\n{\\n  \"decision\": \"BUY\",\\n  \"confidence score\": 80,\\n  \"reason\": \"Gross profit and EPS have increased over time, indicating a strong financial performance\"\\n}\\n```\\n\\nI have computed the following financial ratios:\\n\\n1. Gross Margin: \\n   - 2020: 3.786000e+09 / 8.111000e+09 = 0.466\\n   - 2019: 3.803000e+09 / 7.991000e+09 = 0.477\\n   - 2018: 3.858000e+09 / 8.171000e+09 = 0.471\\n   - 2017: 3.553000e+09 / 7.863000e+09 = 0.453\\n   - 2016: 3.885000e+09 / 7.945000e+09 = 0.492\\n\\nThe gross margin has been increasing over time, indicating a strong financial performance.\\n\\n2. EPS:\\n   - 2020: 9.690000e+08 / 1.012600e+10 = 0.953\\n   - 2019: 1.583000e+09 / 1.076400e+10 = 1.465\\n   - 2018: 1.127000e+09 / 1.014200e+10 = 1.117\\n   - 2017: 1.347000e+09 / 9.848000e+09 = 0.137\\n   - 2016: 1.543000e+09 / 9.848000e+09 = 0.156\\n\\nThe EPS has been increasing over time, indicating a strong financial performance.\\n\\n3. Current Ratio:\\n   - 2020: 2.441000e+09 / 9.222000e+09 = 0.264\\n   - 2019: 1.588000e+09 / 7.821000e+09 = 0.201\\n   - 2018: 1.131000e+09 / 7.265000e+09 = 0.157\\n   - 2017: 8.930000e+08 / 7.244000e+09 = 0.123\\n   - 2016: 8.910000e+08 / 5.020000e+09 = 0.177\\n\\nThe current ratio has been increasing over time, indicating a strong financial performance.\\n\\nBased on these financial ratios, the company has been performing well financially and has a strong track record of increasing gross profit and EPS over time. Therefore, I recommend a BUY decision with a confidence score of 80.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_result['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5728ba70-2042-4250-af42-cac37f26eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30024d1f-b16b-4214-962c-190106dc2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_json(llm_output):\n",
    "    # remove all the broken lines\n",
    "    form = llm_output.replace('\\n','')\n",
    "    # Find the start and end of the JSON input\n",
    "    #try:\n",
    "    soj = form.find('```json')\n",
    "    eoj = form.find('}```')\n",
    "\n",
    "    if eoj == -1:\n",
    "        eoj = len(llm_output)\n",
    "        llm_output = llm_output + '}```'\n",
    "    # Pull out the additional context\n",
    "    additional = form[:soj]\n",
    "    additional += form[eoj + 4:]\n",
    "    json_obj = json.loads(form[soj + 7:eoj + 1])\n",
    "    json_obj['AdditionalContext'] = additional\n",
    "    return json_obj\n",
    "    #except:\n",
    "    #    return llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73b1782b-e52e-407d-b9d6-2d691cb8fc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  \"decision\": \"BUY\",  \"confidence score\": 80,  \"reason\": \"Gross profit and EPS have increased over time, indicating a strong financial performance\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'decision': 'BUY',\n",
       " 'confidence score': 80,\n",
       " 'reason': 'Gross profit and EPS have increased over time, indicating a strong financial performance',\n",
       " 'AdditionalContext': 'Here is the JSON response:I have computed the following financial ratios:1. Gross Margin:    - 2020: 3.786000e+09 / 8.111000e+09 = 0.466   - 2019: 3.803000e+09 / 7.991000e+09 = 0.477   - 2018: 3.858000e+09 / 8.171000e+09 = 0.471   - 2017: 3.553000e+09 / 7.863000e+09 = 0.453   - 2016: 3.885000e+09 / 7.945000e+09 = 0.492The gross margin has been increasing over time, indicating a strong financial performance.2. EPS:   - 2020: 9.690000e+08 / 1.012600e+10 = 0.953   - 2019: 1.583000e+09 / 1.076400e+10 = 1.465   - 2018: 1.127000e+09 / 1.014200e+10 = 1.117   - 2017: 1.347000e+09 / 9.848000e+09 = 0.137   - 2016: 1.543000e+09 / 9.848000e+09 = 0.156The EPS has been increasing over time, indicating a strong financial performance.3. Current Ratio:   - 2020: 2.441000e+09 / 9.222000e+09 = 0.264   - 2019: 1.588000e+09 / 7.821000e+09 = 0.201   - 2018: 1.131000e+09 / 7.265000e+09 = 0.157   - 2017: 8.930000e+08 / 7.244000e+09 = 0.123   - 2016: 8.910000e+08 / 5.020000e+09 = 0.177The current ratio has been increasing over time, indicating a strong financial performance.Based on these financial ratios, the company has been performing well financially and has a strong track record of increasing gross profit and EPS over time. Therefore, I recommend a BUY decision with a confidence score of 80.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_json(output_result['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50b16da3-394a-4aeb-8c5d-2a7456d3e0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved s3://awmgd-prod-finml-sandbox-user/bclarke16/tmp/fs/logs/results - llama - data_quarterly_pit_indu\n",
      "Run Completed!\n"
     ]
    }
   ],
   "source": [
    "ir.save_run({'test':'1234'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eade0d8d-ba3b-4706-a46a-9e76a77ec445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old Code\n",
    "# run_name = f\"{run_config['model_s3_loc']}_{run_config['dataset']}_finetuned\"\n",
    "# ir = model_inference.InferenceRun(run_name, run_config)\n",
    "\n",
    "# # Create the prompts and save to the Data folder\n",
    "# prompt_set = ir.create_all_prompts(force_refresh=True, is_save_prompts=True)\n",
    "# for prompt in prompt_set:\n",
    "#     prompt['prompt'] += 'The next period EPS is '\n",
    "#     #prompt['prompt'] += \"\\nAnswer in JSON format with the next period EPS, the direction, the magnitude and a confidence.\"\n",
    "\n",
    "# from peft import PeftModel\n",
    "# from transformers import AutoTokenizer\n",
    "# import json\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# outputs = []\n",
    "# def run_finetuned_backtest(prompts, fine_tuned_model, tokenizer):\n",
    "#     count = 0\n",
    "#     progress = tqdm(total=len(prompts), position=0, leave=True)\n",
    "#     for prompt in prompts:\n",
    "#         tokens = tokenizer.apply_chat_template(prompt['prompt'], tokenize=False, add_generation_prompt=True )\n",
    "#         print(tokens)\n",
    "#         model_inputs = tokenizer([tokens], return_tensors='pt').to(\"cuda\")\n",
    "#         generated_ids = fine_tuned_model.generate(**model_inputs, \n",
    "#                                        pad_token_id=tokenizer.eos_token_id, \n",
    "#                                        max_new_tokens=50,\n",
    "#                                       temperature=0.001)\n",
    "#         parsed_ids = [\n",
    "#             output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "#         ]\n",
    "#         resp = {\n",
    "#             'date': prompt['date'],\n",
    "#             'security': prompt['security'],\n",
    "#             'response': tokenizer.batch_decode(parsed_ids, skip_special_tokens=True)[0]\n",
    "#         }\n",
    "#         outputs.append(resp)\n",
    "#         progress.update()\n",
    "\n",
    "# # Load the base model \n",
    "# model_helper = mh.ModelHelper('tmp/fs')\n",
    "# base_model = ir.load_model_from_storage(run_config['model_s3_loc'])\n",
    "# # clear the local folder once completed loading into memory\n",
    "# model_helper.clear_folder(run_config['model_s3_loc'])\n",
    "\n",
    "# # Update the base model with the fine-tuned modules\n",
    "# fine_tuned_model = PeftModel.from_pretrained(base_model, 'fine_tuned2')\n",
    "# # Create the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(run_config['model_hf_id'])\n",
    "# run_finetuned_backtest(prompt_set[:2], fine_tuned_model, tokenizer)\n",
    "\n",
    "# # save the output\n",
    "# with open(f\"Results/Earnings/results - Qwen3B Finetuned - EPS only.json\", 'w') as f:\n",
    "#     json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4e95d-81d2-428d-9789-02d82b3f374f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd5cab-e6c8-457a-a6ab-0a66da205511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb49190-da5c-41b8-a4e8-b90767203d16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bloomberg Lab Python 3",
   "language": "python",
   "name": "remote-jupyterpython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
